# Chapter 37: Caching Patterns

## Introduction

Caching is a performance optimization technique that stores frequently accessed data in fast-access storage. Proper caching can dramatically improve application performance, reduce server load, and decrease costs.

---

## 37.1 Caching Strategies

### 1. Cache-Aside (Lazy Loading)

The application is responsible for loading data into the cache. Most common pattern.

**Flow:**
1. Check cache for data
2. If miss, fetch from database
3. Store in cache for future requests

```javascript
class CacheAside {
  constructor(dataSource, cache) {
    this.dataSource = dataSource;
    this.cache = cache;
  }
  
  async get(key) {
    // 1. Try to get from cache
    let value = await this.cache.get(key);
    
    if (value !== null) {
      console.log('Cache hit:', key);
      return value;
    }
    
    console.log('Cache miss:', key);
    
    // 2. Fetch from data source
    value = await this.dataSource.fetch(key);
    
    // 3. Store in cache
    if (value !== null) {
      await this.cache.set(key, value);
    }
    
    return value;
  }
  
  async invalidate(key) {
    await this.cache.delete(key);
  }
}

// Usage
const userCache = new CacheAside(
  { fetch: (id) => database.query('SELECT * FROM users WHERE id = ?', [id]) },
  new Map()
);

const user = await userCache.get('user:123');
```

### 2. Read-Through Cache

Cache sits between application and database. Cache is responsible for loading data.

```javascript
class ReadThroughCache {
  constructor(dataSource, ttl = 3600) {
    this.cache = new Map();
    this.dataSource = dataSource;
    this.ttl = ttl;
  }
  
  async get(key) {
    const cached = this.cache.get(key);
    
    // Check if cache is valid
    if (cached && Date.now() - cached.timestamp < this.ttl * 1000) {
      return cached.value;
    }
    
    // Cache miss or expired - fetch from source
    const value = await this.dataSource.fetch(key);
    
    this.cache.set(key, {
      value,
      timestamp: Date.now()
    });
    
    return value;
  }
}
```

### 3. Write-Through Cache

Data is written to cache and database simultaneously.

```javascript
class WriteThroughCache {
  constructor(dataSource, cache) {
    this.dataSource = dataSource;
    this.cache = cache;
  }
  
  async get(key) {
    let value = await this.cache.get(key);
    if (value !== null) return value;
    
    value = await this.dataSource.fetch(key);
    await this.cache.set(key, value);
    return value;
  }
  
  async set(key, value) {
    // Write to both cache and database
    await Promise.all([
      this.cache.set(key, value),
      this.dataSource.save(key, value)
    ]);
  }
}
```

### 4. Write-Behind (Write-Back) Cache

Data is written to cache first, then asynchronously to database.

```javascript
class WriteBehindCache {
  constructor(dataSource, flushInterval = 5000) {
    this.cache = new Map();
    this.dirtyKeys = new Set();
    this.dataSource = dataSource;
    this.flushInterval = flushInterval;
    
    // Start periodic flush
    this.startFlushTimer();
  }
  
  async get(key) {
    if (this.cache.has(key)) {
      return this.cache.get(key);
    }
    
    const value = await this.dataSource.fetch(key);
    this.cache.set(key, value);
    return value;
  }
  
  set(key, value) {
    this.cache.set(key, value);
    this.dirtyKeys.add(key);
    return value;
  }
  
  startFlushTimer() {
    this.timer = setInterval(() => {
      this.flush();
    }, this.flushInterval);
  }
  
  async flush() {
    if (this.dirtyKeys.size === 0) return;
    
    const keys = Array.from(this.dirtyKeys);
    this.dirtyKeys.clear();
    
    // Batch write to database
    const writes = keys.map(key => ({
      key,
      value: this.cache.get(key)
    }));
    
    try {
      await this.dataSource.batchSave(writes);
      console.log(`Flushed ${keys.length} entries to database`);
    } catch (error) {
      // Re-add failed keys to dirty set
      keys.forEach(key => this.dirtyKeys.add(key));
      console.error('Flush failed:', error);
    }
  }
  
  destroy() {
    clearInterval(this.timer);
    return this.flush(); // Final flush
  }
}
```

---

## 37.2 Cache Implementation Patterns

### 1. In-Memory Cache with TTL

```javascript
class TTLCache {
  constructor(defaultTTL = 3600) {
    this.cache = new Map();
    this.timers = new Map();
    this.defaultTTL = defaultTTL;
  }
  
  set(key, value, ttl = this.defaultTTL) {
    // Clear existing timer
    if (this.timers.has(key)) {
      clearTimeout(this.timers.get(key));
    }
    
    this.cache.set(key, value);
    
    // Set expiration timer
    const timer = setTimeout(() => {
      this.delete(key);
    }, ttl * 1000);
    
    this.timers.set(key, timer);
  }
  
  get(key) {
    return this.cache.get(key);
  }
  
  delete(key) {
    if (this.timers.has(key)) {
      clearTimeout(this.timers.get(key));
      this.timers.delete(key);
    }
    this.cache.delete(key);
  }
  
  clear() {
    this.timers.forEach(timer => clearTimeout(timer));
    this.timers.clear();
    this.cache.clear();
  }
  
  size() {
    return this.cache.size;
  }
}

// Usage
const cache = new TTLCache(300); // 5 minutes default
cache.set('session:abc123', { userId: 1, name: 'John' });
cache.set('temp:data', { value: 42 }, 60); // 1 minute TTL
```

### 2. LRU (Least Recently Used) Cache

Removes least recently used items when capacity is reached.

```javascript
class LRUCache {
  constructor(capacity) {
    this.capacity = capacity;
    this.cache = new Map();
  }
  
  get(key) {
    if (!this.cache.has(key)) {
      return null;
    }
    
    // Move to end (most recently used)
    const value = this.cache.get(key);
    this.cache.delete(key);
    this.cache.set(key, value);
    
    return value;
  }
  
  set(key, value) {
    // If key exists, remove it first
    if (this.cache.has(key)) {
      this.cache.delete(key);
    }
    
    // Check capacity
    if (this.cache.size >= this.capacity) {
      // Remove first (least recently used)
      const firstKey = this.cache.keys().next().value;
      this.cache.delete(firstKey);
      console.log('Evicted LRU item:', firstKey);
    }
    
    // Add new item (most recently used)
    this.cache.set(key, value);
  }
  
  delete(key) {
    return this.cache.delete(key);
  }
  
  clear() {
    this.cache.clear();
  }
  
  size() {
    return this.cache.size;
  }
  
  keys() {
    return Array.from(this.cache.keys());
  }
}

// Usage
const lruCache = new LRUCache(3);
lruCache.set('a', 1);
lruCache.set('b', 2);
lruCache.set('c', 3);
lruCache.set('d', 4); // 'a' is evicted
lruCache.get('b');    // 'b' becomes most recent
lruCache.set('e', 5); // 'c' is evicted (least recent)
```

### 3. Memoization Pattern

Cache function results based on arguments:

```javascript
function memoize(fn) {
  const cache = new Map();
  
  return function(...args) {
    const key = JSON.stringify(args);
    
    if (cache.has(key)) {
      console.log('Returning cached result for:', args);
      return cache.get(key);
    }
    
    const result = fn.apply(this, args);
    cache.set(key, result);
    return result;
  };
}

// Usage - expensive calculation
const fibonacci = memoize(function(n) {
  if (n <= 1) return n;
  return fibonacci(n - 1) + fibonacci(n - 2);
});

console.log(fibonacci(40)); // First call - slow
console.log(fibonacci(40)); // Cached - instant

// Advanced memoization with TTL
function memoizeWithTTL(fn, ttl = 60000) {
  const cache = new Map();
  
  return function(...args) {
    const key = JSON.stringify(args);
    const cached = cache.get(key);
    
    if (cached && Date.now() - cached.timestamp < ttl) {
      return cached.value;
    }
    
    const value = fn.apply(this, args);
    cache.set(key, { value, timestamp: Date.now() });
    return value;
  };
}

// API call with 5-minute cache
const fetchUserData = memoizeWithTTL(async (userId) => {
  const response = await fetch(`/api/users/${userId}`);
  return response.json();
}, 300000);
```

### 4. Cache Warming Pattern

Pre-populate cache with frequently accessed data:

```javascript
class WarmableCache {
  constructor(dataSource) {
    this.cache = new Map();
    this.dataSource = dataSource;
  }
  
  async warm(keys) {
    console.log(`Warming cache with ${keys.length} items...`);
    
    const promises = keys.map(async key => {
      try {
        const value = await this.dataSource.fetch(key);
        this.cache.set(key, value);
        return { key, status: 'success' };
      } catch (error) {
        return { key, status: 'failed', error: error.message };
      }
    });
    
    const results = await Promise.all(promises);
    
    const successful = results.filter(r => r.status === 'success').length;
    console.log(`Cache warmed: ${successful}/${keys.length} items loaded`);
    
    return results;
  }
  
  async get(key) {
    if (this.cache.has(key)) {
      return this.cache.get(key);
    }
    
    const value = await this.dataSource.fetch(key);
    this.cache.set(key, value);
    return value;
  }
}

// Usage
const productCache = new WarmableCache({
  fetch: (id) => database.getProduct(id)
});

// Warm cache on startup with popular products
await productCache.warm(['prod:1', 'prod:5', 'prod:23', 'prod:42']);
```

---

## 37.3 Cache Invalidation Strategies

> "There are only two hard things in Computer Science: cache invalidation and naming things." - Phil Karlton

### 1. Time-Based Expiration (TTL)

```javascript
class CacheWithExpiry {
  constructor() {
    this.cache = new Map();
  }
  
  set(key, value, ttlSeconds = 3600) {
    const expiresAt = Date.now() + (ttlSeconds * 1000);
    this.cache.set(key, { value, expiresAt });
  }
  
  get(key) {
    const item = this.cache.get(key);
    
    if (!item) return null;
    
    // Check expiration
    if (Date.now() > item.expiresAt) {
      this.cache.delete(key);
      return null;
    }
    
    return item.value;
  }
  
  // Cleanup expired entries
  cleanup() {
    const now = Date.now();
    for (const [key, item] of this.cache.entries()) {
      if (now > item.expiresAt) {
        this.cache.delete(key);
      }
    }
  }
}
```

### 2. Event-Based Invalidation

```javascript
class EventDrivenCache {
  constructor(eventEmitter) {
    this.cache = new Map();
    this.eventEmitter = eventEmitter;
    this.setupListeners();
  }
  
  setupListeners() {
    // Invalidate on data updates
    this.eventEmitter.on('user:updated', (userId) => {
      this.invalidate(`user:${userId}`);
    });
    
    this.eventEmitter.on('user:deleted', (userId) => {
      this.invalidate(`user:${userId}`);
    });
  }
  
  invalidate(key) {
    if (this.cache.has(key)) {
      this.cache.delete(key);
      console.log('Cache invalidated:', key);
    }
  }
  
  invalidatePattern(pattern) {
    const regex = new RegExp(pattern);
    const keysToDelete = [];
    
    for (const key of this.cache.keys()) {
      if (regex.test(key)) {
        keysToDelete.push(key);
      }
    }
    
    keysToDelete.forEach(key => this.cache.delete(key));
    console.log(`Invalidated ${keysToDelete.length} cache entries`);
  }
  
  set(key, value) {
    this.cache.set(key, value);
  }
  
  get(key) {
    return this.cache.get(key);
  }
}

// Usage
const events = new EventEmitter();
const cache = new EventDrivenCache(events);

cache.set('user:123', { name: 'John' });

// Later, when user is updated
events.emit('user:updated', '123'); // Cache automatically invalidated
```

### 3. Cache Tags Pattern

Group related cache entries for batch invalidation:

```javascript
class TaggedCache {
  constructor() {
    this.cache = new Map();
    this.tags = new Map(); // tag -> Set of keys
  }
  
  set(key, value, tags = []) {
    this.cache.set(key, value);
    
    // Associate key with tags
    tags.forEach(tag => {
      if (!this.tags.has(tag)) {
        this.tags.set(tag, new Set());
      }
      this.tags.get(tag).add(key);
    });
  }
  
  get(key) {
    return this.cache.get(key);
  }
  
  invalidateTag(tag) {
    const keys = this.tags.get(tag);
    if (!keys) return;
    
    keys.forEach(key => this.cache.delete(key));
    this.tags.delete(tag);
    
    console.log(`Invalidated tag "${tag}": ${keys.size} entries removed`);
  }
  
  invalidateTags(tags) {
    tags.forEach(tag => this.invalidateTag(tag));
  }
}

// Usage
const cache = new TaggedCache();

cache.set('user:123', { name: 'John' }, ['user', 'user:123']);
cache.set('user:456', { name: 'Jane' }, ['user', 'user:456']);
cache.set('post:789', { title: 'Test' }, ['post', 'user:123']);

// Invalidate all user-related cache
cache.invalidateTag('user'); // Removes user:123 and user:456

// Invalidate specific user's data
cache.invalidateTag('user:123'); // Removes all cache tagged with user:123
```

---

## 37.4 Advanced Caching Patterns

### 1. Multi-Level Cache

Combine fast local cache with slower distributed cache:

```javascript
class MultiLevelCache {
  constructor(l1Cache, l2Cache) {
    this.l1 = l1Cache; // Fast (memory)
    this.l2 = l2Cache; // Slower (Redis)
  }
  
  async get(key) {
    // Try L1 first
    let value = this.l1.get(key);
    if (value !== undefined) {
      return value;
    }
    
    // Try L2
    value = await this.l2.get(key);
    if (value !== undefined) {
      // Populate L1
      this.l1.set(key, value);
      return value;
    }
    
    return null;
  }
  
  async set(key, value) {
    // Write to both levels
    this.l1.set(key, value);
    await this.l2.set(key, value);
  }
  
  async delete(key) {
    this.l1.delete(key);
    await this.l2.delete(key);
  }
}

// Usage
const l1Cache = new Map(); // In-memory
const l2Cache = {
  get: (key) => redisClient.get(key),
  set: (key, value) => redisClient.set(key, JSON.stringify(value)),
  delete: (key) => redisClient.del(key)
};

const cache = new MultiLevelCache(l1Cache, l2Cache);
```

### 2. Conditional Caching

Cache based on request characteristics:

```javascript
class ConditionalCache {
  constructor() {
    this.cache = new Map();
  }
  
  shouldCache(request) {
    // Cache GET requests only
    if (request.method !== 'GET') return false;
    
    // Don't cache authenticated requests
    if (request.headers['authorization']) return false;
    
    // Don't cache if cache-control: no-cache
    if (request.headers['cache-control']?.includes('no-cache')) {
      return false;
    }
    
    return true;
  }
  
  getCacheKey(request) {
    return `${request.method}:${request.url}:${request.query || ''}`;
  }
  
  async handle(request, handler) {
    if (!this.shouldCache(request)) {
      return handler(request);
    }
    
    const key = this.getCacheKey(request);
    
    if (this.cache.has(key)) {
      return this.cache.get(key);
    }
    
    const response = await handler(request);
    this.cache.set(key, response);
    
    return response;
  }
}
```

### 3. Distributed Cache Synchronization

Keep multiple cache instances in sync:

```javascript
class SynchronizedCache {
  constructor(localCache, pubSub) {
    this.cache = localCache;
    this.pubSub = pubSub;
    
    // Listen for invalidation messages
    this.pubSub.subscribe('cache:invalidate', (key) => {
      this.cache.delete(key);
      console.log('Remote invalidation:', key);
    });
  }
  
  set(key, value) {
    this.cache.set(key, value);
  }
  
  get(key) {
    return this.cache.get(key);
  }
  
  delete(key) {
    this.cache.delete(key);
    
    // Notify other instances
    this.pubSub.publish('cache:invalidate', key);
  }
}

// Usage with Redis pub/sub
const cache = new SynchronizedCache(
  new Map(),
  {
    subscribe: (channel, handler) => redisClient.subscribe(channel, handler),
    publish: (channel, message) => redisClient.publish(channel, message)
  }
);
```

---

## 37.5 Best Practices

1. **Choose the Right TTL**: Balance freshness vs performance
2. **Monitor Cache Hit Rates**: Aim for >80% hit rate
3. **Implement Cache Warming**: Pre-load critical data
4. **Use Cache Keys Wisely**: Include versioning in keys
5. **Handle Cache Stampede**: Use locks or probalistic early expiration
6. **Set Memory Limits**: Prevent unbounded growth
7. **Log Cache Misses**: Monitor and optimize
8. **Consider Cache Coherence**: In distributed systems

```javascript
// Example: Preventing cache stampede
class StampedeProtectedCache {
  constructor() {
    this.cache = new Map();
    this.loading = new Map(); // Track in-flight requests
  }
  
  async get(key, loader) {
    // Return cached value
    if (this.cache.has(key)) {
      return this.cache.get(key);
    }
    
    // Wait for in-flight request
    if (this.loading.has(key)) {
      return this.loading.get(key);
    }
    
    // Start new request
    const promise = loader(key).then(value => {
      this.cache.set(key, value);
      this.loading.delete(key);
      return value;
    });
    
    this.loading.set(key, promise);
    return promise;
  }
}
```