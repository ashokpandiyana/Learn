# Chapter 13: Pattern 12 - Probability DP

## Overview

Probability DP deals with computing probabilities or expected values in stochastic processes. Instead of tracking optimal solutions, we track probability distributions or expected outcomes.

**Pattern Recognition**:
- Problems involving randomness or probability
- Expected value calculations
- Markov chains and state transitions
- Game outcomes with probability
- Multiple possible outcomes from each state

**Key Difference**: Use **addition** for probabilities of disjoint events, **multiplication** for independent events.

---

## 13.1 Core Concepts

### 13.1.1 Probability Basics for DP

**Fundamental Rules**:
1. **Addition Rule**: P(A or B) = P(A) + P(B) - P(A and B)
   - For disjoint events: P(A or B) = P(A) + P(B)

2. **Multiplication Rule**: P(A and B) = P(A) × P(B|A)
   - For independent events: P(A and B) = P(A) × P(B)

3. **Total Probability**: P(B) = Σ P(B|Ai) × P(Ai)

4. **Expected Value**: E[X] = Σ xi × P(xi)

**State Definition**: `dp[state]` = probability of reaching state OR expected value at state

---

### 13.1.2 Template for Probability DP

```python
def probability_dp(initial_state, target_state):
    # Use float arrays
    dp = {}  # or array with float type
    dp[initial_state] = 1.0  # Probability 1 at start
    
    # Process states
    for state in states:
        if dp[state] == 0:
            continue
        
        # For each possible transition
        for next_state, probability in transitions(state):
            dp[next_state] += dp[state] * probability
    
    return dp[target_state]
```

---

## 13.2 Classic Problems

### 13.2.1 Knight Probability in Chessboard

**Problem**: Knight on n×n chessboard. Each step, moves randomly to valid position. Find probability knight stays on board after k moves.

**LeetCode**: #688

#### Analysis

**State**: `dp[i][j][step]` = probability of being at (i,j) after `step` moves

**Recurrence**: 
```python
dp[i][j][step] = Σ (dp[prev_i][prev_j][step-1] / 8) 
                 for all valid previous positions
```

```python
def knightProbability(n, k, row, column):
    """
    n: board size (n x n)
    k: number of moves
    row, column: starting position
    """
    # Knight moves
    moves = [
        (2, 1), (2, -1), (-2, 1), (-2, -1),
        (1, 2), (1, -2), (-1, 2), (-1, -2)
    ]
    
    # dp[r][c] = probability at position (r, c)
    dp = [[0.0] * n for _ in range(n)]
    dp[row][column] = 1.0
    
    for step in range(k):
        new_dp = [[0.0] * n for _ in range(n)]
        
        for r in range(n):
            for c in range(n):
                if dp[r][c] > 0:
                    # Try all 8 knight moves
                    for dr, dc in moves:
                        nr, nc = r + dr, c + dc
                        if 0 <= nr < n and 0 <= nc < n:
                            # Each move has probability 1/8
                            new_dp[nr][nc] += dp[r][c] / 8.0
        
        dp = new_dp
    
    # Sum all probabilities on board
    total_probability = sum(sum(row) for row in dp)
    return total_probability

# Memoization approach
from functools import cache

def knightProbability_memo(n, k, row, column):
    moves = [
        (2, 1), (2, -1), (-2, 1), (-2, -1),
        (1, 2), (1, -2), (-1, 2), (-1, -2)
    ]
    
    @cache
    def dp(r, c, steps):
        # Base case: out of bounds
        if r < 0 or r >= n or c < 0 or c >= n:
            return 0.0
        
        # Base case: no more moves
        if steps == 0:
            return 1.0
        
        # Try all moves
        probability = 0.0
        for dr, dc in moves:
            probability += dp(r + dr, c + dc, steps - 1) / 8.0
        
        return probability
    
    return dp(row, column, k)

# Test
print(knightProbability(3, 2, 0, 0))  # 0.0625
```

**Visualization**:
```
3x3 board, k=2, start=(0,0)

After move 1:
Knight can go to (1,2) or (2,1) with prob 1/8 each
Other 6 moves go off board (prob 0)

After move 2:
From (1,2): can move to various positions
From (2,1): can move to various positions
Some moves stay on board, some don't

Total probability on board after 2 moves: 0.0625
```

**Complexity**: O(n² × k) time and space

---

### 13.2.2 Soup Servings

**Problem**: Two types of soup A and B. Each operation serves soup with certain probabilities. Find probability that A empties first or both empty simultaneously.

**LeetCode**: #808

#### Analysis

**State**: `dp[a][b]` = probability that A empties first when we have `a` ml of A and `b` ml of B

```python
def soupServings(n):
    """
    n: initial amount of each soup (in ml)
    Operations:
    1. Serve 100ml A, 0ml B (prob 0.25)
    2. Serve 75ml A, 25ml B (prob 0.25)
    3. Serve 50ml A, 50ml B (prob 0.25)
    4. Serve 25ml A, 75ml B (prob 0.25)
    """
    # Optimization: for large n, probability approaches 1
    if n > 4800:
        return 1.0
    
    # Convert to units of 25ml for simplicity
    n = (n + 24) // 25
    
    from functools import cache
    
    @cache
    def dp(a, b):
        # Base cases
        if a <= 0 and b <= 0:
            return 0.5  # Both empty simultaneously
        if a <= 0:
            return 1.0  # A empties first
        if b <= 0:
            return 0.0  # B empties first
        
        # Try all 4 operations with equal probability
        prob = 0.0
        prob += dp(a - 4, b)      # Serve 100ml A, 0ml B
        prob += dp(a - 3, b - 1)  # Serve 75ml A, 25ml B
        prob += dp(a - 2, b - 2)  # Serve 50ml A, 50ml B
        prob += dp(a - 1, b - 3)  # Serve 25ml A, 75ml B
        
        return prob * 0.25
    
    return dp(n, n)

# Test
print(soupServings(50))   # 0.625
print(soupServings(100))  # 0.71875
```

**Complexity**: O(n²) time and space (with memoization)

---

### 13.2.3 New 21 Game

**Problem**: Start with 0 points. Draw cards with values 1 to maxPts. Stop when points >= k. Find probability of having <= n points when stopping.

**LeetCode**: #837

#### Analysis

**Key Insight**: Use sliding window to optimize!

**State**: `dp[i]` = probability of getting exactly i points

```python
def new21Game(n, k, maxPts):
    """
    n: target points
    k: stop threshold
    maxPts: maximum points per draw
    """
    if k == 0 or n >= k + maxPts:
        return 1.0
    
    # dp[i] = probability of getting exactly i points
    dp = [0.0] * (n + 1)
    dp[0] = 1.0
    
    # Window sum for optimization
    window_sum = 1.0
    result = 0.0
    
    for i in range(1, n + 1):
        # Probability of reaching i is average of previous maxPts positions
        dp[i] = window_sum / maxPts
        
        if i < k:
            # Can still draw from this position
            window_sum += dp[i]
        else:
            # Reached stopping condition, add to result
            result += dp[i]
        
        # Maintain sliding window
        if i >= maxPts:
            window_sum -= dp[i - maxPts]
    
    return result

# Alternative: Direct DP (slower)
def new21Game_direct(n, k, maxPts):
    if k == 0 or n >= k + maxPts:
        return 1.0
    
    dp = [0.0] * (n + 1)
    dp[0] = 1.0
    
    for i in range(1, n + 1):
        for j in range(1, min(i, maxPts) + 1):
            if i - j < k:
                dp[i] += dp[i - j] / maxPts
    
    return sum(dp[k:])

# Test
print(new21Game(10, 1, 10))  # 1.0
print(new21Game(6, 1, 10))   # 0.6
print(new21Game(21, 17, 10)) # 0.73278
```

**Complexity**: O(n + k) time, O(n) space

---

## 13.3 Expected Value Problems

### 13.3.1 Number of Dice Rolls With Target Sum

**Problem**: Roll n dice with k faces. Count ways to get sum equal to target.

**LeetCode**: #1155

This is actually a counting problem, but shows the connection to probability!

```python
def numRollsToTarget(n, k, target):
    """
    n: number of dice
    k: number of faces per die
    target: target sum
    Returns: number of ways (mod 10^9 + 7)
    """
    MOD = 10**9 + 7
    
    # dp[i][j] = ways to get sum j using i dice
    dp = [[0] * (target + 1) for _ in range(n + 1)]
    dp[0][0] = 1
    
    for i in range(1, n + 1):
        for j in range(i, min(target, i * k) + 1):
            # Try each face value
            for face in range(1, k + 1):
                if j >= face:
                    dp[i][j] = (dp[i][j] + dp[i - 1][j - face]) % MOD
    
    return dp[n][target]

# Space optimized
def numRollsToTarget_optimized(n, k, target):
    MOD = 10**9 + 7
    dp = [0] * (target + 1)
    dp[0] = 1
    
    for i in range(1, n + 1):
        new_dp = [0] * (target + 1)
        for j in range(i, min(target, i * k) + 1):
            for face in range(1, k + 1):
                if j >= face:
                    new_dp[j] = (new_dp[j] + dp[j - face]) % MOD
        dp = new_dp
    
    return dp[target]

# Test
print(numRollsToTarget(1, 6, 3))   # 1
print(numRollsToTarget(2, 6, 7))   # 6
print(numRollsToTarget(30, 30, 500)) # 222616187
```

**Probability Version**:
```python
def probability_of_target(n, k, target):
    """
    Probability of getting target sum with n dice of k faces
    """
    ways = numRollsToTarget(n, k, target)
    total_outcomes = k ** n
    return ways / total_outcomes
```

**Complexity**: O(n × target × k) time, O(target) space optimized

---

### 13.3.2 Minimum Number of Flips

**Problem**: Expected number of coin flips to get n consecutive heads.

#### Analysis

**State**: `dp[i]` = expected flips to get i consecutive heads

**Recurrence**:
```
E[i] = 0.5 × (1 + E[i+1])  [heads: move to i+1]
     + 0.5 × (1 + E[0])     [tails: restart]
```

```python
def expected_flips_consecutive_heads(n):
    """
    Expected number of flips to get n consecutive heads
    """
    # dp[i] = expected flips to go from i consecutive heads to n
    dp = [0.0] * (n + 1)
    
    # Work backwards from n-1 to 0
    for i in range(n - 1, -1, -1):
        # From state i:
        # - Flip heads (prob 0.5): move to i+1, cost 1 flip
        # - Flip tails (prob 0.5): restart to 0, cost 1 flip
        dp[i] = 1 + 0.5 * dp[i + 1] + 0.5 * dp[0]
    
    # Solve for dp[0]
    # dp[0] = 1 + 0.5 * dp[1] + 0.5 * dp[0]
    # 0.5 * dp[0] = 1 + 0.5 * dp[1]
    # dp[0] = 2 + dp[1]
    
    # Actually, solve system of equations
    # E[i] = 1 + 0.5*E[i+1] + 0.5*E[0] for i=0 to n-1
    # E[n] = 0
    
    # Closed form: E[0] = 2^(n+1) - 2
    return 2 ** (n + 1) - 2

# Test
print(expected_flips_consecutive_heads(1))  # 2
print(expected_flips_consecutive_heads(2))  # 6
print(expected_flips_consecutive_heads(3))  # 14
```

---

## 13.4 Markov Chains

### 13.4.1 Random Walk

**Problem**: Start at position 0. Each step, move +1 or -1 with equal probability. Find probability of reaching position n before position -m.

```python
def random_walk_probability(n, m):
    """
    Probability of reaching n before -m from position 0
    Using Markov chain analysis
    """
    # For symmetric random walk:
    # P(reach n before -m from 0) = m / (m + n)
    return m / (m + n)

# DP approach for general case
def random_walk_dp(n, m, prob_right=0.5):
    """
    prob_right: probability of moving right (default 0.5)
    """
    # dp[i] = probability of reaching n before -m from position i
    # Range: positions from -m to n
    positions = m + n + 1
    dp = [0.0] * positions
    
    # Boundary conditions
    dp[n + m] = 1.0  # Position n (reached target)
    dp[0] = 0.0      # Position -m (failed)
    
    # Iterate until convergence (or use linear algebra)
    for _ in range(1000):  # Sufficient iterations
        new_dp = dp[:]
        for i in range(1, n + m):
            # Position i (actual position = i - m)
            new_dp[i] = prob_right * dp[i + 1] + (1 - prob_right) * dp[i - 1]
        
        # Check convergence
        if all(abs(new_dp[i] - dp[i]) < 1e-9 for i in range(positions)):
            break
        
        dp = new_dp
    
    return dp[m]  # Starting position 0 is at index m

# Test
print(random_walk_probability(5, 5))   # 0.5
print(random_walk_probability(3, 7))   # 0.7
print(random_walk_dp(5, 5))            # ~0.5
```

---

## 13.5 Advanced Probability DP

### 13.5.1 Broken Calculator

**Problem**: Calculator with display showing X. Operations: multiply by 2 or subtract 1. Minimum operations to show Y.

**LeetCode**: #991

**Trick**: Work backwards from Y!

```python
def brokenCalc(startValue, target):
    """
    Operations: X -> 2*X or X -> X-1
    Work backwards: Y -> Y//2 or Y -> Y+1
    """
    operations = 0
    
    while target > startValue:
        if target % 2 == 0:
            # Undo multiply by 2
            target //= 2
        else:
            # Undo subtract 1 (so add 1)
            target += 1
        operations += 1
    
    # If target <= startValue, just subtract
    operations += startValue - target
    
    return operations

# Test
print(brokenCalc(2, 3))    # 2 (2 -> 4 -> 3)
print(brokenCalc(5, 8))    # 2 (5 -> 10 -> 9 -> 8)
print(brokenCalc(3, 10))   # 3 (3 -> 6 -> 5 -> 10)
```

---

## 13.6 Common Patterns

### Pattern 1: Forward Probability Propagation
```python
# Start with probability 1 at initial state
dp[initial] = 1.0

for state in states:
    for next_state, transition_prob in transitions(state):
        dp[next_state] += dp[state] * transition_prob
```

### Pattern 2: Backward Expected Value
```python
# Start from end states
dp[end_states] = 0.0  # or known values

for state in reversed(states):
    dp[state] = sum(
        (value + dp[next_state]) * probability
        for next_state, probability, value in transitions(state)
    )
```

### Pattern 3: Sliding Window Optimization
```python
# For problems where current state depends on window of previous states
window_sum = sum(dp[i-w:i])
dp[i] = window_sum / window_size
```

---

## 13.7 Precision and Numerical Stability

### Important Considerations

```python
# 1. Use float, not int
dp = [0.0] * n  # Good
# dp = [0] * n  # Bad for probabilities

# 2. Handle very small probabilities
if prob < 1e-10:
    continue  # Skip negligible probabilities

# 3. Use log probabilities for products
log_prob = sum(log(p) for p in probabilities)
prob = exp(log_prob)

# 4. Normalize when needed
total = sum(dp)
if total > 0:
    dp = [p / total for p in dp]

# 5. Set precision threshold
EPSILON = 1e-9
if abs(a - b) < EPSILON:
    # Consider equal
```

---

## 13.8 Common Pitfalls

### Pitfall 1: Integer Division
```python
# WRONG
prob = 1 / 2  # In Python 2, this is 0!

# CORRECT
prob = 1.0 / 2  # or use from __future__ import division
prob = 1 / 2.0
```

### Pitfall 2: Forgetting Normalization
```python
# After many operations, probabilities might not sum to 1
total = sum(dp.values())
dp = {k: v/total for k, v in dp.items()}
```

### Pitfall 3: Dependency Cycles
```python
# WRONG: Circular dependency
dp[i] = 0.5 * dp[i+1] + 0.5 * dp[i-1]

# CORRECT: Use iteration or linear algebra
```

---

## Practice Problems Summary

### Beginner
1. Coin Toss Probability (basic)
2. Dice Roll Probability (#1155)
3. Random Pick with Weight (#528)

### Intermediate
4. Knight Probability in Chessboard (#688)
5. Soup Servings (#808)
6. New 21 Game (#837)

### Advanced
7. Minimum Cost to Merge Stones with Probability
8. Expected Value of Expression
9. Markov Chain Steady State

---

## Key Takeaways

1. **Use float arrays** for probability values
2. **Forward propagation**: Start with prob=1.0 at initial state
3. **Backward computation**: For expected values
4. **Multiplication** for independent events, **addition** for disjoint events
5. **Sliding window** optimization when applicable
6. **Memoization** is crucial for recursive probability problems
7. **Markov chains**: State transitions with probabilities
8. **Expected value**: E[X] = Σ xi × P(xi)
9. **Convergence**: Iterate until probabilities stabilize
10. **Numerical stability**: Watch for underflow/overflow

Probability DP requires careful handling of floating-point arithmetic and understanding of probability theory!