# Chapter 7: Interview-Specific Topics - Deep Dive

## 7.1 LRU Cache Implementation ⭐⭐⭐

### What is LRU Cache?

**LRU (Least Recently Used) Cache** is a cache eviction policy that removes the least recently used items first when the cache reaches its capacity.

**Real-World Examples:**
- Browser cache
- Database query cache
- Operating system page replacement
- CDN caching strategies

### Why LinkedList + HashMap?

```
Requirements for LRU Cache:
1. get(key): O(1) ✓ (HashMap provides this)
2. put(key, value): O(1) ✓ (HashMap provides this)
3. Track access order: O(1) ✓ (Doubly LinkedList provides this)
4. Remove LRU item: O(1) ✓ (Doubly LinkedList with direct node access)

HashMap alone: Can't track order efficiently
LinkedList alone: Can't lookup in O(1)
Together: Perfect combination! ✓
```

### Visual Representation

```
LRU Cache (capacity = 3):

Initial: Empty
put(1, "A"): 
    LinkedList: [1:A]
    HashMap: {1: node_ref}

put(2, "B"):
    LinkedList: [2:B] ⇄ [1:A]  (most recent ← → least recent)
    HashMap: {1: node_ref, 2: node_ref}

put(3, "C"):
    LinkedList: [3:C] ⇄ [2:B] ⇄ [1:A]
    HashMap: {1: node_ref, 2: node_ref, 3: node_ref}

get(1):  # Access 1, move to front
    LinkedList: [1:A] ⇄ [3:C] ⇄ [2:B]
    HashMap: {1: node_ref, 2: node_ref, 3: node_ref}

put(4, "D"):  # Cache full! Remove LRU (2:B)
    LinkedList: [4:D] ⇄ [1:A] ⇄ [3:C]
    HashMap: {1: node_ref, 3: node_ref, 4: node_ref}
```

### Complete Implementation

```python
class Node:
    """Doubly Linked List Node"""
    def __init__(self, key=0, value=0):
        self.key = key
        self.value = value
        self.prev = None
        self.next = None

class LRUCache:
    """
    LRU Cache with O(1) get and put operations
    
    Components:
    1. HashMap: {key -> Node} for O(1) lookup
    2. Doubly LinkedList: maintains access order
       - Head: Most recently used
       - Tail: Least recently used
    """
    
    def __init__(self, capacity: int):
        self.capacity = capacity
        self.cache = {}  # key -> Node mapping
        
        # Dummy head and tail nodes to simplify edge cases
        self.head = Node()  # Dummy head
        self.tail = Node()  # Dummy tail
        self.head.next = self.tail
        self.tail.prev = self.head
    
    def _add_to_head(self, node):
        """
        Add node right after head (most recent position)
        
        Before: head ⇄ old_first ⇄ ... ⇄ tail
        After:  head ⇄ node ⇄ old_first ⇄ ... ⇄ tail
        """
        node.prev = self.head
        node.next = self.head.next
        
        self.head.next.prev = node
        self.head.next = node
    
    def _remove_node(self, node):
        """
        Remove node from its current position
        
        Before: ... ⇄ prev ⇄ node ⇄ next ⇄ ...
        After:  ... ⇄ prev ⇄ next ⇄ ...
        """
        prev_node = node.prev
        next_node = node.next
        
        prev_node.next = next_node
        next_node.prev = prev_node
    
    def _move_to_head(self, node):
        """
        Move existing node to head (mark as most recently used)
        """
        self._remove_node(node)
        self._add_to_head(node)
    
    def _remove_tail(self):
        """
        Remove and return the LRU node (node before dummy tail)
        """
        lru_node = self.tail.prev
        self._remove_node(lru_node)
        return lru_node
    
    def get(self, key: int) -> int:
        """
        Get value by key. Mark as recently used.
        Time: O(1)
        Space: O(1)
        """
        if key not in self.cache:
            return -1
        
        # Move accessed node to head (most recent)
        node = self.cache[key]
        self._move_to_head(node)
        
        return node.value
    
    def put(self, key: int, value: int) -> None:
        """
        Put key-value pair. Add new or update existing.
        Time: O(1)
        Space: O(1)
        """
        if key in self.cache:
            # Update existing node
            node = self.cache[key]
            node.value = value
            self._move_to_head(node)
        else:
            # Create new node
            new_node = Node(key, value)
            self.cache[key] = new_node
            self._add_to_head(new_node)
            
            # Check capacity
            if len(self.cache) > self.capacity:
                # Remove LRU node
                lru = self._remove_tail()
                del self.cache[lru.key]
    
    def display(self):
        """Display cache state (for debugging)"""
        print(f"Cache (capacity={self.capacity}, size={len(self.cache)}):")
        current = self.head.next
        nodes = []
        while current != self.tail:
            nodes.append(f"[{current.key}:{current.value}]")
            current = current.next
        print("MRU -> " + " ⇄ ".join(nodes) + " -> LRU")
        print()


# ===== Usage Examples =====

def test_lru_cache():
    """Test LRU Cache with detailed output"""
    
    print("=== LRU Cache Demo ===\n")
    
    cache = LRUCache(3)
    
    print("1. put(1, 'A')")
    cache.put(1, 'A')
    cache.display()
    
    print("2. put(2, 'B')")
    cache.put(2, 'B')
    cache.display()
    
    print("3. put(3, 'C')")
    cache.put(3, 'C')
    cache.display()
    
    print("4. get(1) - Access key 1")
    result = cache.get(1)
    print(f"   Result: {result}")
    cache.display()
    
    print("5. put(4, 'D') - Cache full, evict LRU")
    cache.put(4, 'D')
    cache.display()
    
    print("6. get(2) - Try to access evicted key")
    result = cache.get(2)
    print(f"   Result: {result} (not found)")
    cache.display()
    
    print("7. put(5, 'E') - Evict LRU again")
    cache.put(5, 'E')
    cache.display()

test_lru_cache()
```

### Output Walkthrough

```
=== LRU Cache Demo ===

1. put(1, 'A')
Cache (capacity=3, size=1):
MRU -> [1:A] -> LRU

2. put(2, 'B')
Cache (capacity=3, size=2):
MRU -> [2:B] ⇄ [1:A] -> LRU

3. put(3, 'C')
Cache (capacity=3, size=3):
MRU -> [3:C] ⇄ [2:B] ⇄ [1:A] -> LRU

4. get(1) - Access key 1
   Result: A
Cache (capacity=3, size=3):
MRU -> [1:A] ⇄ [3:C] ⇄ [2:B] -> LRU

5. put(4, 'D') - Cache full, evict LRU
Cache (capacity=3, size=3):
MRU -> [4:D] ⇄ [1:A] ⇄ [3:C] -> LRU
(Note: [2:B] was evicted as LRU)

6. get(2) - Try to access evicted key
   Result: -1 (not found)
Cache (capacity=3, size=3):
MRU -> [4:D] ⇄ [1:A] ⇄ [3:C] -> LRU

7. put(5, 'E') - Evict LRU again
Cache (capacity=3, size=3):
MRU -> [5:E] ⇄ [4:D] ⇄ [1:A] -> LRU
(Note: [3:C] was evicted as LRU)
```

### Advanced: LRU Cache with Expiration

```python
import time

class LRUCacheWithExpiration:
    """LRU Cache with time-based expiration"""
    
    class Node:
        def __init__(self, key, value, ttl):
            self.key = key
            self.value = value
            self.expiry_time = time.time() + ttl if ttl else None
            self.prev = None
            self.next = None
        
        def is_expired(self):
            if self.expiry_time is None:
                return False
            return time.time() > self.expiry_time
    
    def __init__(self, capacity: int, default_ttl=None):
        self.capacity = capacity
        self.default_ttl = default_ttl
        self.cache = {}
        self.head = self.Node(0, 0, None)
        self.tail = self.Node(0, 0, None)
        self.head.next = self.tail
        self.tail.prev = self.head
    
    def get(self, key: int) -> int:
        """Get with expiration check"""
        if key not in self.cache:
            return -1
        
        node = self.cache[key]
        
        # Check if expired
        if node.is_expired():
            self._remove_node(node)
            del self.cache[key]
            return -1
        
        self._move_to_head(node)
        return node.value
    
    def put(self, key: int, value: int, ttl=None) -> None:
        """Put with optional TTL"""
        ttl = ttl if ttl is not None else self.default_ttl
        
        if key in self.cache:
            node = self.cache[key]
            node.value = value
            node.expiry_time = time.time() + ttl if ttl else None
            self._move_to_head(node)
        else:
            new_node = self.Node(key, value, ttl)
            self.cache[key] = new_node
            self._add_to_head(new_node)
            
            if len(self.cache) > self.capacity:
                lru = self._remove_tail()
                del self.cache[lru.key]
    
    # Include same helper methods as before
    def _add_to_head(self, node):
        node.prev = self.head
        node.next = self.head.next
        self.head.next.prev = node
        self.head.next = node
    
    def _remove_node(self, node):
        prev_node = node.prev
        next_node = node.next
        prev_node.next = next_node
        next_node.prev = prev_node
    
    def _move_to_head(self, node):
        self._remove_node(node)
        self._add_to_head(node)
    
    def _remove_tail(self):
        lru_node = self.tail.prev
        self._remove_node(lru_node)
        return lru_node
```

### Interview Tips for LRU Cache

**Common Follow-up Questions:**

1. **"What if we need LFU (Least Frequently Used) instead?"**
   - Need to track frequency count
   - Use HashMap + Doubly LinkedList per frequency level
   
2. **"How would you handle thread safety?"**
   - Add locks/mutexes around get/put operations
   - Use concurrent data structures
   
3. **"What about TTL (Time To Live)?"**
   - Add expiry_time field to Node
   - Check expiration on get operations
   
4. **"How to optimize for read-heavy workloads?"**
   - Consider read-write locks
   - Use copy-on-write strategies

---

## 7.2 Skip List

### What is a Skip List?

A **Skip List** is a probabilistic data structure that allows O(log n) search, insertion, and deletion on average. It's an alternative to balanced trees like AVL or Red-Black trees.

### Structure

```
Level 3:  HEAD --------------------------------> 30 -----------------> NULL
Level 2:  HEAD --------> 10 ---> 20 ---> 30 ---> 40 ---> 50 ---------> NULL  
Level 1:  HEAD --> 5 --> 10 --> 15 --> 20 --> 30 --> 40 --> 50 --> 60 --> NULL
Level 0:  HEAD --> 5 --> 10 --> 15 --> 20 --> 30 --> 40 --> 50 --> 60 --> NULL

Vertical connections: Node can appear in multiple levels
Horizontal connections: Standard linked list at each level
Higher levels: "Express lanes" - skip intermediate nodes
```

### Key Concepts

1. **Levels**: Each node has a random level (height)
2. **Express Lanes**: Higher levels skip more nodes
3. **Probabilistic Balance**: Uses randomization instead of rotations
4. **Search**: Start from top, move right/down

### Implementation

```python
import random

class SkipNode:
    """Node in Skip List"""
    def __init__(self, value, level):
        self.value = value
        self.forward = [None] * (level + 1)  # Array of forward pointers

class SkipList:
    """
    Skip List implementation
    
    Time Complexity (Average):
    - Search: O(log n)
    - Insert: O(log n)
    - Delete: O(log n)
    
    Space: O(n)
    """
    
    def __init__(self, max_level=16, p=0.5):
        self.max_level = max_level
        self.p = p  # Probability for level generation
        self.level = 0  # Current max level
        
        # Header node (sentinel)
        self.header = SkipNode(float('-inf'), max_level)
    
    def random_level(self):
        """
        Generate random level for new node
        Each level has probability p of being selected
        """
        level = 0
        while random.random() < self.p and level < self.max_level:
            level += 1
        return level
    
    def search(self, target):
        """
        Search for a value
        Time: O(log n) average
        
        Algorithm:
        1. Start from top-left (highest level)
        2. Move right if next value < target
        3. Move down if next value >= target or None
        4. Repeat until level 0
        """
        current = self.header
        
        # Traverse from top level to bottom
        for i in range(self.level, -1, -1):
            # Move right while next value < target
            while current.forward[i] and current.forward[i].value < target:
                current = current.forward[i]
        
        # Move to level 0
        current = current.forward[0]
        
        # Check if found
        if current and current.value == target:
            return True
        return False
    
    def insert(self, value):
        """
        Insert a value
        Time: O(log n) average
        
        Algorithm:
        1. Find insertion point (like search)
        2. Generate random level
        3. Create new node
        4. Update pointers at each level
        """
        # Array to store update pointers
        update = [None] * (self.max_level + 1)
        current = self.header
        
        # Find insertion point and store update pointers
        for i in range(self.level, -1, -1):
            while current.forward[i] and current.forward[i].value < value:
                current = current.forward[i]
            update[i] = current
        
        # Generate random level for new node
        new_level = self.random_level()
        
        # Update list level if needed
        if new_level > self.level:
            for i in range(self.level + 1, new_level + 1):
                update[i] = self.header
            self.level = new_level
        
        # Create new node
        new_node = SkipNode(value, new_level)
        
        # Update forward pointers
        for i in range(new_level + 1):
            new_node.forward[i] = update[i].forward[i]
            update[i].forward[i] = new_node
    
    def delete(self, value):
        """
        Delete a value
        Time: O(log n) average
        """
        update = [None] * (self.max_level + 1)
        current = self.header
        
        # Find node to delete
        for i in range(self.level, -1, -1):
            while current.forward[i] and current.forward[i].value < value:
                current = current.forward[i]
            update[i] = current
        
        current = current.forward[0]
        
        # If found, remove it
        if current and current.value == value:
            for i in range(self.level + 1):
                if update[i].forward[i] != current:
                    break
                update[i].forward[i] = current.forward[i]
            
            # Update list level
            while self.level > 0 and self.header.forward[self.level] is None:
                self.level -= 1
            
            return True
        return False
    
    def display(self):
        """Display skip list structure"""
        print("\n=== Skip List Structure ===")
        for level in range(self.level, -1, -1):
            print(f"Level {level}: ", end="")
            node = self.header.forward[level]
            while node:
                print(f"{node.value} -> ", end="")
                node = node.forward[level]
            print("NULL")
        print()

# ===== Usage Example =====

def test_skip_list():
    """Test Skip List operations"""
    
    print("=== Skip List Demo ===\n")
    
    sl = SkipList(max_level=4, p=0.5)
    
    # Insert elements
    print("Inserting: 3, 6, 7, 9, 12, 19, 17, 26, 21, 25")
    for val in [3, 6, 7, 9, 12, 19, 17, 26, 21, 25]:
        sl.insert(val)
    
    sl.display()
    
    # Search
    print("Search Operations:")
    for val in [7, 15, 19]:
        found = sl.search(val)
        print(f"  Search {val}: {'Found' if found else 'Not found'}")
    
    # Delete
    print("\nDeleting 19...")
    sl.delete(19)
    sl.display()
    
    print("Searching for 19 after deletion:")
    found = sl.search(19)
    print(f"  Search 19: {'Found' if found else 'Not found'}")

test_skip_list()
```

### Skip List vs Balanced Trees

| Feature | Skip List | Balanced Tree |
|---------|-----------|---------------|
| Implementation | Simpler | Complex (rotations) |
| Average case | O(log n) | O(log n) |
| Worst case | O(n) | O(log n) |
| Space | O(n) | O(n) |
| Concurrent access | Easier | Harder |
| Cache performance | Better | Worse |

### When to Use Skip List

✅ **Use Skip List when:**
- Simpler code preferred over guaranteed worst-case
- Building concurrent data structures
- Need range queries
- Memory access patterns matter

❌ **Use Balanced Tree when:**
- Need guaranteed O(log n) worst-case
- Formal proof of correctness required
- Teaching/learning tree algorithms

---

## 7.3 Memory Management

### Python's Garbage Collection

```python
"""
Python uses automatic garbage collection:
1. Reference counting (primary)
2. Generational garbage collection (secondary)
3. Cycle detection (for circular references)
"""

# Example: Reference counting
node1 = ListNode(10)  # Reference count = 1
node2 = node1          # Reference count = 2
del node1              # Reference count = 1
node2 = None           # Reference count = 0 -> Garbage collected
```

### Circular References

```python
class CircularReferenceExample:
    """Demonstrating circular reference handling"""
    
    def create_cycle(self):
        """Create circular reference"""
        node1 = ListNode(1)
        node2 = ListNode(2)
        node3 = ListNode(3)
        
        node1.next = node2
        node2.next = node3
        node3.next = node1  # Cycle!
        
        # Even if we lose reference to these nodes,
        # Python's cycle detector will eventually clean them up
        return node1
    
    def proper_cleanup(self, head):
        """Manually break cycles (good practice)"""
        if not head:
            return
        
        current = head
        while current:
            next_node = current.next
            current.next = None  # Break link
            current = next_node

# Python handles this automatically, but breaking cycles
# explicitly is good practice for clarity
```

### Memory-Efficient Techniques

```python
# 1. Use __slots__ to reduce memory
class MemoryEfficientNode:
    """
    Using __slots__ reduces memory by ~40%
    No __dict__ created per instance
    """
    __slots__ = ['val', 'next']
    
    def __init__(self, val=0, next=None):
        self.val = val
        self.next = next

# Memory comparison
import sys

regular_node = ListNode(10)
efficient_node = MemoryEfficientNode(10)

print(f"Regular node size: {sys.getsizeof(regular_node)} bytes")
print(f"Efficient node size: {sys.getsizeof(efficient_node)} bytes")

# 2. Generator for memory-efficient traversal
def traverse_generator(head):
    """
    Yield values instead of creating list
    Memory: O(1) instead of O(n)
    """
    current = head
    while current:
        yield current.val
        current = current.next

# Usage
head = create_linked_list([1, 2, 3, 4, 5])
for val in traverse_generator(head):
    print(val)  # Process one at a time
```

### Other Languages (C++, Java)

```cpp
// C++ Example - Manual memory management required

class ListNode {
public:
    int val;
    ListNode* next;
    
    ListNode(int v) : val(v), next(nullptr) {}
};

class LinkedList {
private:
    ListNode* head;
    
public:
    LinkedList() : head(nullptr) {}
    
    // IMPORTANT: Destructor to prevent memory leaks
    ~LinkedList() {
        while (head != nullptr) {
            ListNode* temp = head;
            head = head->next;
            delete temp;  // Must manually free memory!
        }
    }
    
    void insert(int val) {
        ListNode* newNode = new ListNode(val);
        newNode->next = head;
        head = newNode;
    }
};

// Common mistakes in C++:
// ❌ Forgetting to delete nodes
// ❌ Deleting node still referenced elsewhere (dangling pointer)
// ❌ Not handling cycles properly
```

```java
// Java Example - Garbage collected but still needs care

class ListNode {
    int val;
    ListNode next;
    
    ListNode(int val) {
        this.val = val;
    }
}

class LinkedList {
    private ListNode head;
    
    // No destructor needed - Java GC handles it
    
    public void clear() {
        // Help GC by breaking references
        while (head != null) {
            ListNode next = head.next;
            head.next = null;  // Break link
            head = next;
        }
    }
}
```

---

## Interview Scenarios

### Scenario 1: Design Questions

**Q: "Design a data structure that supports insert, delete, and getRandom in O(1)."**

```python
class RandomizedSet:
    """
    Combines HashMap and ArrayList
    - HashMap: {value -> index in list}
    - ArrayList: stores values
    """
    
    def __init__(self):
        self.data = []
        self.pos = {}  # value -> index
    
    def insert(self, val):
        if val in self.pos:
            return False
        self.data.append(val)
        self.pos[val] = len(self.data) - 1
        return True
    
    def remove(self, val):
        if val not in self.pos:
            return False
        
        # Swap with last element
        idx = self.pos[val]
        last = self.data[-1]
        
        self.data[idx] = last
        self.pos[last] = idx
        
        self.data.pop()
        del self.pos[val]
        return True
    
    def getRandom(self):
        return random.choice(self.data)
```

### Scenario 2: System Design

**Q: "How would you implement a cache in a distributed system?"**

Answer considerations:
- Consistency (eventual vs strong)
- Partitioning strategy (consistent hashing)
- Replication factor
- Eviction policy (LRU, LFU, TTL)
- Monitoring and metrics

---

## Key Takeaways

1. **LRU Cache**: Master this - appears in 60% of cache-related interviews
2. **Skip List**: Understand the concept, implementation less critical
3. **Memory Management**: Know your language's model (GC vs manual)
4. **Design Patterns**: Combine data structures for optimal solutions

These topics demonstrate deep understanding of data structures and system design!