# Chapter 10: Time Complexity Cheatsheet - Deep Dive

## Understanding Big-O Notation

### Visual Growth Rates

```
Operations needed for input size n:

n = 100:
O(1):      1 operation       ⚡ Instant
O(log n):  ~7 operations     ⚡ Very fast
O(n):      100 operations    ✓ Fast
O(n log n): ~664 operations  ✓ Good
O(n²):     10,000 operations ⚠️ Slow
O(2ⁿ):     1.27×10³⁰ ops     ❌ Impossible

For LinkedList of 1000 nodes:
O(1):      1 step            → Access tail with pointer
O(log n):  ~10 steps         → Skip list search
O(n):      1000 steps        → Linear search
O(n log n): ~10,000 steps    → Merge sort
O(n²):     1,000,000 steps   → Nested loops
```

---

## 10.1 Basic Operations Complexity

### Comprehensive Operations Table

```python
"""
Operation                    | Singly    | Doubly    | Circular  | Array     |
----------------------------|-----------|-----------|-----------|-----------|
Access by Index             | O(n)      | O(n)      | O(n)      | O(1) ⚡   |
Search by Value             | O(n)      | O(n)      | O(n)      | O(n)      |
Insert at Head              | O(1) ⚡    | O(1) ⚡    | O(1) ⚡    | O(n)      |
Insert at Tail (no ptr)     | O(n)      | O(n)      | O(n)      | O(1)* ⚡   |
Insert at Tail (with ptr)   | O(1) ⚡    | O(1) ⚡    | O(1) ⚡    | O(1)* ⚡   |
Insert at Middle            | O(n)      | O(n)      | O(n)      | O(n)      |
Delete at Head              | O(1) ⚡    | O(1) ⚡    | O(1) ⚡    | O(n)      |
Delete at Tail (no ptr)     | O(n)      | O(n)      | O(n)      | O(1) ⚡   |
Delete at Tail (with ptr)   | O(n)      | O(1) ⚡    | O(n)      | O(1) ⚡   |
Delete at Middle            | O(n)      | O(n)      | O(n)      | O(n)      |
Delete Given Node           | O(n)†     | O(1) ⚡    | O(n)†     | O(n)      |
Reverse                     | O(n)      | O(n)      | O(n)      | O(n)      |
Sort                        | O(n log n)| O(n log n)| O(n log n)| O(n log n)|
Find Middle                 | O(n)      | O(n)      | O(n)      | O(1) ⚡   |
Find Kth from End           | O(n)      | O(n)      | O(n)      | O(1) ⚡   |

Legend:
⚡ = Very efficient
* = Amortized (occasionally O(n) for resize)
† = O(1) if previous node known, O(n) to find it
"""
```

### Detailed Analysis

```python
# 1. Access by Index - O(n)
def access_by_index(head, index):
    """
    Why O(n)? Must traverse from head
    Best case: O(1) - index 0
    Worst case: O(n) - last index
    Average: O(n/2) = O(n)
    """
    current = head
    for _ in range(index):
        if not current:
            return None
        current = current.next
    return current.val if current else None

# Time breakdown for list of size n:
"""
Index 0:     1 step
Index n/2:   n/2 steps
Index n-1:   n-1 steps
Average:     (1 + 2 + ... + n) / n = n/2 ≈ O(n)
"""

# 2. Insert at Head - O(1)
def insert_at_head(head, val):
    """
    Why O(1)? Fixed number of operations
    - Create new node: 1 operation
    - Set new_node.next: 1 operation
    - Update head: 1 operation
    Total: 3 operations (constant!)
    """
    new_node = ListNode(val)
    new_node.next = head
    return new_node

# 3. Insert at Tail - O(n) without tail pointer
def insert_at_tail_slow(head, val):
    """
    Why O(n)? Must traverse entire list
    """
    if not head:
        return ListNode(val)
    
    current = head
    while current.next:  # O(n) traversal
        current = current.next
    
    current.next = ListNode(val)
    return head

# vs O(1) with tail pointer
def insert_at_tail_fast(self, val):
    """
    Why O(1)? Direct access to tail
    """
    new_node = ListNode(val)
    if not self.tail:
        self.head = self.tail = new_node
    else:
        self.tail.next = new_node
        self.tail = new_node

# 4. Delete at Tail - Different complexities
def delete_tail_singly(head):
    """
    Singly: O(n) - Need previous node
    Must traverse to second-to-last
    """
    if not head or not head.next:
        return None
    
    current = head
    while current.next.next:  # O(n)
        current = current.next
    
    current.next = None
    return head

def delete_tail_doubly(tail):
    """
    Doubly: O(1) - Have prev pointer
    Direct access to previous node
    """
    if not tail or not tail.prev:
        return None
    
    tail.prev.next = None
    return tail.prev
```

---

## 10.2 Common Patterns Complexity

### Pattern-Based Analysis

```python
"""
Pattern                          | Time      | Space     | Why?
---------------------------------|-----------|-----------|------------------
Two Pointers (Fast-Slow)         | O(n)      | O(1)      | Single pass
Two Pointers (Gap)               | O(n)      | O(1)      | Single pass
Dummy Node                       | O(n)*     | O(1)      | Depends on logic
Reversal (Iterative)             | O(n)      | O(1)      | Visit each once
Reversal (Recursive)             | O(n)      | O(n)      | Call stack
Merge Two Lists                  | O(n+m)    | O(1)      | Visit all nodes
Merge K Lists (Heap)             | O(N log k)| O(k)      | N nodes, k lists
Merge K Lists (D&C)              | O(N log k)| O(log k)  | Divide & conquer
Cycle Detection                  | O(n)      | O(1)      | Meet in cycle
Find Middle                      | O(n)      | O(1)      | Half traversal
Palindrome Check                 | O(n)      | O(1)      | Reverse + compare
LRU Cache get/put                | O(1)      | O(n)      | HashMap + DLL
Skip List operations             | O(log n)† | O(n)      | Probabilistic
Sort (Merge)                     | O(n log n)| O(log n)  | Divide & conquer
Sort (Insertion)                 | O(n²)     | O(1)      | Nested comparisons

* Depends on operation performed
† Average case; worst case O(n)
"""
```

### Detailed Pattern Analysis

```python
# 1. Two Pointers (Fast-Slow) - O(n), O(1)
def find_middle(head):
    """
    Analysis:
    - Fast moves 2 steps per iteration
    - Slow moves 1 step per iteration
    - Loop runs n/2 times
    - Total steps: n/2 (slow) + n (fast) = 3n/2 ≈ O(n)
    - Space: 2 pointers = O(1)
    """
    slow = fast = head
    iterations = 0
    
    while fast and fast.next:
        slow = slow.next      # 1 step
        fast = fast.next.next # 2 steps
        iterations += 1
    
    # For list of length 10:
    # Iterations = 5
    # Slow moves = 5, Fast moves = 10
    # Total = 15 ≈ O(n)
    
    return slow

# 2. Merge K Lists - O(N log k) analysis
def merge_k_lists_heap(lists):
    """
    N = total number of nodes
    k = number of lists
    
    Analysis:
    - Heap operations: O(log k)
    - Performed N times (once per node)
    - Total: O(N log k)
    - Space: O(k) for heap
    
    Example: k=3 lists, 100 nodes each
    N = 300, k = 3
    Time: 300 × log(3) ≈ 300 × 1.6 = 480 operations
    """
    import heapq
    
    heap = []
    # Initial heap: O(k log k)
    for i, lst in enumerate(lists):
        if lst:
            heapq.heappush(heap, (lst.val, i, lst))
    
    dummy = ListNode(0)
    current = dummy
    
    # Main loop: O(N log k)
    while heap:
        val, i, node = heapq.heappop(heap)  # O(log k)
        current.next = node
        current = current.next
        
        if node.next:
            heapq.heappush(heap, (node.next.val, i, node.next))  # O(log k)
    
    return dummy.next

# 3. Palindrome Check - O(n), O(1)
def is_palindrome(head):
    """
    Analysis - Three phases:
    1. Find middle: O(n/2)
    2. Reverse second half: O(n/2)
    3. Compare: O(n/2)
    Total: 3 × n/2 = 3n/2 ≈ O(n)
    Space: O(1) - only pointers
    """
    # Phase 1: Find middle - O(n/2)
    slow = fast = head
    while fast and fast.next:
        slow = slow.next
        fast = fast.next.next
    
    # Phase 2: Reverse - O(n/2)
    prev = None
    while slow:
        temp = slow.next
        slow.next = prev
        prev = slow
        slow = temp
    
    # Phase 3: Compare - O(n/2)
    left, right = head, prev
    while right:
        if left.val != right.val:
            return False
        left = left.next
        right = right.next
    
    return True
```

---

## 10.3 Space Complexity Analysis

### Memory Usage Breakdown

```python
"""
Data Structure          | Per Node Memory      | n Nodes Total  |
------------------------|---------------------|----------------|
Singly LinkedList       | val + 1 pointer     | n × (8+8) = 16n bytes |
Doubly LinkedList       | val + 2 pointers    | n × (8+16) = 24n bytes |
Array (Python list)     | val only            | n × 8 = 8n bytes* |
Skip List              | val + level pointers | n × (8+8×level) bytes |

* Plus overhead for dynamic array
Note: Actual Python object overhead adds ~28 bytes per object
"""
```

### Space Complexity Categories

```python
# O(1) Space - Constant
def reverse_iterative(head):
    """
    Uses only 3 pointers regardless of list size
    prev, current, next_temp = 3 pointers
    3 × 8 bytes = 24 bytes constant
    """
    prev = None
    current = head
    
    while current:
        next_temp = current.next  # Only temporary variable
        current.next = prev
        prev = current
        current = next_temp
    
    return prev

# O(n) Space - Linear
def reverse_recursive(head):
    """
    Call stack depth = n
    Each call: ~100 bytes (stack frame)
    Total: n × 100 bytes
    """
    if not head or not head.next:
        return head
    
    new_head = reverse_recursive(head.next)  # Recursive call
    head.next.next = head
    head.next = None
    
    return new_head

def values_to_list(head):
    """
    Creates new list of n values
    Space: n × 8 bytes (Python integers)
    """
    result = []
    current = head
    while current:
        result.append(current.val)  # Stores value
        current = current.next
    return result

# O(log n) Space - Logarithmic
def merge_sort(head):
    """
    Recursion depth = log n (binary division)
    Each level: constant space
    Total: log n × constant
    """
    if not head or not head.next:
        return head
    
    # Find middle
    slow = fast = head
    prev = None
    while fast and fast.next:
        prev = slow
        slow = slow.next
        fast = fast.next.next
    
    prev.next = None
    
    # Recursive calls (log n depth)
    left = merge_sort(head)
    right = merge_sort(slow)
    
    return merge(left, right)

# Space Complexity with HashMap
def copy_with_random(head):
    """
    HashMap stores n references
    Space: n × 16 bytes (key-value pairs)
    Plus original/copy lists: 2n nodes
    Total: O(n) space
    """
    if not head:
        return None
    
    old_to_new = {}  # O(n) space
    
    # First pass
    current = head
    while current:
        old_to_new[current] = Node(current.val)
        current = current.next
    
    # Second pass
    current = head
    while current:
        new_node = old_to_new[current]
        new_node.next = old_to_new.get(current.next)
        new_node.random = old_to_new.get(current.random)
        current = current.next
    
    return old_to_new[head]
```

---

## 10.4 Best, Average, and Worst Case

### Comprehensive Case Analysis

```python
"""
Operation          | Best Case | Average  | Worst Case | Notes
-------------------|-----------|----------|------------|------------------
Search             | O(1)      | O(n)     | O(n)       | Best: first element
Insert at head     | O(1)      | O(1)     | O(1)       | Always constant
Insert at position | O(1)      | O(n)     | O(n)       | Best: position 0
Delete by value    | O(1)      | O(n)     | O(n)       | Best: first match
Cycle detection    | O(1)      | O(n)     | O(n)       | Best: no cycle, empty
Sort (Merge)       | O(n log n)| O(n log n)| O(n log n)| Always same
Sort (Quick)       | O(n log n)| O(n log n)| O(n²)      | Worst: sorted input
Skip List search   | O(log n)  | O(log n) | O(n)       | Probabilistic
"""
```

### Detailed Case Studies

```python
# Example 1: Search - Different Cases
def search_analysis(head, target):
    """
    Best Case: O(1)
    - Target is first element
    - Only 1 comparison
    
    Average Case: O(n/2) ≈ O(n)
    - Target in middle
    - n/2 comparisons
    
    Worst Case: O(n)
    - Target is last or not found
    - n comparisons
    """
    current = head
    steps = 0
    
    while current:
        steps += 1
        if current.val == target:
            print(f"Found in {steps} steps")
            return current
        current = current.next
    
    print(f"Not found after {steps} steps")
    return None

# Example: Best vs Worst
"""
List: 1 → 2 → 3 → 4 → 5 → 6 → 7 → 8 → 9 → 10

Search for 1:  Best case - 1 step
Search for 5:  Average - 5 steps
Search for 10: Worst case - 10 steps
Search for 11: Worst case - 10 steps (not found)
"""

# Example 2: Delete by Value - Cases
def delete_value_analysis(head, target):
    """
    Best Case: O(1)
    - Target is head
    - Immediate deletion
    
    Average Case: O(n/2) ≈ O(n)
    - Target in middle
    - Traverse half list
    
    Worst Case: O(n)
    - Target is last or not found
    - Traverse entire list
    """
    # Best case
    if head and head.val == target:
        print("Best case: O(1)")
        return head.next
    
    # General case
    current = head
    steps = 0
    
    while current and current.next:
        steps += 1
        if current.next.val == target:
            print(f"Found after {steps} steps")
            current.next = current.next.next
            return head
        current = current.next
    
    print(f"Not found after {steps} steps")
    return head
```

---

## 10.5 Amortized Analysis

### Understanding Amortized Complexity

```python
"""
Amortized analysis considers:
- Average cost over sequence of operations
- Not single operation cost
- Accounts for occasional expensive operations

Example: Dynamic Array
- Most insertions: O(1)
- Occasional resize: O(n)
- Amortized: O(1)

Sequence of 8 insertions in array with doubling:
Insert 1: Size 1→2, copy 1 element
Insert 2: Add to size 2, no copy
Insert 3: Size 2→4, copy 2 elements
Insert 4: Add to size 4, no copy
Insert 5: Size 4→8, copy 4 elements
Insert 6: Add to size 8, no copy
Insert 7: Add to size 8, no copy
Insert 8: Add to size 8, no copy

Total copies: 1 + 2 + 4 = 7
Total operations: 8 + 7 = 15
Amortized per operation: 15/8 ≈ 2 = O(1)
"""

class DynamicArrayAnalysis:
    """
    Demonstrating amortized O(1) append
    """
    def __init__(self):
        self.capacity = 1
        self.size = 0
        self.array = [None] * self.capacity
        self.total_copies = 0
    
    def append(self, val):
        """
        Individual operation: O(1) or O(n)
        Amortized: O(1)
        """
        if self.size == self.capacity:
            # Resize: O(n) operation
            new_capacity = self.capacity * 2
            new_array = [None] * new_capacity
            
            for i in range(self.size):
                new_array[i] = self.array[i]
                self.total_copies += 1
            
            self.array = new_array
            self.capacity = new_capacity
        
        self.array[self.size] = val
        self.size += 1
    
    def analyze(self):
        """Show amortized analysis"""
        avg_copies = self.total_copies / self.size if self.size > 0 else 0
        print(f"Size: {self.size}")
        print(f"Total copies: {self.total_copies}")
        print(f"Average copies per insert: {avg_copies:.2f}")

# Test
arr = DynamicArrayAnalysis()
for i in range(16):
    arr.append(i)
arr.analyze()

"""
Output:
Size: 16
Total copies: 15
Average copies per insert: 0.94 ≈ O(1)
"""
```

---

## 10.6 Comparison with Other Data Structures

### Complete Comparison Table

```python
"""
Operation         | LinkedList | Array  | BST     | Hash Table | Heap    |
------------------|------------|--------|---------|------------|---------|
Access            | O(n)       | O(1)   | O(log n)| O(1)†      | O(n)    |
Search            | O(n)       | O(n)   | O(log n)| O(1)†      | O(n)    |
Insert at end     | O(1)*      | O(1)†  | O(log n)| O(1)†      | O(log n)|
Insert at start   | O(1)       | O(n)   | O(log n)| O(1)†      | O(log n)|
Delete            | O(n)       | O(n)   | O(log n)| O(1)†      | O(log n)|
Find min/max      | O(n)       | O(n)   | O(log n)| O(n)       | O(1)    |
Space             | O(n)       | O(n)   | O(n)    | O(n)       | O(n)    |

* With tail pointer
† Average case; worst case may be O(n)
"""
```

### When to Use What

```python
"""
Choose LinkedList when:
✓ Frequent insertions/deletions in middle
✓ Unknown or dynamic size
✓ No need for random access
✓ Implementing other structures (Stack, Queue)
✓ Memory fragmentation acceptable

Choose Array when:
✓ Random access needed (indexing)
✓ Size known or rarely changes
✓ Cache locality important
✓ Minimal insert/delete operations
✓ Searching with binary search

Choose BST when:
✓ Need sorted order
✓ Frequent search, insert, delete
✓ Range queries needed
✓ All operations should be O(log n)

Choose Hash Table when:
✓ Fast lookup primary requirement
✓ Order doesn't matter
✓ Keys are hashable
✓ Average O(1) acceptable (not guaranteed)

Choose Heap when:
✓ Need min/max quickly
✓ Priority queue operations
✓ Top-k elements
✓ Median finding
"""
```

---

## 10.7 Space-Time Tradeoffs

### Classic Tradeoffs

```python
# Example 1: Two Sum Problem

# Approach 1: O(n²) time, O(1) space
def two_sum_slow(nums, target):
    """Nested loops - no extra space"""
    for i in range(len(nums)):
        for j in range(i + 1, len(nums)):
            if nums[i] + nums[j] == target:
                return [i, j]
    return []

# Approach 2: O(n) time, O(n) space
def two_sum_fast(nums, target):
    """Hash map - trades space for speed"""
    seen = {}
    for i, num in enumerate(nums):
        complement = target - num
        if complement in seen:
            return [seen[complement], i]
        seen[num] = i
    return []

# Example 2: Copy List with Random Pointer

# Approach 1: O(n) time, O(n) space
def copy_with_hashmap(head):
    """Use HashMap for mapping"""
    old_to_new = {}
    current = head
    
    while current:
        old_to_new[current] = Node(current.val)
        current = current.next
    
    current = head
    while current:
        new_node = old_to_new[current]
        new_node.next = old_to_new.get(current.next)
        new_node.random = old_to_new.get(current.random)
        current = current.next
    
    return old_to_new[head]

# Approach 2: O(n) time, O(1) space
def copy_interweave(head):
    """Interweave nodes - no extra space"""
    if not head:
        return None
    
    # Interweave
    current = head
    while current:
        copy = Node(current.val)
        copy.next = current.next
        current.next = copy
        current = copy.next
    
    # Set random pointers
    current = head
    while current:
        if current.random:
            current.next.random = current.random.next
        current = current.next.next
    
    # Separate lists
    current = head
    copy_head = head.next
    while current:
        copy = current.next
        current.next = copy.next
        copy.next = copy.next.next if copy.next else None
        current = current.next
    
    return copy_head
```

---

## 10.8 Practical Performance

### Real-World Benchmarks

```python
import time
import random

def benchmark_comparison():
    """Compare LinkedList vs Array performance"""
    
    # Setup
    n = 10000
    
    # Create LinkedList
    head = ListNode(0)
    current = head
    for i in range(1, n):
        current.next = ListNode(i)
        current = current.next
    
    # Create Array
    arr = list(range(n))
    
    # Test 1: Insert at beginning
    print("=== Insert at Beginning ===")
    
    # LinkedList
    start = time.time()
    for _ in range(1000):
        head = ListNode(-1, head)
    print(f"LinkedList: {time.time() - start:.4f}s")
    
    # Array
    start = time.time()
    for _ in range(1000):
        arr.insert(0, -1)
    print(f"Array: {time.time() - start:.4f}s")
    
    # Test 2: Random access
    print("\n=== Random Access ===")
    indices = [random.randint(0, n-1) for _ in range(1000)]
    
    # LinkedList
    start = time.time()
    for idx in indices:
        current = head
        for _ in range(idx):
            current = current.next
    print(f"LinkedList: {time.time() - start:.4f}s")
    
    # Array
    start = time.time()
    for idx in indices:
        _ = arr[idx]
    print(f"Array: {time.time() - start:.4f}s")

"""
Typical Output:
=== Insert at Beginning ===
LinkedList: 0.0003s  ⚡
Array: 2.5423s

=== Random Access ===
LinkedList: 5.2341s
Array: 0.0001s  ⚡

Conclusion: Use the right tool for the job!
"""
```

---

## Summary Table: Quick Reference

```python
"""
┌─────────────────────────────────────────────────────────────────┐
│                    LINKEDLIST COMPLEXITY                        │
├─────────────────────┬──────────┬──────────┬─────────────────────┤
│ Operation           │ Time     │ Space    │ Notes               │
├─────────────────────┼──────────┼──────────┼─────────────────────┤
│ Access              │ O(n)     │ O(1)     │ Must traverse       │
│ Search              │ O(n)     │ O(1)     │ Linear scan         │
│ Insert Head         │ O(1)     │ O(1)     │ Optimal!            │
│ Insert Tail         │ O(1)*    │ O(1)     │ *with tail pointer  │
│ Delete Head         │ O(1)     │ O(1)     │ Optimal!            │
│ Delete Tail         │ O(n)     │ O(1)     │ O(1) for doubly     │
│ Reverse             │ O(n)     │ O(1)     │ Iterative best      │
│ Sort                │ O(n log n)│ O(log n)│ Use merge sort      │
│ Cycle Detect        │ O(n)     │ O(1)     │ Floyd's algorithm   │
│ Find Middle         │ O(n)     │ O(1)     │ Fast-slow pointers  │
│ Merge K Lists       │ O(N log k)│ O(k)    │ N=total, k=lists    │
│ LRU Cache           │ O(1)     │ O(n)     │ With HashMap        │
└─────────────────────┴──────────┴──────────┴─────────────────────┘
"""
```

## Key Takeaways

1. **Know your complexities** - Essential for interviews
2. **Consider both time and space** - Tradeoffs exist
3. **Best/Average/Worst** - Know when each applies
4. **Amortized analysis** - Understand occasional costs
5. **Choose right structure** - Based on operations needed

**Pro Tip:** In interviews, always state time and space complexity with your solution!