# Chapter 10: Optimization Techniques - Deep Dive

## 10.1 Space Optimization Strategies

### Overview

While the standard prefix sum + HashMap approach uses O(n) space, there are techniques to reduce this in specific scenarios.

---

## 10.2 Optimization 1: Array Instead of HashMap

### When to Use

When keys are small non-negative integers (typically 0 to K).

### Comparison

```python
# Standard HashMap - O(n) space worst case
def with_hashmap(arr, k):
    hashmap = {0: 1}
    prefix = 0
    count = 0
    
    for num in arr:
        prefix += num
        remainder = (prefix % k + k) % k
        if remainder in hashmap:
            count += hashmap[remainder]
        hashmap[remainder] = hashmap.get(remainder, 0) + 1
    
    return count

# Optimized with Array - O(k) space
def with_array(arr, k):
    remainder_count = [0] * k
    remainder_count[0] = 1
    prefix = 0
    count = 0
    
    for num in arr:
        prefix += num
        remainder = (prefix % k + k) % k
        count += remainder_count[remainder]
        remainder_count[remainder] += 1
    
    return count
```

### Performance Comparison

```
For k = 1000, n = 1,000,000:

HashMap approach:
  - Space: Up to 1,000,000 entries worst case
  - Lookup: O(1) average, some overhead
  
Array approach:
  - Space: Exactly 1,000 entries
  - Lookup: O(1) guaranteed, faster access
  
Array is 1000Ã— more space efficient!
```

### When NOT to Use

```python
# DON'T use array if:

# 1. Keys can be negative
arr = [-5, 3, 2]  # Negative indices!

# 2. Keys have large range
k = 10**9  # Can't allocate 10^9 array!

# 3. Keys are non-integers
k = 3.14  # Floats as indices?

# Use HashMap for these cases
```

---

## 10.3 Optimization 2: In-Place Prefix Sum

### Standard Approach

```python
def standard_prefix(arr):
    """
    Builds separate prefix array.
    Space: O(n) for prefix array
    """
    n = len(arr)
    prefix = [0] * (n + 1)
    
    for i in range(n):
        prefix[i + 1] = prefix[i] + arr[i]
    
    return prefix
```

### Optimized: In-Place

```python
def in_place_prefix(arr):
    """
    Modify original array to store prefix sums.
    Space: O(1) additional
    WARNING: Destroys original array!
    """
    for i in range(1, len(arr)):
        arr[i] += arr[i - 1]
    
    return arr

# Example
arr = [1, 2, 3, 4, 5]
in_place_prefix(arr)
print(arr)  # [1, 3, 6, 10, 15]
```

### Safe In-Place (If Allowed)

```python
def safe_in_place(arr):
    """
    Only if you know array won't be used later.
    """
    if not arr:
        return 0
    
    # Store previous for calculation
    prev = 0
    result = 0
    
    for i, num in enumerate(arr):
        arr[i] += prev
        prev = arr[i]
        # Do calculations here with prefix sum
    
    return result
```

### Trade-offs

| Approach | Space | Array Preserved | Use Case |
|----------|-------|-----------------|----------|
| Separate array | O(n) | Yes | Default choice |
| In-place | O(1) | No | Space-critical |
| Rolling variable | O(1) | Yes | Single pass only |

---

## 10.4 Optimization 3: Rolling Hash for State

### Problem: Large State Tracking

```python
# Tracking character frequencies
# State: [count_a, count_b, ..., count_z]
# Naive: Store entire 26-element array as tuple

# Issue:
state = tuple([0] * 26)  # 26 integers
hashmap[state] = index   # Large keys!
```

### Optimization: Rolling Hash

```python
def rolling_hash_state(s):
    """
    Use hash value instead of full state.
    Space: O(1) per state instead of O(26)
    """
    hashmap = {0: -1}  # hash_value -> index
    state_hash = 0
    max_len = 0
    
    # Simple polynomial rolling hash
    BASE = 31
    MOD = 10**9 + 7
    
    for i, char in enumerate(s):
        # Update hash incrementally
        char_val = ord(char) - ord('a') + 1
        state_hash = (state_hash * BASE + char_val) % MOD
        
        if state_hash in hashmap:
            max_len = max(max_len, i - hashmap[state_hash])
        else:
            hashmap[state_hash] = i
    
    return max_len
```

### Collision Handling

```python
def safe_rolling_hash(s):
    """
    Handle potential collisions.
    """
    hashmap = defaultdict(list)  # hash -> list of full states
    state = [0] * 26
    
    for i, char in enumerate(s):
        state[ord(char) - ord('a')] += 1
        
        # Calculate hash
        hash_val = hash(tuple(state)) % (10**9 + 7)
        
        # Check if we've seen this hash
        for stored_state in hashmap[hash_val]:
            if stored_state == tuple(state):
                # Found match!
                return i
        
        hashmap[hash_val].append(tuple(state))
    
    return -1
```

---

## 10.5 Optimization 4: Early Termination

### For Existence Queries

```python
# Naive: Check all elements
def exists_naive(arr, k):
    hashmap = {0: 1}
    prefix = 0
    count = 0
    
    for num in arr:
        prefix += num
        if prefix - k in hashmap:
            count += 1  # Still counting!
        hashmap[prefix] = hashmap.get(prefix, 0) + 1
    
    return count > 0

# Optimized: Return immediately
def exists_optimized(arr, k):
    hashmap = {0}
    prefix = 0
    
    for num in arr:
        prefix += num
        if prefix - k in hashmap:
            return True  # Early exit!
        hashmap.add(prefix)
    
    return False

# Average case: Much faster!
```

### For Maximum/Minimum

```python
def first_occurrence_optimization(arr, k):
    """
    Stop when we find any subarray (not necessarily first).
    """
    hashmap = {0: -1}
    prefix = 0
    
    for i, num in enumerate(arr):
        prefix += num
        
        if prefix - k in hashmap:
            # Return immediately
            start = hashmap[prefix - k] + 1
            return (start, i)  # Found one!
        
        hashmap[prefix] = i
    
    return None
```

---

## 10.6 Optimization 5: Bit Manipulation for State

### For Binary States

```python
# Problem: Track which characters seen (0-25)
# Naive: tuple of 26 booleans

# Optimized: Single integer bitmask
def with_bitmask(s):
    """
    Use bitmask for character presence.
    Space: O(1) per state (just one integer!)
    """
    hashmap = {0: -1}  # bitmask -> index
    mask = 0
    max_len = 0
    
    for i, char in enumerate(s):
        bit = 1 << (ord(char) - ord('a'))
        mask ^= bit  # Toggle bit
        
        if mask in hashmap:
            max_len = max(max_len, i - hashmap[mask])
        else:
            hashmap[mask] = i
    
    return max_len

# Example for "abbccc"
# Bitmask evolution:
# ''  -> 0000000...0 (0)
# 'a' -> 0000000...1 (1)
# 'ab'-> 0000000..11 (3)
# 'abb'->0000000..01 (1) - b toggled
```

### Bitmask Operations

```python
# Set bit (mark character present)
mask |= (1 << i)

# Clear bit (mark character absent)
mask &= ~(1 << i)

# Toggle bit
mask ^= (1 << i)

# Check bit
is_set = (mask >> i) & 1

# Count set bits
count = bin(mask).count('1')
```

---

## 10.7 Optimization 6: Lazy Evaluation

### Defer Expensive Operations

```python
# Naive: Calculate everything upfront
def naive_approach(arr, k):
    # Build full prefix array
    prefix = [0] * (len(arr) + 1)
    for i in range(len(arr)):
        prefix[i + 1] = prefix[i] + arr[i]
    
    # Then use HashMap
    hashmap = {}
    for i in range(len(prefix)):
        hashmap[prefix[i]] = i
    
    # Finally search
    # ...

# Optimized: Calculate on-the-fly
def optimized_approach(arr, k):
    hashmap = {0: -1}
    prefix = 0
    
    for i, num in enumerate(arr):
        prefix += num  # Calculate only when needed
        
        if prefix - k in hashmap:
            return True
        
        hashmap[prefix] = i
    
    return False
```

---

## 10.8 Optimization 7: Cache-Friendly Access

### Memory Layout Optimization

```python
# Less cache-friendly: Objects
class Node:
    def __init__(self, sum, count):
        self.sum = sum
        self.count = count

def with_objects(arr, k):
    hashmap = {0: Node(0, 1)}
    # Objects scattered in memory
    # Poor cache locality

# More cache-friendly: Tuples
def with_tuples(arr, k):
    hashmap = {0: (0, 1)}
    # Tuples are more compact
    # Better cache locality
    
# Best: Separate arrays (if possible)
def with_arrays(arr, k):
    sums = [0]
    counts = [1]
    indices = [0]
    # Sequential access patterns
```

---

## 10.9 Optimization 8: Compiler-Level Tricks

### Python-Specific Optimizations

```python
# Slower: Using get() repeatedly
def slower(arr, k):
    hashmap = {}
    for num in arr:
        hashmap[num] = hashmap.get(num, 0) + 1

# Faster: defaultdict
from collections import defaultdict

def faster(arr, k):
    hashmap = defaultdict(int)
    for num in arr:
        hashmap[num] += 1  # No lookup needed!

# Even faster: Counter (built-in C)
from collections import Counter

def fastest(arr, k):
    return Counter(arr)
```

### Using Built-in Functions

```python
# Slower: Manual iteration
def manual_sum(arr):
    total = 0
    for num in arr:
        total += num
    return total

# Faster: Built-in sum (optimized in C)
def builtin_sum(arr):
    return sum(arr)
```

---

## 10.10 Optimization 9: Parallel Processing

### For Independent Queries

```python
from concurrent.futures import ThreadPoolExecutor

def parallel_queries(arr, queries):
    """
    Process multiple queries in parallel.
    Each query is independent.
    """
    # Build prefix sum once
    n = len(arr)
    prefix = [0] * (n + 1)
    for i in range(n):
        prefix[i + 1] = prefix[i] + arr[i]
    
    def process_query(query):
        left, right, k = query
        subarray_sum = prefix[right + 1] - prefix[left]
        # Do something with sum and k
        return subarray_sum == k
    
    # Process queries in parallel
    with ThreadPoolExecutor(max_workers=4) as executor:
        results = list(executor.map(process_query, queries))
    
    return results
```

---

## 10.11 Optimization 10: Algorithm-Level Changes

### Problem-Specific Optimizations

```python
# For non-negative integers only
def optimized_for_positive(arr, k):
    """
    Use sliding window instead of HashMap.
    Only works for non-negative numbers!
    Time: O(n), Space: O(1)
    """
    left = 0
    current_sum = 0
    count = 0
    
    for right in range(len(arr)):
        current_sum += arr[right]
        
        while current_sum > k and left <= right:
            current_sum -= arr[left]
            left += 1
        
        if current_sum == k:
            count += 1
    
    return count

# This is O(1) space vs O(n) for HashMap!
```

---

## 10.12 When to Optimize

### Optimization Priority

```
1. Correctness first!
   - Make it work
   - Handle edge cases
   - Pass all tests

2. Clarity second
   - Readable code
   - Good variable names
   - Comments for tricky parts

3. Performance third
   - Only if needed
   - Profile first
   - Optimize bottlenecks

4. Space last
   - Only if constrained
   - Don't sacrifice clarity
   - Document trade-offs
```

### Profiling Example

```python
import time
import tracemalloc

def profile_solution(func, arr, k):
    """
    Measure time and space.
    """
    # Start tracking memory
    tracemalloc.start()
    
    # Measure time
    start = time.time()
    result = func(arr, k)
    end = time.time()
    
    # Get memory usage
    current, peak = tracemalloc.get_traced_memory()
    tracemalloc.stop()
    
    print(f"Time: {end - start:.6f} seconds")
    print(f"Peak memory: {peak / 1024:.2f} KB")
    print(f"Result: {result}")
    
    return result
```

---

## 10.13 Optimization Checklist

Before optimizing, ask:

```
â–¡ Is the current solution too slow?
  - Profile it first
  - Know the bottleneck

â–¡ Is there a mathematical optimization?
  - Better algorithm (sliding window?)
  - Mathematical property (modulo?)

â–¡ Is space the constraint?
  - Can use array instead of HashMap?
  - Can reduce state size?

â–¡ Are there redundant operations?
  - Calculating same thing multiple times?
  - Can cache results?

â–¡ Can we exit early?
  - For existence checks
  - For first occurrence

â–¡ Is the code still readable?
  - Don't sacrifice clarity
  - Document optimizations
```

---

## 10.14 Real Interview Optimization

### What to Say

```
"The standard approach is O(n) time and O(n) space.

For this specific problem, I notice [observation].

We could optimize space to O(k) by using an array
instead of HashMap, since we only have k possible
remainders.

However, this assumes k is reasonable. If k could
be very large, the HashMap approach is better.

For now, I'll implement the standard O(n) solution
which is clearer and handles all cases."
```

### When Interviewer Asks for Optimization

```
1. Clarify constraints
   "How large can n and k be?"
   "Is space a concern?"

2. Discuss trade-offs
   "We can reduce space from O(n) to O(k)
    but this requires k to be small."

3. Implement if time permits
   "Let me optimize if we have time,
    but I want to make sure the main
    solution is correct first."
```

---

## 10.15 Key Takeaways

ðŸŽ¯ **Main Optimizations:**
1. Array instead of HashMap (when keys are small)
2. Early termination (for existence)
3. Bit manipulation (for binary states)
4. In-place operations (if array can be modified)

ðŸ”‘ **Space Optimizations:**
- O(n) â†’ O(k) for modulo problems
- O(k) for character state problems
- O(1) with sliding window (positive only)

ðŸ’¡ **When to Optimize:**
- After solving correctly
- When constraints require it
- When you understand trade-offs

âš ï¸ **Don't Over-Optimize:**
- Clarity > Performance (usually)
- Standard O(n) is acceptable
- Discuss before implementing

ðŸš€ **Interview Strategy:**
- Solve correctly first
- Discuss optimizations
- Implement if time permits
- Know the trade-offs