# Chapter 8: Complexity Analysis - Deep Dive

## 8.1 Time Complexity Fundamentals

### Big O Notation Review

```
O(1)      - Constant
O(log n)  - Logarithmic
O(n)      - Linear
O(n log n) - Linearithmic
O(n¬≤)     - Quadratic
O(n¬≥)     - Cubic
O(2‚Åø)     - Exponential
```

### For Prefix Sum + HashMap Problems

**Standard pattern:** O(n)
- One pass through array
- O(1) operations per element
- Total: O(n)

---

## 8.2 Detailed Time Complexity Analysis

### Basic Pattern: Count Subarrays

```python
def count_sum_equals_k(arr, k):
    hashmap = {0: 1}          # O(1)
    prefix = 0                 # O(1)
    count = 0                  # O(1)
    
    for num in arr:            # n iterations
        prefix += num          # O(1)
        if prefix - k in hashmap:    # O(1) average
            count += hashmap[prefix - k]  # O(1)
        hashmap[prefix] = hashmap.get(prefix, 0) + 1  # O(1)
    
    return count               # O(1)

# Total: O(1) + n √ó O(1) = O(n)
```

### Operation-by-Operation Breakdown

| Operation | Time | Count | Total |
|-----------|------|-------|-------|
| Initialize | O(1) | 1 | O(1) |
| Loop | - | n | - |
| ‚îî‚îÄ Addition | O(1) | n | O(n) |
| ‚îî‚îÄ HashMap lookup | O(1) avg | n | O(n) |
| ‚îî‚îÄ HashMap insert | O(1) avg | n | O(n) |
| **Total** | | | **O(n)** |

### Why HashMap Operations Are O(1)

```
Hash Function: O(1)
  - Calculate hash of key
  - Takes constant time for integers

Lookup: O(1) average
  - Use hash to find bucket
  - Check if key exists
  - Average case: one bucket
  
Insert: O(1) average
  - Calculate hash
  - Insert into bucket
  - Average case: no collisions
  
Worst Case: O(n)
  - All keys hash to same bucket
  - Rare with good hash function
```

---

## 8.3 Space Complexity Analysis

### Basic Pattern Space

```python
def count_sum_equals_k(arr, k):
    hashmap = {0: 1}          # Space: O(?)
    prefix = 0                 # Space: O(1)
    count = 0                  # Space: O(1)
    
    for num in arr:
        # ...
        hashmap[prefix] = ...  # Growing map
    
    return count

# Space complexity: O(n) worst case
```

### Space Growth Analysis

```
Worst case: All prefix sums are unique

arr = [1, 2, 3, 4, 5]
prefix sums = [1, 3, 6, 10, 15]

HashMap size grows:
{0: 1}
{0: 1, 1: 1}
{0: 1, 1: 1, 3: 1}
{0: 1, 1: 1, 3: 1, 6: 1}
{0: 1, 1: 1, 3: 1, 6: 1, 10: 1}
{0: 1, 1: 1, 3: 1, 6: 1, 10: 1, 15: 1}

Size: n + 1 = O(n)
```

### Best Case Space

```
Best case: All prefix sums are same

arr = [0, 0, 0, 0]
prefix sums = [0, 0, 0, 0]

HashMap stays:
{0: 1}
{0: 2}
{0: 3}
{0: 4}
{0: 5}

Size: 1 = O(1)

But we can't rely on this!
Space complexity: O(n) worst case
```

---

## 8.4 Pattern-Specific Complexity

### Pattern 1: Sum Equals K

```python
def count_sum_equals_k(arr, k):
    # Time: O(n)
    # Space: O(n)
    pass
```

**Why O(n) space:**
- Up to n different prefix sums
- Each stored once in HashMap

### Pattern 2: Divisible by K

```python
def count_divisible_k(arr, k):
    # Time: O(n)
    # Space: O(min(n, k))
    pass
```

**Why O(min(n, k)) space:**
```
Only k possible remainders: 0, 1, 2, ..., k-1

arr = [1, 2, 3, 4, 5, ...]
k = 3

Possible remainders: 0, 1, 2
HashMap size ‚â§ 3 = k

So: O(min(n, k))
- If k ‚â• n: size is at most n
- If k < n: size is at most k
```

### Pattern 3: XOR Equals K

```python
def count_xor_equals_k(arr, k):
    # Time: O(n)
    # Space: O(n)
    pass
```

**Why O(n) space:**
- XOR can produce many unique values
- Similar to sum pattern

### Pattern 4: 2D Matrix

```python
def count_submatrix_sum(matrix, target):
    # Time: O(rows¬≤ √ó cols)
    # Space: O(cols)
    pass
```

**Detailed breakdown:**
```
Outer loop (top row): rows iterations
Inner loop (bottom row): rows iterations
Total row combinations: rows¬≤

For each combination:
  - Build column sums: O(cols)
  - Apply 1D pattern: O(cols)

Total: O(rows¬≤) √ó O(cols) = O(rows¬≤ √ó cols)

Space: O(cols) for column sum array
       O(cols) for HashMap in 1D pattern
Total: O(cols)
```

---

## 8.5 Comparison with Other Approaches

### Approach 1: Brute Force

```python
def brute_force(arr, k):
    count = 0
    n = len(arr)
    
    for i in range(n):           # O(n)
        for j in range(i, n):    # O(n)
            # Calculate sum from i to j
            subarray_sum = sum(arr[i:j+1])  # O(n)
            if subarray_sum == k:
                count += 1
    
    return count

# Time: O(n¬≥)
# Space: O(1)
```

### Approach 2: Prefix Sum (No HashMap)

```python
def with_prefix_only(arr, k):
    n = len(arr)
    prefix = [0] * (n + 1)
    
    # Build prefix: O(n)
    for i in range(n):
        prefix[i+1] = prefix[i] + arr[i]
    
    count = 0
    # Check all pairs: O(n¬≤)
    for i in range(n):
        for j in range(i, n):
            if prefix[j+1] - prefix[i] == k:
                count += 1
    
    return count

# Time: O(n¬≤)
# Space: O(n)
```

### Approach 3: Prefix Sum + HashMap (Optimal)

```python
def optimal(arr, k):
    hashmap = {0: 1}
    prefix = 0
    count = 0
    
    for num in arr:  # O(n)
        prefix += num
        if prefix - k in hashmap:  # O(1)
            count += hashmap[prefix - k]
        hashmap[prefix] = hashmap.get(prefix, 0) + 1
    
    return count

# Time: O(n)
# Space: O(n)
```

### Complexity Comparison Table

| Approach | Time | Space | Practical Limit |
|----------|------|-------|-----------------|
| Brute Force | O(n¬≥) | O(1) | n ‚âà 1,000 |
| Prefix Only | O(n¬≤) | O(n) | n ‚âà 10,000 |
| Prefix + HashMap | O(n) | O(n) | n ‚âà 10‚Å∂+ |

---

## 8.6 Practical Performance

### Input Size Guidelines

```
n ‚â§ 100:
  - Any approach works
  - O(n¬≥) runs in < 1 second

n ‚â§ 10,000:
  - O(n¬≤) acceptable
  - O(n¬≥) too slow

n ‚â§ 100,000:
  - O(n log n) good
  - O(n¬≤) too slow

n ‚â§ 1,000,000:
  - O(n) required
  - O(n log n) might be tight

n > 1,000,000:
  - O(n) required
  - Watch constant factors
```

### Real Timing Example

```python
import time

def measure_time(func, arr, k):
    start = time.time()
    result = func(arr, k)
    end = time.time()
    return result, end - start

# Test with different sizes
for n in [100, 1000, 10000]:
    arr = list(range(n))
    k = n // 2
    
    _, t_brute = measure_time(brute_force, arr, k)
    _, t_optimal = measure_time(optimal, arr, k)
    
    print(f"n={n:5d}: Brute={t_brute:.4f}s, Optimal={t_optimal:.4f}s")

# Output (approximate):
# n=  100: Brute=0.0050s, Optimal=0.0001s
# n= 1000: Brute=5.0000s, Optimal=0.0010s
# n=10000: Brute=Timeout,  Optimal=0.0100s
```

---

## 8.7 Amortized Analysis

### HashMap Resizing

```
HashMap grows when load factor exceeded:
  - Typically at 75% full
  - Resizing: Create new array, rehash all elements
  - Cost: O(n) for resize
  
But: Resizing happens rarely
  - Start: size 16
  - After 12 inserts: resize to 32
  - After 24 inserts: resize to 64
  - After 48 inserts: resize to 128
  
Amortized cost per insert: Still O(1)!
```

### Amortized Complexity Proof

```
n insertions total cost:
  - Most inserts: O(1) each
  - Resize at sizes: 16, 32, 64, ..., closest power of 2 to n
  - Resize costs: 16, 32, 64, ..., n
  
Total resize cost: 16 + 32 + 64 + ... + n
                 = 2n (geometric series)
                 = O(n)

Total cost: n √ó O(1) + O(n) = O(n)
Average per insert: O(n) / n = O(1) amortized
```

---

## 8.8 Worst-Case Scenarios

### Worst Case for Time

```python
# All hash collisions (theoretically possible but rare)
# With poor hash function:

class BadHash:
    def __hash__(self):
        return 0  # All hash to same value!

# All elements in one bucket ‚Üí O(n) per lookup
# Total: O(n¬≤) worst case

# But: Modern hash functions prevent this
# Practical: O(n) time
```

### Worst Case for Space

```python
# All prefix sums unique
arr = [1, 2, 3, 4, ..., n]
# Prefix: [1, 3, 6, 10, ..., n(n+1)/2]
# All different ‚Üí O(n) space

# Can't do better than O(n) for this pattern
```

---

## 8.9 Space Optimization Techniques

### Technique 1: Modulo Problems

```python
def optimized_divisible_k(arr, k):
    # Instead of O(n), use O(k)
    remainder_count = [0] * k  # Fixed size k
    remainder_count[0] = 1
    prefix = 0
    count = 0
    
    for num in arr:
        prefix += num
        rem = (prefix % k + k) % k
        count += remainder_count[rem]
        remainder_count[rem] += 1
    
    return count

# Space: O(k) instead of O(min(n, k))
# Trade-off: Array vs HashMap
#   - Array: Better for small k
#   - HashMap: Better for large k
```

### Technique 2: Counting vs Length

```python
# Counting: O(n) space (store all frequencies)
def counting(arr, k):
    hashmap = {0: 1}  # Can grow to O(n)
    # ...

# Length: Often better space in practice
def length(arr, k):
    hashmap = {0: -1}  # Often smaller
    # Why? Only stores first occurrence
    # If many duplicates, much smaller!
```

---

## 8.10 Advanced: Cache Performance

### Locality of Reference

```
Array access:
  - Sequential: Good cache performance
  - Random: Poor cache performance

Our pattern:
  - Sequential array iteration ‚úì
  - HashMap lookups: Random ‚úó
  
Overall: Still very fast due to O(1) lookups
```

### Memory Access Pattern

```python
# Good: Sequential access
for i in range(n):
    arr[i]  # Cache-friendly

# Mixed: Sequential + random
for i in range(n):
    arr[i]           # Cache-friendly
    hashmap[key]     # Random access
    
# Still efficient due to HashMap implementation
```

---

## 8.11 Complexity by Problem Variation

### Comprehensive Table

| Problem | Time | Space | Notes |
|---------|------|-------|-------|
| Sum = K (count) | O(n) | O(n) | Standard |
| Sum = K (length) | O(n) | O(n) | May be smaller |
| Divisible by K | O(n) | O(min(n,k)) | k remainders |
| Divisible by K (array) | O(n) | O(k) | Fixed array |
| Equal 0s/1s | O(n) | O(n) | Transform to sum=0 |
| XOR = K | O(n) | O(n) | Similar to sum |
| 2D Matrix | O(r¬≤c) | O(c) | r=rows, c=cols |
| Multiple conditions | O(n) | O(n√óm) | m conditions |
| k distinct chars | O(n) | O(k) | Sliding window |

---

## 8.12 Interview Complexity Discussion

### What Interviewers Want to Hear

```
"The brute force approach checks all subarrays,
which takes O(n¬≤) time. By using prefix sum with
a HashMap, we can solve it in O(n) time with O(n)
space. The HashMap provides O(1) lookups on average,
allowing us to check each element once. The space
complexity comes from storing up to n different
prefix sums in the HashMap."
```

### Follow-up Questions

**Q: Can we do better than O(n) time?**
```
A: No, we must examine each element at least once,
   so O(n) is optimal for time complexity.
```

**Q: Can we reduce space to O(1)?**
```
A: Not for the general case. We need to store
   previously seen prefix sums. However, for
   specific cases like "divisible by K", we can
   use O(k) space if k is small.
```

**Q: What about cache performance?**
```
A: Sequential array access is cache-friendly.
   HashMap lookups are random but still very fast.
   Overall, the algorithm has good practical performance.
```

---

## 8.13 Complexity Visualization

### Time Complexity Growth

```
Input Size (n) vs Time (operations)

n=10:     10 operations
n=100:    100 operations
n=1000:   1,000 operations
n=10000:  10,000 operations

Linear growth: O(n)

Compare to O(n¬≤):
n=10:     100 operations (10√ó)
n=100:    10,000 operations (100√ó)
n=1000:   1,000,000 operations (1000√ó)
n=10000:  100,000,000 operations (10000√ó)
```

### Space Complexity Growth

```
Worst case: O(n)

n=100:    ‚â§ 100 entries
n=1000:   ‚â§ 1,000 entries
n=10000:  ‚â§ 10,000 entries

Average case: Often smaller
- Duplicate prefix sums
- Limited value range

Modulo case: O(k)
k=10:     Exactly 10 entries
k=100:    Exactly 100 entries
(regardless of n!)
```

---

## 8.14 Practical Optimization Tips

### Tip 1: Use Array for Small K

```python
# If k ‚â§ 10,000, use array
def with_array(arr, k):
    count_array = [0] * k
    count_array[0] = 1
    # Faster than HashMap for small k

# If k > 10,000, use HashMap
def with_hashmap(arr, k):
    hashmap = {0: 1}
    # More memory-efficient for large k
```

### Tip 2: Early Termination

```python
def exists_subarray(arr, k):
    hashmap = {0}
    prefix = 0
    
    for num in arr:
        prefix += num
        if prefix - k in hashmap:
            return True  # Early exit!
        hashmap.add(prefix)
    
    return False

# Average case: Better than O(n)
# Worst case: Still O(n)
```

### Tip 3: Use Primitive Types

```python
# Slower: defaultdict
from collections import defaultdict
hashmap = defaultdict(int)

# Faster: Regular dict with get()
hashmap = {}
value = hashmap.get(key, 0)

# Reason: Less overhead
```

---

## 8.15 Key Takeaways

üéØ **Standard Complexity:**
- Time: O(n) - single pass
- Space: O(n) - worst case

üîë **Optimizations:**
- Modulo: O(k) space if k is small
- Early exit: Better average case
- Array vs HashMap: Choose based on k

üí° **Comparison:**
- Brute Force: O(n¬≥) time, O(1) space
- Prefix Only: O(n¬≤) time, O(n) space
- Prefix + HashMap: O(n) time, O(n) space ‚úì

‚ö†Ô∏è **Remember:**
- HashMap: O(1) average, O(n) worst (rare)
- Must examine all elements ‚Üí O(n) minimum
- Space-time tradeoff inherent

üöÄ **Interview Tip:** Always discuss both time AND space complexity!