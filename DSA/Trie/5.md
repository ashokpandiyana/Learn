# Chapter 5: Advanced Trie Concepts

## 5.1 Suffix Trie/Tree

### 5.1.1 What is a Suffix Trie?

A **Suffix Trie** is a trie containing all suffixes of a given text. It enables powerful pattern matching and substring operations.

### Visual Example

```
Text: "banana"
Suffixes: ["banana", "anana", "nana", "ana", "na", "a"]

Suffix Trie:
              root
         /    |    |    \
        b     a    n     $
        |    / \   |
        a   n   $  a
        |   |      |
        n   a      $
        |   |
        a   n
        |   |
        n   a
        |   |
        a   $
        |
        $

$ = end marker (important for distinguishing substrings)
```

### 5.1.2 Construction

```python
class SuffixTrieNode:
    def __init__(self):
        self.children = {}
        self.indexes = []  # Store all starting positions of this suffix

class SuffixTrie:
    def __init__(self, text):
        self.root = SuffixTrieNode()
        self.text = text
        self.end_symbol = '$'
        self._build_suffix_trie()
    
    def _build_suffix_trie(self):
        """Build suffix trie - O(n²) time and space"""
        text = self.text + self.end_symbol
        
        # Insert all suffixes
        for i in range(len(text)):
            self._insert_suffix(text[i:], i)
    
    def _insert_suffix(self, suffix, start_idx):
        """Insert a single suffix with its starting index"""
        node = self.root
        
        for char in suffix:
            if char not in node.children:
                node.children[char] = SuffixTrieNode()
            node = node.children[char]
            node.indexes.append(start_idx)  # Track where this suffix starts
    
    def search_pattern(self, pattern):
        """Check if pattern exists in original text - O(m)"""
        node = self.root
        
        for char in pattern:
            if char not in node.children:
                return False
            node = node.children[char]
        
        return True
    
    def find_all_occurrences(self, pattern):
        """Find all starting positions of pattern - O(m + k)"""
        node = self.root
        
        # Navigate to end of pattern
        for char in pattern:
            if char not in node.children:
                return []
            node = node.children[char]
        
        # Return all starting positions
        return sorted(node.indexes)
    
    def longest_repeated_substring(self):
        """Find longest substring that appears at least twice"""
        self.longest = ""
        self._dfs_longest(self.root, "")
        return self.longest
    
    def _dfs_longest(self, node, current):
        """DFS to find deepest node with 2+ indexes"""
        # If this node represents a repeated substring
        if len(node.indexes) > 1 and len(current) > len(self.longest):
            if current[-1] != self.end_symbol:  # Exclude end marker
                self.longest = current
        
        for char, child in node.children.items():
            if char != self.end_symbol:
                self._dfs_longest(child, current + char)

# Example usage
text = "banana"
st = SuffixTrie(text)

print(st.search_pattern("ana"))  # True
print(st.find_all_occurrences("ana"))  # [1, 3] (positions in "banana")
print(st.longest_repeated_substring())  # "ana"
```

### 5.1.3 Applications of Suffix Trie

#### Application 1: Longest Repeated Substring

```python
def longest_repeated_substring(text):
    """
    Find the longest substring that appears at least twice
    Example: "banana" → "ana"
    """
    st = SuffixTrie(text)
    return st.longest_repeated_substring()
```

#### Application 2: Longest Common Substring of Two Strings

```python
class GeneralizedSuffixTrie:
    """Suffix trie for multiple strings"""
    
    def __init__(self, strings):
        self.root = SuffixTrieNode()
        self.strings = strings
        self._build_generalized_trie()
    
    def _build_generalized_trie(self):
        """Build trie with suffixes from all strings"""
        for string_idx, string in enumerate(self.strings):
            text = string + f'${string_idx}'  # Unique end marker
            
            for i in range(len(text)):
                self._insert_suffix_with_id(text[i:], string_idx)
    
    def _insert_suffix_with_id(self, suffix, string_id):
        node = self.root
        for char in suffix:
            if char not in node.children:
                node.children[char] = SuffixTrieNode()
            node = node.children[char]
            
            # Mark which string this suffix belongs to
            if not hasattr(node, 'string_ids'):
                node.string_ids = set()
            node.string_ids.add(string_id)
    
    def longest_common_substring(self):
        """Find longest substring common to all strings"""
        num_strings = len(self.strings)
        self.lcs = ""
        self._dfs_lcs(self.root, "", num_strings)
        return self.lcs
    
    def _dfs_lcs(self, node, current, num_strings):
        # Check if current substring appears in all strings
        if hasattr(node, 'string_ids') and len(node.string_ids) == num_strings:
            if len(current) > len(self.lcs):
                # Make sure we don't include end markers
                if not any(c == '$' for c in current):
                    self.lcs = current
        
        for char, child in node.children.items():
            if char not in ['$', '$0', '$1']:  # Skip end markers
                self._dfs_lcs(child, current + char, num_strings)

# Example
gst = GeneralizedSuffixTrie(["abcpqr", "xyzabcd"])
print(gst.longest_common_substring())  # "abc"
```

### 5.1.4 Suffix Tree vs Suffix Trie

**Suffix Trie Problems:**
- O(n²) space complexity
- Many single-child chains waste space

**Suffix Tree Solution:**
- Compressed version of suffix trie
- Edges can represent strings (not just characters)
- O(n) space complexity
- Construction: Ukkonen's algorithm O(n)

```
Suffix Trie for "banana$":     Suffix Tree for "banana$":
       root                           root
      /  |  \                        /  |  \
     b   a   n                    banana$ |  na
     |  / \  |                           /\   |
    ... (many nodes)                    na$ $  na$
                                         |      |
                                        na$     $
```

### 5.1.5 Suffix Tree Implementation (Simplified)

```python
class SuffixTreeNode:
    def __init__(self):
        self.children = {}
        self.start = -1      # Start index of edge label
        self.end = None      # End index of edge label (reference for active updates)
        self.suffix_link = None

class SimplifiedSuffixTree:
    """
    Simplified suffix tree (not full Ukkonen's algorithm)
    This is a compressed suffix trie
    """
    def __init__(self, text):
        self.text = text + '$'
        self.root = SuffixTreeNode()
        self._build_naive()
    
    def _build_naive(self):
        """O(n²) naive construction by compressing suffix trie"""
        n = len(self.text)
        
        # Insert all suffixes
        for i in range(n):
            self._insert_suffix(i)
    
    def _insert_suffix(self, suffix_start):
        """Insert suffix starting at position suffix_start"""
        node = self.root
        i = suffix_start
        
        while i < len(self.text):
            char = self.text[i]
            
            # Find matching child edge
            if char in node.children:
                child = node.children[char]
                edge_start = child.start
                edge_end = len(self.text) if child.end is None else child.end
                
                # Match along the edge
                j = 0
                while (j < edge_end - edge_start and 
                       i + j < len(self.text) and 
                       self.text[edge_start + j] == self.text[i + j]):
                    j += 1
                
                if j == edge_end - edge_start:
                    # Matched entire edge, continue with child
                    node = child
                    i += j
                else:
                    # Need to split edge
                    self._split_edge(node, child, edge_start, j, i + j)
                    return
            else:
                # Create new leaf
                leaf = SuffixTreeNode()
                leaf.start = i
                leaf.end = None  # Goes to end of text
                node.children[char] = leaf
                return
    
    def _split_edge(self, parent, child, edge_start, split_pos, text_pos):
        """Split an edge when partial match occurs"""
        # Create internal node
        internal = SuffixTreeNode()
        internal.start = edge_start
        internal.end = edge_start + split_pos
        
        # Update parent
        first_char = self.text[edge_start]
        parent.children[first_char] = internal
        
        # Update old child
        child.start = edge_start + split_pos
        next_char = self.text[child.start]
        internal.children[next_char] = child
        
        # Add new leaf for remaining suffix
        leaf = SuffixTreeNode()
        leaf.start = text_pos
        leaf.end = None
        new_char = self.text[text_pos]
        internal.children[new_char] = leaf
    
    def search(self, pattern):
        """Search for pattern in O(m) time"""
        node = self.root
        i = 0
        
        while i < len(pattern):
            char = pattern[i]
            if char not in node.children:
                return False
            
            child = node.children[char]
            edge_start = child.start
            edge_end = len(self.text) if child.end is None else child.end
            
            # Match along edge
            j = 0
            while j < edge_end - edge_start and i < len(pattern):
                if self.text[edge_start + j] != pattern[i]:
                    return False
                i += 1
                j += 1
            
            node = child
        
        return True

# Example
text = "banana"
suffix_tree = SimplifiedSuffixTree(text)
print(suffix_tree.search("ana"))  # True
print(suffix_tree.search("xyz"))  # False
```

### 5.1.6 When to Use Suffix Trie vs Suffix Tree

| Feature | Suffix Trie | Suffix Tree |
|---------|------------|-------------|
| Space | O(n²) | O(n) |
| Construction | O(n²) | O(n) with Ukkonen |
| Implementation | Simple | Complex |
| Use Case | Small texts, learning | Production, large texts |

---

## 5.2 Trie with Multiple Attributes

### 5.2.1 Extended Node Structure

Beyond basic tries, we often need to store additional metadata:

```python
class EnhancedTrieNode:
    def __init__(self):
        self.children = {}
        self.isEnd = False
        
        # Additional attributes
        self.frequency = 0           # How often word appears
        self.word = None             # Store complete word
        self.timestamp = None        # When inserted
        self.metadata = {}           # Flexible key-value storage
        self.user_id = None          # For personalization
        self.score = 0.0             # Ranking/relevance score
        self.synonyms = []           # Related words
```

### 5.2.2 Application: Search Engine with Ranking

```python
import time
from collections import defaultdict

class SearchEngine:
    def __init__(self):
        self.root = EnhancedTrieNode()
        self.user_preferences = defaultdict(dict)  # user_id -> {word -> click_count}
    
    def insert(self, word, frequency=1, user_id=None, metadata=None):
        """Insert word with metadata"""
        node = self.root
        
        for char in word:
            if char not in node.children:
                node.children[char] = EnhancedTrieNode()
            node = node.children[char]
        
        node.isEnd = True
        node.word = word
        node.frequency += frequency
        node.timestamp = time.time()
        node.user_id = user_id
        
        if metadata:
            node.metadata.update(metadata)
    
    def record_click(self, word, user_id):
        """Record when user clicks a suggestion"""
        self.user_preferences[user_id][word] = \
            self.user_preferences[user_id].get(word, 0) + 1
        
        # Update global frequency
        node = self._find_node(word)
        if node:
            node.frequency += 1
    
    def _find_node(self, word):
        """Helper to find node for a word"""
        node = self.root
        for char in word:
            if char not in node.children:
                return None
            node = node.children[char]
        return node if node.isEnd else None
    
    def personalized_search(self, prefix, user_id, limit=5):
        """Get personalized suggestions based on user history"""
        node = self.root
        
        # Navigate to prefix
        for char in prefix:
            if char not in node.children:
                return []
            node = node.children[char]
        
        # Collect all words with scores
        candidates = []
        self._collect_with_scores(node, candidates, user_id)
        
        # Sort by personalized score
        candidates.sort(key=lambda x: x[1], reverse=True)
        
        return [word for word, score in candidates[:limit]]
    
    def _collect_with_scores(self, node, candidates, user_id):
        """Collect words with personalized scoring"""
        if node.isEnd:
            # Calculate personalized score
            base_score = node.frequency
            personal_score = self.user_preferences[user_id].get(node.word, 0)
            recency_bonus = 1.0 / (time.time() - node.timestamp + 1)
            
            # Weighted combination
            final_score = (base_score * 0.5 + 
                          personal_score * 3.0 + 
                          recency_bonus * 0.1)
            
            candidates.append((node.word, final_score))
        
        for child in node.children.values():
            self._collect_with_scores(child, candidates, user_id)

# Example usage
engine = SearchEngine()

# Insert documents
engine.insert("python", frequency=100, metadata={"category": "programming"})
engine.insert("java", frequency=80, metadata={"category": "programming"})
engine.insert("javascript", frequency=90, metadata={"category": "programming"})

# Simulate user interactions
engine.record_click("python", user_id="user1")
engine.record_click("python", user_id="user1")
engine.record_click("java", user_id="user2")

# Personalized search
print(engine.personalized_search("py", "user1", limit=3))
# ["python"] with higher score for user1
```

### 5.2.3 Multi-dimensional Trie

For advanced filtering (e.g., search by prefix + category + date range):

```python
class MultiDimensionalNode:
    def __init__(self):
        self.children = {}
        self.isEnd = False
        self.documents = []  # List of document IDs
        
class MultiDimensionalTrie:
    """
    Trie that supports multiple filter dimensions
    Example: Find all "python" documents in "tutorials" category from 2024
    """
    def __init__(self):
        self.root = MultiDimensionalNode()
        self.documents = {}  # doc_id -> full document data
    
    def insert(self, word, doc_id, category, date, content):
        """Insert document with multiple dimensions"""
        # Store document
        self.documents[doc_id] = {
            "word": word,
            "category": category,
            "date": date,
            "content": content
        }
        
        # Build composite key: word + category
        composite_key = f"{word}#{category}"
        
        node = self.root
        for char in composite_key:
            if char not in node.children:
                node.children[char] = MultiDimensionalNode()
            node = node.children[char]
        
        node.isEnd = True
        node.documents.append(doc_id)
    
    def search(self, prefix, category=None, date_range=None):
        """Search with multiple filters"""
        # Build query key
        query_key = prefix
        if category:
            query_key += f"#{category}"
        
        node = self.root
        for char in query_key:
            if char not in node.children:
                return []
            node = node.children[char]
        
        # Collect all matching documents
        doc_ids = []
        self._collect_docs(node, doc_ids)
        
        # Apply date filter if specified
        if date_range:
            start_date, end_date = date_range
            doc_ids = [
                doc_id for doc_id in doc_ids
                if start_date <= self.documents[doc_id]["date"] <= end_date
            ]
        
        return [self.documents[doc_id] for doc_id in doc_ids]
    
    def _collect_docs(self, node, doc_ids):
        if node.isEnd:
            doc_ids.extend(node.documents)
        
        for child in node.children.values():
            self._collect_docs(child, doc_ids)
```

---

## 5.3 Concurrent/Thread-Safe Tries

### 5.3.1 Why Concurrency Matters

In production systems, multiple threads may:
- Insert new words simultaneously
- Search while insertions happen
- Update metadata concurrently

### 5.3.2 Approach 1: Coarse-Grained Locking

```python
import threading

class ThreadSafeTrie:
    """Simple approach: Lock entire trie for each operation"""
    
    def __init__(self):
        self.root = TrieNode()
        self.lock = threading.RLock()  # Reentrant lock
    
    def insert(self, word):
        with self.lock:
            node = self.root
            for char in word:
                if char not in node.children:
                    node.children[char] = TrieNode()
                node = node.children[char]
            node.isEnd = True
    
    def search(self, word):
        with self.lock:
            node = self.root
            for char in word:
                if char not in node.children:
                    return False
                node = node.children[char]
            return node.isEnd

# Pros: Simple, correct
# Cons: Poor concurrency - only one operation at a time
```

### 5.3.3 Approach 2: Fine-Grained Locking

```python
class FineGrainedTrieNode:
    def __init__(self):
        self.children = {}
        self.isEnd = False
        self.lock = threading.Lock()  # Lock per node

class FineGrainedTrie:
    """Lock only the nodes being modified"""
    
    def __init__(self):
        self.root = FineGrainedTrieNode()
    
    def insert(self, word):
        node = self.root
        
        for char in word:
            with node.lock:
                if char not in node.children:
                    node.children[char] = FineGrainedTrieNode()
                next_node = node.children[char]
            
            node = next_node
        
        with node.lock:
            node.isEnd = True
    
    def search(self, word):
        node = self.root
        
        for char in word:
            with node.lock:
                if char not in node.children:
                    return False
                next_node = node.children[char]
            
            node = next_node
        
        with node.lock:
            return node.isEnd

# Pros: Better concurrency - multiple operations on different paths
# Cons: More complex, potential deadlocks, lock overhead
```

### 5.3.4 Approach 3: Read-Write Locks

```python
from threading import RLock
from threading import Condition

class ReadWriteLock:
    """Custom read-write lock"""
    def __init__(self):
        self.readers = 0
        self.writers = 0
        self.read_ready = Condition(RLock())
        self.write_ready = Condition(RLock())
    
    def acquire_read(self):
        self.read_ready.acquire()
        while self.writers > 0:
            self.read_ready.wait()
        self.readers += 1
        self.read_ready.release()
    
    def release_read(self):
        self.read_ready.acquire()
        self.readers -= 1
        if self.readers == 0:
            self.write_ready.notify()
        self.read_ready.release()
    
    def acquire_write(self):
        self.write_ready.acquire()
        while self.readers > 0 or self.writers > 0:
            self.write_ready.wait()
        self.writers += 1
        self.write_ready.release()
    
    def release_write(self):
        self.write_ready.acquire()
        self.writers -= 1
        self.write_ready.notify()
        self.read_ready.notify_all()
        self.write_ready.release()

class ReadWriteTrie:
    """Optimized for read-heavy workloads"""
    def __init__(self):
        self.root = TrieNode()
        self.rw_lock = ReadWriteLock()
    
    def insert(self, word):
        self.rw_lock.acquire_write()
        try:
            node = self.root
            for char in word:
                if char not in node.children:
                    node.children[char] = TrieNode()
                node = node.children[char]
            node.isEnd = True
        finally:
            self.rw_lock.release_write()
    
    def search(self, word):
        self.rw_lock.acquire_read()
        try:
            node = self.root
            for char in word:
                if char not in node.children:
                    return False
                node = node.children[char]
            return node.isEnd
        finally:
            self.rw_lock.release_read()

# Pros: Multiple concurrent readers
# Cons: Writers block everyone
```

### 5.3.5 Approach 4: Lock-Free (Copy-on-Write)

```python
import copy

class ImmutableTrieNode:
    """Immutable node - creates new version on modification"""
    def __init__(self, children=None, isEnd=False):
        self.children = children if children else {}
        self.isEnd = isEnd
    
    def with_child(self, char, child):
        """Return new node with added child"""
        new_children = self.children.copy()
        new_children[char] = child
        return ImmutableTrieNode(new_children, self.isEnd)
    
    def with_end(self):
        """Return new node marked as end"""
        return ImmutableTrieNode(self.children, True)

class LockFreeTrie:
    """Uses atomic reference updates for thread safety"""
    def __init__(self):
        self.root = ImmutableTrieNode()
    
    def insert(self, word):
        """Creates new path for insertion"""
        while True:
            old_root = self.root
            new_root = self._insert_recursive(old_root, word, 0)
            
            # Atomically update root (compare-and-swap)
            if self._cas(old_root, new_root):
                break
    
    def _insert_recursive(self, node, word, index):
        if index == len(word):
            return node.with_end()
        
        char = word[index]
        
        if char in node.children:
            old_child = node.children[char]
            new_child = self._insert_recursive(old_child, word, index + 1)
            return node.with_child(char, new_child)
        else:
            # Create new path
            new_child = self._create_path(word[index + 1:])
            return node.with_child(char, new_child)
    
    def _create_path(self, suffix):
        """Create new nodes for remaining suffix"""
        if not suffix:
            return ImmutableTrieNode(isEnd=True)
        
        char = suffix[0]
        child = self._create_path(suffix[1:])
        return ImmutableTrieNode({char: child}, False)
    
    def _cas(self, old_root, new_root):
        """Compare-and-swap (simplified - use atomic library in production)"""
        if self.root is old_root:
            self.root = new_root
            return True
        return False
    
    def search(self, word):
        """Lock-free read"""
        node = self.root
        for char in word:
            if char not in node.children:
                return False
            node = node.children[char]
        return node.isEnd

# Pros: True lock-free, great for read-heavy
# Cons: Memory overhead, complex implementation
```

---

## 5.4 Persistent Trie

### 5.4.1 Concept

A **persistent trie** maintains all historical versions efficiently. Each modification creates a new version while preserving old versions.

### Use Cases
- Version control systems
- Undo/redo functionality
- Time-travel debugging
- Audit trails

### 5.4.2 Path Copying Technique

```python
class PersistentTrieNode:
    def __init__(self, children=None, isEnd=False):
        self.children = children if children else {}
        self.isEnd = isEnd
        self.version = 0

class PersistentTrie:
    """Maintains all historical versions"""
    def __init__(self):
        self.versions = [PersistentTrieNode()]  # List of root versions
        self.current_version = 0
    
    def insert(self, word):
        """Insert and create new version"""
        old_root = self.versions[self.current_version]
        new_root = self._insert_recursive(old_root, word, 0)
        
        self.versions.append(new_root)
        self.current_version += 1
        
        return self.current_version
    
    def _insert_recursive(self, node, word, index):
        """Create new node with path copying"""
        if index == len(word):
            # Create new node with isEnd = True
            new_node = PersistentTrieNode(node.children.copy(), True)
            new_node.version = self.current_version + 1
            return new_node
        
        char = word[index]
        
        # Create new node (copy-on-write)
        new_node = PersistentTrieNode(node.children.copy(), node.isEnd)
        new_node.version = self.current_version + 1
        
        if char in node.children:
            # Recursively copy path
            new_node.children[char] = self._insert_recursive(
                node.children[char], word, index + 1
            )
        else:
            # Create new branch
            new_node.children[char] = self._create_new_path(word[index + 1:])
        
        return new_node
    
    def _create_new_path(self, suffix):
        """Create completely new path"""
        if not suffix:
            return PersistentTrieNode(isEnd=True)
        
        node = PersistentTrieNode()
        node.children[suffix[0]] = self._create_new_path(suffix[1:])
        return node
    
    def search(self, word, version=None):
        """Search in specific version (default: current)"""
        if version is None:
            version = self.current_version
        
        if version >= len(self.versions):
            return False
        
        node = self.versions[version]
        
        for char in word:
            if char not in node.children:
                return False
            node = node.children[char]
        
        return node.isEnd
    
    def get_all_words(self, version=None):
        """Get all words in specific version"""
        if version is None:
            version = self.current_version
        
        root = self.versions[version]
        words = []
        self._collect_words(root, "", words)
        return words
    
    def _collect_words(self, node, prefix, words):
        if node.isEnd:
            words.append(prefix)
        
        for char, child in node.children.items():
            self._collect_words(child, prefix + char, words)
    
    def diff_versions(self, v1, v2):
        """Find differences between two versions"""
        words_v1 = set(self.get_all_words(v1))
        words_v2 = set(self.get_all_words(v2))
        
        added = words_v2 - words_v1
        removed = words_v1 - words_v2
        
        return {
            "added": list(added),
            "removed": list(removed),
            "unchanged": list(words_v1 & words_v2)
        }

# Example usage
pt = PersistentTrie()

v0 = pt.current_version  # Version 0
v1 = pt.insert("hello")   # Version 1
v2 = pt.insert("world")   # Version 2
v3 = pt.insert("hello")   # Version 3 (duplicate insert)

print(pt.search("hello", version=0))  # False
print(pt.search("hello", version=1))  # True
print(pt.search("world", version=1))  # False
print(pt.search("world", version=2))  # True

print(pt.diff_versions(1, 2))
# {"added": ["world"], "removed": [], "unchanged": ["hello"]}
```

### 5.4.3 Space Complexity Analysis

- **Naive approach**: O(V × N) where V = versions, N = nodes
- **Path copying**: O(V × log N) amortized
- Only modified path is copied, rest is shared

### Visual Example
```
Version 1: Insert "app"        Version 2: Insert "apple"

     root(v1)                       root(v2)
       |                              |
       a                              a (NEW)
       |                              |
       p                              p (NEW)
       |                              |
       p (END)                        p (END)
                                      |
                                      l (NEW)
                                      |
                                      e (NEW)

Nodes from v1 are reused (shared) until modification point
```

---

## Summary: Advanced Concepts Comparison

| Concept | Space | Time | Use Case |
|---------|-------|------|----------|
| Suffix Trie | O(n²) | O(m) search | Small texts, pattern matching |
| Suffix Tree | O(n) | O(m) search | Large texts, production |
| Multi-attribute | O(n × k) | O(m) | Search engines, metadata |
| Thread-safe (Lock) | O(n) | O(m + lock) | Concurrent access |
| Lock-free (CoW) | O(n × v) | O(m) | High read concurrency |
| Persistent | O(n × log v) | O(m) | Version control, undo |

### Key Takeaways

1. **Suffix structures** enable powerful substring operations
2. **Enhanced nodes** support real-world features like ranking and personalization
3. **Concurrency** requires careful design based on read/write patterns
4. **Persistence** allows time-travel queries with reasonable space overhead

### Interview Relevance

- **Suffix Trie/Tree**: Rare in interviews, but shows deep understanding
- **Multi-attribute**: Common in system design questions
- **Concurrent**: Important for senior positions
- **Persistent**: Mostly theoretical, impresses interviewers
