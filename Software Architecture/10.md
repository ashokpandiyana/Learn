# Chapter 10: Load Balancing

## Table of Contents
1. Introduction to Load Balancing
2. Types of Load Balancers
3. Load Balancing Algorithms
4. Layer 4 vs Layer 7 Load Balancing
5. Health Checks
6. Session Persistence (Sticky Sessions)
7. Implementation Examples
8. Hardware vs Software Load Balancers
9. Global Server Load Balancing (GSLB)
10. Advanced Concepts
11. Real-World Scenarios

---

## 1. Introduction to Load Balancing

**Definition:** Load balancing is the process of distributing network traffic across multiple servers to ensure no single server bears too much load, improving responsiveness and availability.

### Why Load Balancing?

**Without Load Balancer:**
```
All traffic → Single Server
- Server gets overwhelmed
- Slow response times
- Single point of failure
- Limited by one server's capacity
```

**With Load Balancer:**
```
Traffic → Load Balancer → Distributes to multiple servers
- No single server overwhelmed
- Fast response times
- High availability (if one fails, others continue)
- Can handle more traffic by adding servers
```

### Basic Architecture

```
┌─────────┐
│ Client  │
└────┬────┘
     │
     ▼
┌─────────────────┐
│ Load Balancer   │ ← Distributes traffic
└────┬────────────┘
     │
     ├──────────┬──────────┬──────────┐
     ▼          ▼          ▼          ▼
┌────────┐ ┌────────┐ ┌────────┐ ┌────────┐
│Server 1│ │Server 2│ │Server 3│ │Server 4│
└────────┘ └────────┘ └────────┘ └────────┘
```

### Benefits

1. **High Availability** - If one server fails, traffic routes to healthy servers
2. **Scalability** - Add more servers to handle increased traffic
3. **Performance** - Distribute load evenly across servers
4. **Flexibility** - Deploy updates without downtime (rolling deployments)
5. **Security** - Hide backend servers, provide SSL termination

---

## 2. Types of Load Balancers

### 2.1 Hardware Load Balancers

**Examples:** F5 Networks, Citrix ADC, A10 Networks

**Characteristics:**
- Physical appliances
- Very high performance (millions of requests/second)
- Expensive ($10,000 - $100,000+)
- Complex to configure
- Limited flexibility

**Use Case:** Large enterprises with very high traffic and budget

### 2.2 Software Load Balancers

**Examples:** Nginx, HAProxy, Apache Traffic Server

**Characteristics:**
- Run on commodity hardware or VMs
- Cost-effective (often free/open source)
- Flexible and programmable
- Easy to scale horizontally
- Community support

**Use Case:** Most modern applications

### 2.3 Cloud Load Balancers

**Examples:**
- **AWS:** Application Load Balancer (ALB), Network Load Balancer (NLB), Classic Load Balancer
- **Google Cloud:** Cloud Load Balancing
- **Azure:** Azure Load Balancer, Application Gateway

**Characteristics:**
- Fully managed (no maintenance)
- Pay per use
- Auto-scaling
- Integration with cloud services
- High availability built-in

**Use Case:** Cloud-native applications

---

## 3. Load Balancing Algorithms

### 3.1 Round Robin

**How it works:** Distribute requests sequentially across all servers.

```
Request 1 → Server 1
Request 2 → Server 2
Request 3 → Server 3
Request 4 → Server 1 (back to start)
Request 5 → Server 2
...
```

**Pros:**
- Simple to implement
- Fair distribution
- Works well when all servers have equal capacity

**Cons:**
- Doesn't consider server load
- Doesn't account for different server capabilities

**Code Example (Node.js):**
```javascript
class RoundRobinLoadBalancer {
    constructor(servers) {
        this.servers = servers;
        this.currentIndex = 0;
    }
    
    getNextServer() {
        const server = this.servers[this.currentIndex];
        this.currentIndex = (this.currentIndex + 1) % this.servers.length;
        return server;
    }
}

// Usage
const lb = new RoundRobinLoadBalancer([
    'http://server1:3000',
    'http://server2:3000',
    'http://server3:3000'
]);

console.log(lb.getNextServer());  // http://server1:3000
console.log(lb.getNextServer());  // http://server2:3000
console.log(lb.getNextServer());  // http://server3:3000
console.log(lb.getNextServer());  // http://server1:3000
```

### 3.2 Weighted Round Robin

**How it works:** Like Round Robin, but servers with higher weights receive more traffic.

```
Servers:
- Server 1: Weight 3
- Server 2: Weight 2
- Server 3: Weight 1

Distribution:
Request 1 → Server 1
Request 2 → Server 1
Request 3 → Server 1
Request 4 → Server 2
Request 5 → Server 2
Request 6 → Server 3
(repeat)
```

**Use Case:** When servers have different capacities (e.g., one server has 4 CPUs, another has 2 CPUs).

**Code Example:**
```python
class WeightedRoundRobinLoadBalancer:
    def __init__(self, servers_with_weights):
        """
        servers_with_weights: [(server, weight), ...]
        Example: [('server1', 3), ('server2', 2), ('server3', 1)]
        """
        self.servers = []
        for server, weight in servers_with_weights:
            self.servers.extend([server] * weight)
        self.current_index = 0
    
    def get_next_server(self):
        server = self.servers[self.current_index]
        self.current_index = (self.current_index + 1) % len(self.servers)
        return server

# Usage
lb = WeightedRoundRobinLoadBalancer([
    ('http://server1:3000', 3),  # Powerful server
    ('http://server2:3000', 2),  # Medium server
    ('http://server3:3000', 1)   # Weaker server
])

for i in range(6):
    print(f"Request {i+1}: {lb.get_next_server()}")

# Output:
# Request 1: http://server1:3000
# Request 2: http://server1:3000
# Request 3: http://server1:3000
# Request 4: http://server2:3000
# Request 5: http://server2:3000
# Request 6: http://server3:3000
```

### 3.3 Least Connections

**How it works:** Send new request to server with fewest active connections.

```
Current state:
- Server 1: 10 connections
- Server 2: 5 connections  ← Choose this
- Server 3: 8 connections

New request → Server 2
```

**Pros:**
- Better than Round Robin for long-lived connections
- Considers actual server load
- Handles different request durations well

**Cons:**
- Slightly more complex
- Needs to track connection counts

**Code Example:**
```java
import java.util.*;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.atomic.AtomicInteger;

public class LeastConnectionsLoadBalancer {
    private List<String> servers;
    private Map<String, AtomicInteger> connections;
    
    public LeastConnectionsLoadBalancer(List<String> servers) {
        this.servers = servers;
        this.connections = new ConcurrentHashMap<>();
        
        for (String server : servers) {
            connections.put(server, new AtomicInteger(0));
        }
    }
    
    public String getNextServer() {
        String selectedServer = null;
        int minConnections = Integer.MAX_VALUE;
        
        // Find server with least connections
        for (String server : servers) {
            int currentConnections = connections.get(server).get();
            if (currentConnections < minConnections) {
                minConnections = currentConnections;
                selectedServer = server;
            }
        }
        
        // Increment connection count
        connections.get(selectedServer).incrementAndGet();
        
        return selectedServer;
    }
    
    public void releaseConnection(String server) {
        connections.get(server).decrementAndGet();
    }
}

// Usage
LeastConnectionsLoadBalancer lb = new LeastConnectionsLoadBalancer(
    Arrays.asList("server1", "server2", "server3")
);

String server = lb.getNextServer();
System.out.println("Routing to: " + server);
// ... handle request ...
lb.releaseConnection(server);
```

### 3.4 Least Response Time

**How it works:** Route to server with lowest average response time + fewest connections.

```
Metric = (Response Time × Active Connections)

Server 1: 50ms × 5 connections = 250
Server 2: 30ms × 10 connections = 300
Server 3: 40ms × 6 connections = 240  ← Choose this

New request → Server 3
```

**Pros:**
- Considers both load and performance
- Best overall performance
- Adapts to server health

**Cons:**
- Most complex to implement
- Requires response time tracking

### 3.5 IP Hash

**How it works:** Use client's IP address to determine which server to use. Same client always goes to same server.

```
hash(client_ip) % num_servers = server_index

Client 1.2.3.4 → hash → Server 2
Client 5.6.7.8 → hash → Server 1
Client 1.2.3.4 → hash → Server 2 (same as before)
```

**Pros:**
- Session persistence without sticky sessions
- Simple to implement
- Deterministic

**Cons:**
- Uneven distribution if few clients
- Adding/removing servers changes hash

**Code Example:**
```typescript
import crypto from 'crypto';

class IPHashLoadBalancer {
    private servers: string[];
    
    constructor(servers: string[]) {
        this.servers = servers;
    }
    
    getServer(clientIP: string): string {
        // Hash the IP address
        const hash = crypto.createHash('md5')
            .update(clientIP)
            .digest('hex');
        
        // Convert hex to number
        const hashNumber = parseInt(hash.substring(0, 8), 16);
        
        // Modulo to get server index
        const serverIndex = hashNumber % this.servers.length;
        
        return this.servers[serverIndex];
    }
}

// Usage
const lb = new IPHashLoadBalancer([
    'http://server1:3000',
    'http://server2:3000',
    'http://server3:3000'
]);

console.log(lb.getServer('192.168.1.100'));  // Always same server
console.log(lb.getServer('192.168.1.101'));  // Different server
console.log(lb.getServer('192.168.1.100'));  // Same as first
```

### 3.6 Random

**How it works:** Randomly select a server.

```
Request 1 → Random → Server 2
Request 2 → Random → Server 1
Request 3 → Random → Server 3
```

**Pros:**
- Simple
- Good distribution over time

**Cons:**
- Can be uneven in short term
- Not deterministic

---

## 4. Layer 4 vs Layer 7 Load Balancing

### Layer 4 (Transport Layer) Load Balancing

**Works at:** TCP/UDP level

**What it sees:**
- Source IP and Port
- Destination IP and Port
- Protocol (TCP/UDP)

**What it CANNOT see:**
- HTTP headers
- URL path
- Cookies
- Request content

**Characteristics:**
- Very fast (just forwards packets)
- Lower latency
- Cannot make routing decisions based on content
- Cannot do SSL termination at this layer

**Example:**
```
Client connects to: load-balancer.com:443
Load balancer forwards to: server1:443
(just forwards TCP packets, doesn't inspect content)
```

### Layer 7 (Application Layer) Load Balancing

**Works at:** HTTP/HTTPS level

**What it sees:**
- HTTP headers
- URL path
- Cookies
- Request body
- HTTP methods

**What it can do:**
- Route based on URL (/api → API servers, /static → CDN)
- SSL termination
- Content-based routing
- HTTP header manipulation
- Request/response modification

**Characteristics:**
- Slower than Layer 4 (must parse HTTP)
- More intelligent routing
- Can inspect and modify content
- SSL termination possible

**Example:**
```
Client → Load Balancer (HTTPS)
Load Balancer inspects URL:
- /api/users → API server pool
- /api/orders → Order server pool
- /images → CDN
- /admin → Admin server pool
```

### Comparison

| Aspect | Layer 4 | Layer 7 |
|--------|---------|---------|
| **Speed** | Very Fast | Slower |
| **Latency** | < 1ms | 1-10ms |
| **Content Awareness** | No | Yes |
| **SSL Termination** | No | Yes |
| **URL Routing** | No | Yes |
| **Header Inspection** | No | Yes |
| **Cost** | Lower | Higher |
| **Use Case** | Simple forwarding | Complex routing |

### Implementation Example

**Layer 4 (Nginx TCP):**
```nginx
# nginx.conf
stream {
    upstream backend {
        server 192.168.1.10:3000;
        server 192.168.1.11:3000;
        server 192.168.1.12:3000;
    }
    
    server {
        listen 80;
        proxy_pass backend;
    }
}
```

**Layer 7 (Nginx HTTP):**
```nginx
# nginx.conf
http {
    upstream api_servers {
        server 192.168.1.10:3000;
        server 192.168.1.11:3000;
    }
    
    upstream static_servers {
        server 192.168.1.20:8080;
        server 192.168.1.21:8080;
    }
    
    server {
        listen 80;
        
        # Route based on URL path
        location /api/ {
            proxy_pass http://api_servers;
        }
        
        location /static/ {
            proxy_pass http://static_servers;
        }
        
        location /admin/ {
            # Additional authentication
            auth_basic "Admin Area";
            auth_basic_user_file /etc/nginx/.htpasswd;
            proxy_pass http://api_servers;
        }
    }
}
```

---

## 5. Health Checks

**Definition:** Health checks are periodic tests to determine if backend servers are healthy and able to handle traffic.

### Types of Health Checks

**1. Passive Health Checks**
- Monitor actual traffic
- Mark server unhealthy if requests fail
- No additional overhead
- Slower to detect failures

**2. Active Health Checks**
- Proactively ping servers
- Send test requests periodically
- Faster failure detection
- Small overhead

### Health Check Methods

**1. TCP Check**
```python
import socket

def tcp_health_check(host, port, timeout=5):
    """Check if server accepts TCP connections"""
    try:
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        sock.settimeout(timeout)
        result = sock.connect_ex((host, port))
        sock.close()
        return result == 0  # 0 means success
    except:
        return False

# Usage
if tcp_health_check('192.168.1.10', 3000):
    print("Server is healthy")
else:
    print("Server is down")
```

**2. HTTP Health Check**
```javascript
const axios = require('axios');

async function httpHealthCheck(url, timeout = 5000) {
    try {
        const response = await axios.get(url, { timeout });
        // Consider healthy if status 200-299
        return response.status >= 200 && response.status < 300;
    } catch (error) {
        return false;
    }
}

// Usage
const servers = [
    'http://server1:3000/health',
    'http://server2:3000/health',
    'http://server3:3000/health'
];

async function checkAllServers() {
    for (const server of servers) {
        const isHealthy = await httpHealthCheck(server);
        console.log(`${server}: ${isHealthy ? 'Healthy' : 'Unhealthy'}`);
    }
}

setInterval(checkAllServers, 10000);  // Check every 10 seconds
```

**3. Application-Level Health Check**
```go
// health.go - Comprehensive health check endpoint
package main

import (
    "database/sql"
    "encoding/json"
    "net/http"
    "time"
)

type HealthStatus struct {
    Status      string            `json:"status"`
    Version     string            `json:"version"`
    Uptime      int64             `json:"uptime"`
    Checks      map[string]bool   `json:"checks"`
}

var startTime = time.Now()

func healthCheckHandler(w http.ResponseWriter, r *http.Request) {
    health := HealthStatus{
        Status:  "healthy",
        Version: "1.0.0",
        Uptime:  int64(time.Since(startTime).Seconds()),
        Checks:  make(map[string]bool),
    }
    
    // Check database
    health.Checks["database"] = checkDatabase()
    
    // Check Redis
    health.Checks["redis"] = checkRedis()
    
    // Check disk space
    health.Checks["disk"] = checkDiskSpace()
    
    // Determine overall health
    for _, check := range health.Checks {
        if !check {
            health.Status = "unhealthy"
            w.WriteHeader(http.StatusServiceUnavailable)
            break
        }
    }
    
    json.NewEncoder(w).Encode(health)
}

func checkDatabase() bool {
    // Ping database
    err := db.Ping()
    return err == nil
}

func checkRedis() bool {
    // Ping Redis
    _, err := redisClient.Ping().Result()
    return err == nil
}

func checkDiskSpace() bool {
    // Check if disk has > 10% free space
    // Implementation...
    return true
}
```

### Health Check Configuration

**Nginx Example:**
```nginx
upstream backend {
    server 192.168.1.10:3000;
    server 192.168.1.11:3000;
    server 192.168.1.12:3000;
    
    # Health check settings
    check interval=3000   # Check every 3 seconds
          rise=2          # Consider healthy after 2 successes
          fall=3          # Consider unhealthy after 3 failures
          timeout=1000    # Timeout after 1 second
          type=http;      # HTTP health check
    
    check_http_send "GET /health HTTP/1.0\r\n\r\n";
    check_http_expect_alive http_2xx http_3xx;
}
```

**HAProxy Example:**
```
backend api_servers
    mode http
    balance roundrobin
    
    option httpchk GET /health
    http-check expect status 200
    
    server server1 192.168.1.10:3000 check inter 5s fall 3 rise 2
    server server2 192.168.1.11:3000 check inter 5s fall 3 rise 2
    server server3 192.168.1.12:3000 check inter 5s fall 3 rise 2
    
    # inter: interval between checks (5 seconds)
    # fall: failures before marking unhealthy (3)
    # rise: successes before marking healthy (2)
```

---

## 6. Session Persistence (Sticky Sessions)

**Problem:** User's session data is stored on one server. If next request goes to different server, session is lost.

```
Request 1 → Server 1 (creates session)
Request 2 → Server 2 (session not found!)
```

### Solution 1: Sticky Sessions (Session Affinity)

Route all requests from same user to same server.

**Implementation Methods:**

**1. Cookie-Based:**
```nginx
upstream backend {
    server 192.168.1.10:3000;
    server 192.168.1.11:3000;
    server 192.168.1.12:3000;
    
    # Use cookie for stickiness
    sticky cookie srv_id expires=1h domain=.example.com path=/;
}
```

**2. IP Hash:**
```nginx
upstream backend {
    ip_hash;  # Same IP → Same server
    server 192.168.1.10:3000;
    server 192.168.1.11:3000;
    server 192.168.1.12:3000;
}
```

**Pros:**
- Simple to implement
- Works with in-memory sessions

**Cons:**
- Uneven load if some users more active
- If server dies, users lose sessions
- Hard to deploy updates

### Solution 2: Shared Session Store

Store sessions in central location.

```
Request → Any Server → Redis/Database (shared sessions)
```

**Implementation:**
```javascript
// Express.js with Redis session store
const express = require('express');
const session = require('express-session');
const RedisStore = require('connect-redis')(session);
const redis = require('redis');

const app = express();
const redisClient = redis.createClient({
    host: 'redis-server',
    port: 6379
});

app.use(session({
    store: new RedisStore({ client: redisClient }),
    secret: 'your-secret-key',
    resave: false,
    saveUninitialized: false,
    cookie: {
        maxAge: 1000 * 60 * 60  // 1 hour
    }
}));

app.get('/login', (req, res) => {
    req.session.userId = '12345';
    req.session.username = 'john';
    res.send('Logged in');
});

app.get('/profile', (req, res) => {
    // Works on any server - session in Redis
    if (req.session.userId) {
        res.send(`Welcome ${req.session.username}`);
    } else {
        res.send('Not logged in');
    }
});

app.listen(3000);
```

**Pros:**
- Any server can handle any request
- Even load distribution
- Easy rolling deployments
- Sessions survive server restarts

**Cons:**
- Adds latency (network call to Redis)
- Redis is single point of failure (mitigate with Redis cluster)
- Additional infrastructure

### Solution 3: JWT (Stateless Sessions)

Store session data in client-side token.

```javascript
const jwt = require('jsonwebtoken');

// Login - create JWT
app.post('/login', (req, res) => {
    const user = { id: '12345', username: 'john' };
    
    const token = jwt.sign(user, 'secret-key', { expiresIn: '1h' });
    
    res.json({ token });
});

// Protected route - verify JWT
app.get('/profile', (req, res) => {
    const token = req.headers.authorization?.split(' ')[1];
    
    try {
        const decoded = jwt.verify(token, 'secret-key');
        res.json({ username: decoded.username });
    } catch (error) {
        res.status(401).json({ error: 'Invalid token' });
    }
});
```

**Pros:**
- Completely stateless
- No session storage needed
- Scales infinitely
- Works across domains

**Cons:**
- Cannot revoke tokens (until expiry)
- Token size (sent with every request)
- Must include all needed data in token

---

## 7. Implementation Examples

### Example 1: Simple Load Balancer in Node.js

```javascript
const http = require('http');
const httpProxy = require('http-proxy');

class LoadBalancer {
    constructor(servers) {
        this.servers = servers;
        this.currentIndex = 0;
        this.proxy = httpProxy.createProxyServer();
        
        // Handle proxy errors
        this.proxy.on('error', (err, req, res) => {
            console.error('Proxy error:', err);
            res.writeHead(500, { 'Content-Type': 'text/plain' });
            res.end('Proxy error');
        });
    }
    
    getNextServer() {
        const server = this.servers[this.currentIndex];
        this.currentIndex = (this.currentIndex + 1) % this.servers.length;
        return server;
    }
    
    handleRequest(req, res) {
        const target = this.getNextServer();
        console.log(`Routing ${req.url} to ${target}`);
        this.proxy.web(req, res, { target });
    }
    
    start(port) {
        const server = http.createServer((req, res) => {
            this.handleRequest(req, res);
        });
        
        server.listen(port, () => {
            console.log(`Load balancer listening on port ${port}`);
        });
    }
}

// Usage
const lb = new LoadBalancer([
    'http://localhost:3001',
    'http://localhost:3002',
    'http://localhost:3003'
]);

lb.start(8080);
```

### Example 2: Advanced Load Balancer with Health Checks

```python
import asyncio
import aiohttp
from aiohttp import web
import time

class Server:
    def __init__(self, url):
        self.url = url
        self.healthy = True
        self.connections = 0
        self.total_response_time = 0
        self.request_count = 0
    
    @property
    def avg_response_time(self):
        if self.request_count == 0:
            return 0
        return self.total_response_time / self.request_count
    
    def record_response(self, response_time):
        self.connections -= 1
        self.total_response_time += response_time
        self.request_count += 1

class AdvancedLoadBalancer:
    def __init__(self, server_urls):
        self.servers = [Server(url) for url in server_urls]
        self.session = None
    
    async def start(self):
        self.session = aiohttp.ClientSession()
        # Start health check loop
        asyncio.create_task(self.health_check_loop())
    
    async def health_check_loop(self):
        """Check server health every 5 seconds"""
        while True:
            for server in self.servers:
                try:
                    async with self.session.get(
                        f"{server.url}/health",
                        timeout=aiohttp.ClientTimeout(total=2)
                    ) as response:
                        server.healthy = response.status == 200
                except:
                    server.healthy = False
                
                status = "✓" if server.healthy else "✗"
                print(f"Health check {server.url}: {status}")
            
            await asyncio.sleep(5)
    
    def get_best_server(self):
        """Get server with least response time and connections"""
        healthy_servers = [s for s in self.servers if s.healthy]
        
        if not healthy_servers:
            return None
        
        # Choose server with best score
        best_server = min(
            healthy_servers,
            key=lambda s: s.avg_response_time * (s.connections + 1)
        )
        
        best_server.connections += 1
        return best_server
    
    async def handle_request(self, request):
        server = self.get_best_server()
        
        if not server:
            return web.Response(
                status=503,
                text='All servers unavailable'
            )
        
        start_time = time.time()
        
        try:
            # Forward request to backend
            async with self.session.request(
                method=request.method,
                url=f"{server.url}{request.path}",
                headers=request.headers,
                data=await request.read()
            ) as response:
                body = await response.read()
                
                # Record metrics
                response_time = time.time() - start_time
                server.record_response(response_time)
                
                return web.Response(
                    status=response.status,
                    body=body,
                    headers=response.headers
                )
        
        except Exception as e:
            server.connections -= 1
            print(f"Error forwarding to {server.url}: {e}")
            return web.Response(status=502, text='Bad Gateway')

# Usage
async def main():
    lb = AdvancedLoadBalancer([
        'http://localhost:3001',
        'http://localhost:3002',
        'http://localhost:3003'
    ])
    
    await lb.start()
    
    app = web.Application()
    app.router.add_route('*', '/{path:.*}', lb.handle_request)
    
    runner = web.AppRunner(app)
    await runner.setup()
    site = web.TCPSite(runner, 'localhost', 8080)
    await site.start()
    
    print('Load balancer running on http://localhost:8080')
    await asyncio.Event().wait()

if __name__ == '__main__':
    asyncio.run(main())
```

### Example 3: Production-Grade Nginx Configuration

```nginx
# /etc/nginx/nginx.conf

user nginx;
worker_processes auto;  # Auto-detect CPU cores
error_log /var/log/nginx/error.log warn;
pid /var/run/nginx.pid;

events {
    worker_connections 10000;  # Connections per worker
    use epoll;                 # Efficient on Linux
}

http {
    include /etc/nginx/mime.types;
    default_type application/octet-stream;
    
    # Logging
    log_format main '$remote_addr - $remote_user [$time_local] "$request" '
                    '$status $body_bytes_sent "$http_referer" '
                    '"$http_user_agent" "$http_x_forwarded_for" '
                    'upstream: $upstream_addr '
                    'response_time: $upstream_response_time';
    
    access_log /var/log/nginx/access.log main;
    
    # Performance
    sendfile on;
    tcp_nopush on;
    tcp_nodelay on;
    keepalive_timeout 65;
    types_hash_max_size 2048;
    
    # Gzip compression
    gzip on;
    gzip_vary on;
    gzip_proxied any;
    gzip_comp_level 6;
    gzip_types text/plain text/css application/json application/javascript;
    
    # Rate limiting
    limit_req_zone $binary_remote_addr zone=api_limit:10m rate=100r/s;
    
    # Upstream servers
    upstream api_servers {
        least_conn;  # Least connections algorithm
        
        server 10.0.1.10:3000 weight=3 max_fails=3 fail_timeout=30s;
        server 10.0.1.11:3000 weight=2 max_fails=3 fail_timeout=30s;
        server 10.0.1.12:3000 weight=2 max_fails=3 fail_timeout=30s;
        server 10.0.1.13:3000 backup;  # Backup server
        
        # Keepalive connections to backend
        keepalive 32;
    }
    
    upstream static_servers {
        server 10.0.2.10:8080;
        server 10.0.2.11:8080;
    }
    
    # Main server block
    server {
        listen 80;
        listen 443 ssl http2;
        server_name api.example.com;
        
        # SSL configuration
        ssl_certificate /etc/nginx/ssl/cert.pem;
        ssl_certificate_key /etc/nginx/ssl/key.pem;
        ssl_protocols TLSv1.2 TLSv1.3;
        ssl_ciphers HIGH:!aNULL:!MD5;
        
        # Health check endpoint
        location /health {
            access_log off;
            return 200 "healthy\n";
            add_header Content-Type text/plain;
        }
        
        # API endpoints
        location /api/ {
            # Rate limiting
            limit_req zone=api_limit burst=20 nodelay;
            
            # Proxy settings
            proxy_pass http://api_servers;
            proxy_http_version 1.1;
            
            # Headers
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            proxy_set_header Connection "";
            
            # Timeouts
            proxy_connect_timeout 5s;
            proxy_send_timeout 60s;
            proxy_read_timeout 60s;
            
            # Buffering
            proxy_buffering on;
            proxy_buffer_size 4k;
            proxy_buffers 8 4k;
            
            # Retry logic
            proxy_next_upstream error timeout http_502 http_503 http_504;
            proxy_next_upstream_tries 3;
        }
        
        # Static files
        location /static/ {
            proxy_pass http://static_servers;
            proxy_cache static_cache;
            proxy_cache_valid 200 1h;
            proxy_cache_valid 404 1m;
            add_header X-Cache-Status $upstream_cache_status;
        }
    }
    
    # Cache configuration
    proxy_cache_path /var/cache/nginx/static 
                     levels=1:2 
                     keys_zone=static_cache:10m 
                     max_size=1g 
                     inactive=1h;
}
```

---

## 8. Hardware vs Software Load Balancers

### Hardware Load Balancers

**Examples:** F5 BIG-IP, Citrix NetScaler, A10 Networks

**Pros:**
- Very high performance (10+ Gbps throughput)
- Dedicated hardware
- Advanced features (WAF, SSL acceleration)
- Vendor support

**Cons:**
- Expensive ($10K-$100K+)
- Limited flexibility
- Difficult to scale
- Vendor lock-in
- Complex configuration

**Use Case:** Large enterprises, financial institutions, very high traffic

### Software Load Balancers

**Examples:** Nginx, HAProxy, Envoy

**Pros:**
- Cost-effective (free/cheap)
- Flexible and programmable
- Easy to scale horizontally
- Large community
- Cloud-friendly

**Cons:**
- Requires more management
- Performance depends on hardware
- May need more instances for high traffic

**Use Case:** Most modern applications, startups, cloud-native

### Cloud Load Balancers

**Examples:** AWS ALB/NLB, Google Cloud Load Balancing, Azure Load Balancer

**Pros:**
- Fully managed
- Auto-scaling
- Pay-as-you-go
- High availability built-in
- Easy setup

**Cons:**
- Vendor lock-in
- Cost can be high at scale
- Less control

**Use Case:** Cloud-native applications

---

## 9. Global Server Load Balancing (GSLB)

**Definition:** Route users to geographically closest data center.

```
User in US → US Data Center
User in Europe → EU Data Center
User in Asia → Asia Data Center
```

### Benefits

1. **Reduced Latency** - Users connect to nearest server
2. **Disaster Recovery** - Failover to different region
3. **Compliance** - Keep data in specific regions
4. **Load Distribution** - Spread traffic globally

### Implementation

**DNS-Based GSLB:**
```
User in US queries DNS:
api.example.com → 1.2.3.4 (US IP)

User in Europe queries DNS:
api.example.com → 5.6.7.8 (EU IP)
```

**GeoDNS Configuration (AWS Route 53):**
```json
{
    "Name": "api.example.com",
    "Type": "A",
    "GeoLocation": {
        "ContinentCode": "NA"
    },
    "ResourceRecords": [
        {"Value": "1.2.3.4"}
    ]
}

{
    "Name": "api.example.com",
    "Type": "A",
    "GeoLocation": {
        "ContinentCode": "EU"
    },
    "ResourceRecords": [
        {"Value": "5.6.7.8"}
    ]
}
```

---

## 10. Advanced Concepts

### Connection Draining

**Problem:** Server being removed still has active connections.

**Solution:** Stop sending new traffic but allow existing connections to complete.

```nginx
# Mark server as draining
upstream backend {
    server 10.0.1.10:3000;
    server 10.0.1.11:3000 down;  # Draining - no new traffic
    server 10.0.1.12:3000;
}
```

### SSL/TLS Termination

**Definition:** Load balancer handles SSL, communicates with backends over HTTP.

```
Client → HTTPS → Load Balancer → HTTP → Backend Servers
         (encrypted)              (unencrypted)
```

**Benefits:**
- Reduce CPU load on backend servers
- Centralized certificate management
- Easier to monitor traffic

**Configuration:**
```nginx
server {
    listen 443 ssl;
    ssl_certificate /path/to/cert.pem;
    ssl_certificate_key /path/to/key.pem;
    
    location / {
        # Forward as HTTP
        proxy_pass http://backend_servers;
    }
}
```

### WebSocket Load Balancing

**Challenge:** WebSocket requires persistent connection.

```nginx
map $http_upgrade $connection_upgrade {
    default upgrade;
    '' close;
}

upstream websocket {
    ip_hash;  # Same client → Same server
    server 10.0.1.10:3000;
    server 10.0.1.11:3000;
}

server {
    listen 80;
    
    location /ws {
        proxy_pass http://websocket;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection $connection_upgrade;
        proxy_set_header Host $host;
        
        # Timeouts for long-lived connections
        proxy_read_timeout 3600s;
        proxy_send_timeout 3600s;
    }
}
```

---

## 11. Real-World Scenarios

### Scenario 1: E-Commerce Website

**Requirements:**
- Handle Black Friday traffic spikes
- Static content (images, CSS, JS)
- Dynamic API calls
- Shopping cart sessions

**Solution:**
```
┌─────────────────────────────────────────────┐
│           CloudFlare CDN (Global)           │
│      Cache static content at edge          │
└────────────┬────────────────────────────────┘
             │
┌────────────▼────────────────────────────────┐
│         AWS Application Load Balancer       │
│   - SSL Termination                        │
│   - Path-based routing                     │
└────┬────────────────┬───────────────────────┘
     │                │
     ▼                ▼
┌─────────┐      ┌─────────┐
│API Nodes│      │Static   │
│(10 nodes)│     │Servers  │
│Redis    │      │(5 nodes)│
│Sessions │      │         │
└─────────┘      └─────────┘
```

**Configuration:**
- CloudFlare for global CDN
- ALB with path-based routing (/api → API servers, /static → Static servers)
- Redis for shared sessions
- Auto-scaling based on CPU

### Scenario 2: Microservices Architecture

**Requirements:**
- 20 different microservices
- Service discovery
- Circuit breaking
- Distributed tracing

**Solution:**
```
Use Service Mesh (Istio/Linkerd):
- Automatic load balancing between service instances
- Circuit breaking
- Retries
- Observability

Or API Gateway:
- Kong/AWS API Gateway
- Route /users → User Service
- Route /orders → Order Service
- Rate limiting per service
```

---

## Chapter 10 Summary

### Key Concepts

1. **Load Balancing** = Distribute traffic across multiple servers
2. **Algorithms**: Round Robin, Least Connections, IP Hash, Least Response Time
3. **Layer 4** = Fast TCP/UDP forwarding
4. **Layer 7** = Intelligent HTTP routing
5. **Health Checks** = Monitor server health
6. **Session Persistence** = Keep user on same server OR use shared session store

### Load Balancing Algorithms Comparison

| Algorithm | Best For | Complexity |
|-----------|----------|------------|
| **Round Robin** | Equal servers, short requests | Simple |
| **Weighted RR** | Unequal server capacity | Simple |
| **Least Connections** | Long-lived connections | Medium |
| **Least Response Time** | Variable server performance | Complex |
| **IP Hash** | Session persistence | Simple |

### Implementation Options

| Option | Cost | Flexibility | Performance | Management |
|--------|------|-------------|-------------|------------|
| **Hardware** | Very High | Low | Very High | Complex |
| **Software** | Low | High | High | Moderate |
| **Cloud** | Medium | Medium | High | Easy |

### Interview Tips

**Common Questions:**
1. "Explain load balancing algorithms"
2. "Layer 4 vs Layer 7 - differences?"
3. "How do you handle sessions in load balanced environment?"
4. "What are health checks?"
5. "How would you load balance WebSockets?"

**How to Answer:**
- Draw diagram showing load balancer and servers
- Explain trade-offs between algorithms
- Mention session store (Redis) for state
- Discuss health checks with examples
- Compare Layer 4 vs Layer 7 with use cases

### Best Practices

1. **Always use health checks**
2. **Prefer shared session store over sticky sessions**
3. **Use Layer 7 for complex routing needs**
4. **Monitor metrics**: request rate, error rate, latency
5. **Plan for scale**: Auto-scaling rules
6. **Test failover**: What happens when server dies?

### Real-World Tips

- Start with cloud load balancers (easiest)
- Use CDN for static content (CloudFlare, CloudFront)
- Implement proper health check endpoints
- Monitor backend server metrics
- Test under load (load testing tools)
- Have runbook for common issues

You've now completed Chapters 8-10! These cover fundamental architectural patterns and infrastructure concepts essential for system design interviews.