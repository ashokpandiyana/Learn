# Chapter 15: Horizontal vs Vertical Scaling

## Table of Contents
1. Introduction to Scaling
2. Vertical Scaling (Scale Up)
3. Horizontal Scaling (Scale Out)
4. Stateless vs Stateful Services
5. Session Management in Scaled Systems
6. Auto-Scaling Strategies
7. Database Scaling
8. Combining Vertical and Horizontal Scaling
9. Real-World Examples
10. Decision Framework

---

## 1. Introduction to Scaling

**Definition:** Scaling is the ability to handle increased load by adding resources.

### Why Scale?

```
Year 1:  1,000 users    → 1 server handles fine
Year 2:  10,000 users   → Server struggling
Year 3:  100,000 users  → Need to scale!
Year 4:  1,000,000 users → Must scale efficiently
```

### Types of Scaling

**Vertical (Scale Up):** Add more resources to existing server
**Horizontal (Scale Out):** Add more servers

---

## 2. Vertical Scaling (Scale Up)

**Definition:** Increase the capacity of a single server (more CPU, RAM, disk).

### Example

```
Before:
Server: 2 CPU cores, 4GB RAM, 100GB SSD
Can handle: 1,000 requests/second

After Vertical Scaling:
Server: 16 CPU cores, 64GB RAM, 1TB SSD
Can handle: 8,000 requests/second
```

### Advantages

✅ **Simple**
- No code changes required
- No architectural changes
- Just upgrade hardware

✅ **Consistency**
- Single database (ACID transactions work)
- No distributed system complexity
- Easier to maintain

✅ **Lower Latency**
- No network calls between nodes
- In-process memory access

### Disadvantages

❌ **Hardware Limits**
```
Physical limits:
- Max CPU cores: ~128
- Max RAM: ~4TB
- Max cost: Exponentially increases
```

❌ **Single Point of Failure**
```
1 server down = Entire system down
No redundancy
```

❌ **Downtime for Upgrades**
```
Upgrading hardware:
1. Shut down server
2. Upgrade hardware
3. Restart
= Downtime required
```

❌ **Cost Inefficient**
```
2x CPU = 3x cost (not linear)
4x CPU = 10x cost
16x CPU = 50x cost
```

### When to Use Vertical Scaling

✅ **Small to medium applications**
✅ **Complex transactions** requiring single database
✅ **Legacy applications** not designed for distribution
✅ **Temporary solution** while planning horizontal scaling

### Implementation

```python
# No code changes needed for vertical scaling!
# Just upgrade server specs

# Before: t2.small (1 vCPU, 2GB RAM)
# After:  t2.large (2 vCPU, 8GB RAM)
# Or:     t2.2xlarge (8 vCPU, 32GB RAM)

# Application code remains the same
class Application:
    def __init__(self):
        self.db = Database()
    
    def handle_request(self, request):
        # Same code, more resources available
        return self.db.query(request)
```

---

## 3. Horizontal Scaling (Scale Out)

**Definition:** Add more servers to distribute the load.

### Example

```
Before:
1 server: 2 CPU, 4GB RAM
Handles: 1,000 req/sec

After Horizontal Scaling:
10 servers: 2 CPU, 4GB RAM each
Handles: 10,000 req/sec
```

### Advantages

✅ **No Hardware Limits**
```
Need more capacity?
→ Add more servers
→ Theoretically infinite scaling
```

✅ **High Availability**
```
Server 1 crashes → Servers 2, 3, 4 continue
No single point of failure
```

✅ **Cost Effective**
```
10x small servers = ~10x cost (linear)
Much cheaper than 1 huge server
```

✅ **No Downtime for Scaling**
```
Add servers: No downtime
Remove servers: Rolling updates
```

✅ **Geographic Distribution**
```
Server in US
Server in Europe
Server in Asia
= Lower latency for all users
```

### Disadvantages

❌ **Complexity**
```
Need:
- Load balancer
- Service discovery
- Distributed state management
- Network overhead
```

❌ **Data Consistency**
```
Multiple servers = Data synchronization issues
Eventual consistency instead of immediate
```

❌ **Network Latency**
```
In-process call: < 1ms
Network call: 10-100ms
```

❌ **Requires Code Changes**
```
Must design for:
- Stateless services
- Distributed sessions
- Shared caching
```

### Implementing Horizontal Scaling

```javascript
// ============================================
// Stateless Service (Horizontally Scalable)
// ============================================
const express = require('express');
const Redis = require('ioredis');

const app = express();
const redis = new Redis({ host: 'redis-server' });

// ✅ Stateless - any server can handle any request
app.get('/api/users/:id', async (req, res) => {
  const userId = req.params.id;
  
  // Check shared cache
  const cached = await redis.get(`user:${userId}`);
  if (cached) {
    return res.json(JSON.parse(cached));
  }
  
  // Query database
  const user = await db.query('SELECT * FROM users WHERE id = ?', [userId]);
  
  // Store in shared cache
  await redis.setex(`user:${userId}`, 3600, JSON.stringify(user));
  
  res.json(user);
});

// ✅ Stateless - session in Redis (shared)
const session = require('express-session');
const RedisStore = require('connect-redis')(session);

app.use(session({
  store: new RedisStore({ client: redis }),
  secret: 'your-secret',
  resave: false,
  saveUninitialized: false
}));

app.post('/api/login', async (req, res) => {
  // Session stored in Redis (accessible from any server)
  req.session.userId = user.id;
  res.json({ success: true });
});

// Start multiple instances
const PORT = process.env.PORT || 3000;
app.listen(PORT, () => {
  console.log(`Server running on port ${PORT}`);
});
```

**Run multiple instances:**
```bash
# Terminal 1
PORT=3001 node server.js

# Terminal 2
PORT=3002 node server.js

# Terminal 3
PORT=3003 node server.js

# Nginx load balances between them
```

---

## 4. Stateless vs Stateful Services

### Stateless Services

**Definition:** No data stored locally between requests. Each request is independent.

```javascript
// ✅ Stateless - Perfect for horizontal scaling
class UserService {
  async getUser(userId) {
    // No local state
    // Query from database or cache
    return await db.query('SELECT * FROM users WHERE id = ?', [userId]);
  }
  
  async createUser(name, email) {
    // No local state
    return await db.query('INSERT INTO users (name, email) VALUES (?, ?)', [name, email]);
  }
}

// Can run 100 instances - all identical
// Any instance can handle any request
```

### Stateful Services

**Definition:** Stores data locally. Subsequent requests depend on previous state.

```javascript
// ❌ Stateful - Hard to scale horizontally
class StatefulUserService {
  constructor() {
    this.sessionData = {};  // Local state!
  }
  
  login(userId) {
    // Stores session locally
    this.sessionData[userId] = {
      loggedIn: true,
      loginTime: Date.now()
    };
  }
  
  isLoggedIn(userId) {
    // Depends on local state
    return this.sessionData[userId]?.loggedIn || false;
  }
}

// Problem: If user logs in on Server 1,
// then next request goes to Server 2,
// Server 2 doesn't have session data!
```

### Converting Stateful to Stateless

```typescript
// ============================================
// Before: Stateful (local sessions)
// ============================================
class StatefulApp {
  private sessions: Map<string, any> = new Map();
  
  login(userId: string): void {
    this.sessions.set(userId, {
      loggedIn: true,
      loginTime: Date.now()
    });
  }
  
  isLoggedIn(userId: string): boolean {
    return this.sessions.get(userId)?.loggedIn || false;
  }
}

// ============================================
// After: Stateless (shared sessions in Redis)
// ============================================
import Redis from 'ioredis';

class StatelessApp {
  private redis: Redis;
  
  constructor() {
    this.redis = new Redis({ host: 'redis-server' });
  }
  
  async login(userId: string): Promise<void> {
    // Store in shared Redis
    await this.redis.setex(
      `session:${userId}`,
      3600,  // 1 hour
      JSON.stringify({
        loggedIn: true,
        loginTime: Date.now()
      })
    );
  }
  
  async isLoggedIn(userId: string): Promise<boolean> {
    // Check shared Redis
    const session = await this.redis.get(`session:${userId}`);
    return session ? JSON.parse(session).loggedIn : false;
  }
}

// Now any server instance can handle any request!
```

---

## 5. Session Management in Scaled Systems

### Problem

```
User logs in → Server 1 (stores session locally)
Next request → Server 2 (doesn't have session)
Result: User appears logged out!
```

### Solution 1: Sticky Sessions (Load Balancer)

```nginx
# Nginx with sticky sessions
upstream backend {
    ip_hash;  # Same IP → Same server
    server 10.0.1.10:3000;
    server 10.0.1.11:3000;
    server 10.0.1.12:3000;
}
```

**Pros:** Simple, no code changes
**Cons:** Uneven load, server failures lose sessions

### Solution 2: Shared Session Store (Recommended)

```javascript
// Express with Redis sessions
const express = require('express');
const session = require('express-session');
const RedisStore = require('connect-redis')(session);
const redis = require('redis');

const app = express();
const redisClient = redis.createClient({
  host: 'redis-cluster.example.com',
  port: 6379
});

app.use(session({
  store: new RedisStore({ client: redisClient }),
  secret: 'your-secret-key',
  resave: false,
  saveUninitialized: false,
  cookie: {
    secure: true,  // HTTPS only
    httpOnly: true,
    maxAge: 24 * 60 * 60 * 1000  // 24 hours
  }
}));

app.post('/login', (req, res) => {
  // Session stored in Redis
  req.session.userId = user.id;
  req.session.role = user.role;
  res.json({ success: true });
});

app.get('/profile', (req, res) => {
  // Works on any server - session in Redis
  if (!req.session.userId) {
    return res.status(401).json({ error: 'Not authenticated' });
  }
  
  res.json({ userId: req.session.userId });
});
```

### Solution 3: JWT (Stateless Tokens)

```javascript
const jwt = require('jsonwebtoken');

// Login - create JWT
app.post('/login', async (req, res) => {
  const user = await authenticateUser(req.body.email, req.body.password);
  
  // Create token with user data
  const token = jwt.sign(
    { 
      userId: user.id,
      email: user.email,
      role: user.role
    },
    process.env.JWT_SECRET,
    { expiresIn: '24h' }
  );
  
  res.json({ token });
});

// Protected route
app.get('/profile', (req, res) => {
  const token = req.headers.authorization?.split(' ')[1];
  
  try {
    const decoded = jwt.verify(token, process.env.JWT_SECRET);
    res.json({ user: decoded });
  } catch (error) {
    res.status(401).json({ error: 'Invalid token' });
  }
});

// No server-side state!
// Token contains all needed information
// Any server can validate token
```

---

## 6. Auto-Scaling Strategies

### Metrics-Based Auto-Scaling

```python
# ============================================
# AWS Auto-Scaling Configuration
# ============================================
import boto3

class AutoScaler:
    def __init__(self):
        self.autoscaling = boto3.client('autoscaling')
        self.cloudwatch = boto3.client('cloudwatch')
    
    def setup_cpu_based_scaling(self, asg_name):
        """Scale based on CPU utilization"""
        # Scale out policy (add instances)
        self.autoscaling.put_scaling_policy(
            AutoScalingGroupName=asg_name,
            PolicyName='scale-out-cpu',
            PolicyType='TargetTrackingScaling',
            TargetTrackingConfiguration={
                'PredefinedMetricSpecification': {
                    'PredefinedMetricType': 'ASGAverageCPUUtilization'
                },
                'TargetValue': 70.0  # Target 70% CPU
            }
        )
        
        # Configuration:
        # CPU > 70% → Add instances
        # CPU < 70% → Remove instances
    
    def setup_request_count_scaling(self, asg_name, alb_arn):
        """Scale based on requests per target"""
        self.autoscaling.put_scaling_policy(
            AutoScalingGroupName=asg_name,
            PolicyName='scale-out-requests',
            PolicyType='TargetTrackingScaling',
            TargetTrackingConfiguration={
                'PredefinedMetricSpecification': {
                    'PredefinedMetricType': 'ALBRequestCountPerTarget',
                    'ResourceLabel': alb_arn
                },
                'TargetValue': 1000.0  # 1000 requests per instance
            }
        )
    
    def setup_custom_metric_scaling(self, asg_name):
        """Scale based on custom metric (queue depth)"""
        # Create CloudWatch alarm
        self.cloudwatch.put_metric_alarm(
            AlarmName='high-queue-depth',
            ComparisonOperator='GreaterThanThreshold',
            EvaluationPeriods=2,
            MetricName='ApproximateNumberOfMessagesVisible',
            Namespace='AWS/SQS',
            Period=60,
            Statistic='Average',
            Threshold=100,
            ActionsEnabled=True,
            AlarmActions=[
                'arn:aws:autoscaling:region:account:scalingPolicy:...'
            ]
        )
```

### Kubernetes Horizontal Pod Autoscaler

```yaml
# hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: api-server-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: api-server
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300  # Wait 5 min before scaling down
      policies:
      - type: Percent
        value: 50  # Scale down max 50% at a time
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 0
      policies:
      - type: Percent
        value: 100  # Can double instances quickly
        periodSeconds: 30
```

### Predictive Auto-Scaling

```python
import boto3
from datetime import datetime, timedelta

class PredictiveScaler:
    def __init__(self):
        self.autoscaling = boto3.client('autoscaling')
        self.cloudwatch = boto3.client('cloudwatch')
    
    def get_historical_load(self, days=7):
        """Get load pattern from past week"""
        end_time = datetime.now()
        start_time = end_time - timedelta(days=days)
        
        response = self.cloudwatch.get_metric_statistics(
            Namespace='AWS/ApplicationELB',
            MetricName='RequestCountPerTarget',
            StartTime=start_time,
            EndTime=end_time,
            Period=3600,  # 1 hour buckets
            Statistics=['Average']
        )
        
        return response['Datapoints']
    
    def predict_and_scale(self, asg_name):
        """Pre-scale based on historical patterns"""
        current_hour = datetime.now().hour
        
        # Historical data shows:
        # 9 AM - 5 PM: High traffic (need 10 instances)
        # 5 PM - 9 AM: Low traffic (need 2 instances)
        
        if 9 <= current_hour < 17:
            desired_capacity = 10
        else:
            desired_capacity = 2
        
        self.autoscaling.set_desired_capacity(
            AutoScalingGroupName=asg_name,
            DesiredCapacity=desired_capacity
        )
        
        print(f"Pre-scaled to {desired_capacity} instances for hour {current_hour}")
```

---

## 7. Database Scaling

### Vertical Database Scaling

```
Before: db.t2.small (1 vCPU, 2GB RAM)
After:  db.t2.xlarge (4 vCPU, 16GB RAM)

Same approach as application scaling
```

### Horizontal Database Scaling

**Read Replicas:**

```python
# ============================================
# Read/Write Splitting
# ============================================
import psycopg2
from psycopg2 import pool

class DatabasePool:
    def __init__(self):
        # Master for writes
        self.master_pool = pool.SimpleConnectionPool(
            minconn=5,
            maxconn=20,
            host='master.db.example.com',
            database='myapp',
            user='app',
            password='secret'
        )
        
        # Replicas for reads
        self.replica_pools = [
            pool.SimpleConnectionPool(
                minconn=10,
                maxconn=50,
                host='replica1.db.example.com',
                database='myapp',
                user='app',
                password='secret'
            ),
            pool.SimpleConnectionPool(
                minconn=10,
                maxconn=50,
                host='replica2.db.example.com',
                database='myapp',
                user='app',
                password='secret'
            )
        ]
        
        self.current_replica = 0
    
    def get_write_connection(self):
        """Get connection to master"""
        return self.master_pool.getconn()
    
    def get_read_connection(self):
        """Get connection to replica (round-robin)"""
        pool = self.replica_pools[self.current_replica]
        self.current_replica = (self.current_replica + 1) % len(self.replica_pools)
        return pool.getconn()
    
    def execute_write(self, query, params):
        """Execute write query on master"""
        conn = self.get_write_connection()
        try:
            cursor = conn.cursor()
            cursor.execute(query, params)
            conn.commit()
            result = cursor.fetchall()
            return result
        finally:
            self.master_pool.putconn(conn)
    
    def execute_read(self, query, params):
        """Execute read query on replica"""
        conn = self.get_read_connection()
        try:
            cursor = conn.cursor()
            cursor.execute(query, params)
            result = cursor.fetchall()
            return result
        finally:
            # Return to appropriate pool
            for pool in self.replica_pools:
                try:
                    pool.putconn(conn)
                    break
                except:
                    continue

# Usage
db_pool = DatabasePool()

# Writes go to master
db_pool.execute_write(
    'INSERT INTO users (name, email) VALUES (%s, %s)',
    ['John', 'john@example.com']
)

# Reads go to replicas
users = db_pool.execute_read(
    'SELECT * FROM users WHERE active = %s',
    [True]
)
```

**Sharding (Horizontal Partitioning):**

```python
class ShardedDatabase:
    def __init__(self):
        self.shards = {
            0: psycopg2.connect(host='shard0.db.example.com', ...),
            1: psycopg2.connect(host='shard1.db.example.com', ...),
            2: psycopg2.connect(host='shard2.db.example.com', ...),
            3: psycopg2.connect(host='shard3.db.example.com', ...)
        }
    
    def get_shard(self, user_id):
        """Determine shard based on user_id"""
        shard_key = user_id % len(self.shards)
        return self.shards[shard_key]
    
    def get_user(self, user_id):
        """Get user from appropriate shard"""
        shard = self.get_shard(user_id)
        cursor = shard.cursor()
        cursor.execute('SELECT * FROM users WHERE id = %s', [user_id])
        return cursor.fetchone()
    
    def create_user(self, user_id, name, email):
        """Create user in appropriate shard"""
        shard = self.get_shard(user_id)
        cursor = shard.cursor()
        cursor.execute(
            'INSERT INTO users (id, name, email) VALUES (%s, %s, %s)',
            [user_id, name, email]
        )
        shard.commit()
```

---

## 8. Combining Vertical and Horizontal Scaling

### Optimal Strategy

```
Phase 1: Single Server (Vertical)
- Start: t2.small
- Traffic grows → Upgrade to t2.large
- Simple, no architecture changes

Phase 2: Add Read Replicas (Horizontal for reads)
- Master handles writes
- 2-3 replicas handle reads
- 10x read capacity

Phase 3: Scale Application Tier (Horizontal)
- 5-10 application servers
- Load balancer
- Shared sessions

Phase 4: Shard Database (Horizontal for writes)
- Only if write load overwhelms master
- Most complex, do last
```

### Example Architecture Evolution

```
Stage 1: Single Server
┌─────────────────┐
│  App + DB       │ 1,000 users
│  t2.small       │
└─────────────────┘

Stage 2: Separate DB
┌─────────────┐     ┌─────────────┐
│  App        │────→│  Database   │ 10,000 users
│  t2.medium  │     │  db.t2.large│
└─────────────┘     └─────────────┘

Stage 3: Multiple App Servers
┌──────────────┐
│Load Balancer │
└───────┬──────┘
    ┌───┴───┬───────┐
┌───▼──┐ ┌──▼───┐ ┌─▼────┐        ┌─────────────┐
│App 1 │ │App 2 │ │App 3 │───────→│  Database   │ 100,000 users
└──────┘ └──────┘ └──────┘        │+ Replicas   │
                                   └─────────────┘

Stage 4: Sharded Database
┌──────────────┐
│Load Balancer │
└───────┬──────┘
    ┌───┴───┬───────┐
┌───▼──┐ ┌──▼───┐ ┌─▼────┐
│App 1 │ │App 2 │ │App 3 │
└───┬──┘ └───┬──┘ └──┬───┘
    │        │        │
    └────────┼────────┘
             │
    ┌────────┼────────┐
┌───▼──┐ ┌──▼───┐ ┌─▼────┐        1,000,000+ users
│Shard0│ │Shard1│ │Shard2│
└──────┘ └──────┘ └──────┘
```

---

## 9. Real-World Examples

### Example 1: Instagram Scaling Journey

```
2010: Single server
2011: Vertical scaling (bigger instances)
2012: Horizontal scaling (multiple app servers)
2013: Database sharding
2014: Cassandra for timeline
2015: CDN for images
2016: Multi-region deployment

Key lesson: Start simple, scale incrementally
```

### Example 2: E-Commerce Platform

```yaml
# Current Architecture (100K daily users)
Application Tier:
  - 5 servers (auto-scale 2-10)
  - Load balancer (AWS ALB)
  - Stateless (sessions in Redis)

Cache Layer:
  - Redis cluster (3 nodes)
  - 95% cache hit ratio

Database Tier:
  - PostgreSQL master (db.m5.xlarge)
  - 2 read replicas (db.m5.large)
  - No sharding yet

CDN:
  - CloudFront for images/static assets
  - 90% traffic served from edge

Storage:
  - S3 for product images
  - CloudFront in front

Cost: ~$2,000/month
```

### Example 3: Scaling API Service

```javascript
// ============================================
// API Service Deployment
// ============================================

// docker-compose.yml
version: '3.8'
services:
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
    depends_on:
      - api1
      - api2
      - api3
  
  api1:
    build: .
    environment:
      - PORT=3000
      - REDIS_HOST=redis
      - DB_HOST=postgres-master
    depends_on:
      - redis
      - postgres-master
  
  api2:
    build: .
    environment:
      - PORT=3000
      - REDIS_HOST=redis
      - DB_HOST=postgres-master
    depends_on:
      - redis
      - postgres-master
  
  api3:
    build: .
    environment:
      - PORT=3000
      - REDIS_HOST=redis
      - DB_HOST=postgres-master
    depends_on:
      - redis
      - postgres-master
  
  redis:
    image: redis:alpine
    ports:
      - "6379:6379"
  
  postgres-master:
    image: postgres:14
    environment:
      - POSTGRES_DB=myapp
      - POSTGRES_USER=app
      - POSTGRES_PASSWORD=secret
```

---

## 10. Decision Framework

### When to Scale Vertically

✅ **Good for:**
- Databases (up to a point)
- Single-threaded applications
- Applications requiring complex transactions
- Quick temporary solution
- Small to medium scale (< 100K users)

**Example:** Your PostgreSQL is at 80% CPU

```
Option 1: Vertical (Quick)
- Upgrade from db.m5.large to db.m5.2xlarge
- Takes 5 minutes
- No code changes
- Cost: +$500/month

Do this first for quick relief
```

### When to Scale Horizontally

✅ **Good for:**
- Stateless applications
- Microservices
- High availability requirements
- Large scale (100K+ users)
- Cost optimization

**Example:** Your API servers are overwhelmed

```
Option 1: Horizontal (Better long-term)
- Add 5 more API servers
- Add load balancer
- Requires: Shared session store
- Cost: +$300/month
- Benefit: High availability + better scaling

Do this for sustained growth
```

### Hybrid Approach

```
Application: Horizontal scaling (2-20 instances)
Benefits:
- High availability
- No single point of failure
- Cost effective

Database: Vertical + Read Replicas
Benefits:
- Simpler than sharding
- Handles most workloads
- ACID transactions work

Cache: Horizontal (Redis Cluster)
Benefits:
- Scales with application
- High availability
- Distributed load
```

---

## Chapter 15 Summary

### Key Concepts

1. **Vertical Scaling** = Bigger server (Scale UP)
2. **Horizontal Scaling** = More servers (Scale OUT)
3. **Stateless** = No local state (easy to scale)
4. **Stateful** = Local state (hard to scale)
5. **Auto-Scaling** = Automatic capacity adjustment
6. **Hybrid** = Combine both strategies

### Comparison

| Aspect | Vertical | Horizontal |
|--------|----------|------------|
| **Complexity** | Low | High |
| **Limits** | Hardware ceiling | Theoretically unlimited |
| **Cost** | Exponential | Linear |
| **Availability** | Single point of failure | High availability |
| **Downtime** | Required for upgrades | Zero downtime |
| **Implementation** | No code changes | Requires architecture changes |

### Scaling Strategy

```
1. Start with vertical scaling (simple)
2. Add horizontal scaling when:
   - Traffic > 50K daily users
   - Need high availability
   - Hit hardware limits
3. Scale application tier horizontally first
4. Use read replicas for database
5. Shard database only if absolutely necessary
```

### Interview Tips

**Common Questions:**
1. "Vertical vs Horizontal scaling - differences?"
2. "How do you handle sessions in horizontally scaled system?"
3. "When would you choose vertical over horizontal?"
4. "Explain auto-scaling"

**How to Answer:**
- Draw diagrams showing both approaches
- Explain stateless design
- Discuss trade-offs
- Give real numbers (cost, capacity)
- Mention specific technologies (AWS, Kubernetes)

### Best Practices

1. **Design stateless from day 1**
2. **Use managed services** (AWS RDS, ElastiCache)
3. **Monitor metrics** (CPU, memory, request rate)
4. **Set up auto-scaling early**
5. **Test failover scenarios**
6. **Plan for 10x growth**

### Next Steps

Chapter 16 will cover **Database Scaling** in depth - advanced techniques for handling massive data volumes and high throughput.