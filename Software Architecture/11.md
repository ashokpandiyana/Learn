# Chapter 11: Caching Strategies

## Table of Contents
1. Introduction to Caching
2. Cache Aside (Lazy Loading)
3. Read Through Cache
4. Write Through Cache
5. Write Behind (Write Back) Cache
6. Cache Invalidation Strategies
7. CDN (Content Delivery Networks)
8. Implementation with Redis and Memcached
9. Cache Eviction Policies
10. Distributed Caching
11. Real-World Patterns

---

## 1. Introduction to Caching

**Definition:** Caching is storing frequently accessed data in a fast-access storage layer (cache) to reduce the load on slower storage systems and improve response times.

### Why Cache?

**Without Cache:**
```
Client → Application → Database (100ms)
Total: 100ms per request
```

**With Cache:**
```
Client → Application → Cache (1ms) → Return
Total: 1ms per request (100x faster!)

Cache Miss:
Client → Application → Cache Miss → Database (100ms) → Store in Cache → Return
Total: 101ms (only first time)
```

### Cache Performance Metrics

**1. Cache Hit Ratio**
```
Hit Ratio = Cache Hits / (Cache Hits + Cache Misses)

Example:
- 90 requests served from cache (hits)
- 10 requests went to database (misses)
- Hit Ratio = 90 / (90 + 10) = 90%

Good: > 80%
Excellent: > 95%
```

**2. Latency Reduction**
```
Database query: 100ms
Cache lookup: 1ms
Improvement: 99ms (99% faster)
```

### Cache Layers

```
┌─────────────────────────────────────────┐
│  Browser Cache (Client-Side)            │ ← Fastest, smallest
├─────────────────────────────────────────┤
│  CDN/Edge Cache                         │ ← Fast, distributed
├─────────────────────────────────────────┤
│  Application Cache (Redis/Memcached)    │ ← Fast, centralized
├─────────────────────────────────────────┤
│  Database Query Cache                   │ ← Medium speed
├─────────────────────────────────────────┤
│  Disk/SSD (Not cache but reference)    │ ← Slower
└─────────────────────────────────────────┘
```

---

## 2. Cache Aside (Lazy Loading)

**Pattern:** Application checks cache first. On miss, loads from database and stores in cache.

### Flow Diagram

```
1. Read:
   App → Check Cache
      Cache Hit? → Return data
      Cache Miss? → Query DB → Store in Cache → Return data

2. Write:
   App → Write to DB → Invalidate Cache
```

### Implementation

```python
import redis
import psycopg2
import json

class CacheAside:
    def __init__(self, redis_client, db_connection):
        self.cache = redis_client
        self.db = db_connection
        self.ttl = 3600  # 1 hour
    
    def get_user(self, user_id):
        """Get user with cache-aside pattern"""
        cache_key = f"user:{user_id}"
        
        # 1. Try to get from cache
        cached_data = self.cache.get(cache_key)
        if cached_data:
            print(f"Cache HIT for user {user_id}")
            return json.loads(cached_data)
        
        print(f"Cache MISS for user {user_id}")
        
        # 2. Query database
        cursor = self.db.cursor()
        cursor.execute("SELECT * FROM users WHERE id = %s", (user_id,))
        user = cursor.fetchone()
        
        if not user:
            return None
        
        user_data = {
            'id': user[0],
            'name': user[1],
            'email': user[2]
        }
        
        # 3. Store in cache
        self.cache.setex(
            cache_key,
            self.ttl,
            json.dumps(user_data)
        )
        
        return user_data
    
    def update_user(self, user_id, name, email):
        """Update user and invalidate cache"""
        # 1. Update database
        cursor = self.db.cursor()
        cursor.execute(
            "UPDATE users SET name = %s, email = %s WHERE id = %s",
            (name, email, user_id)
        )
        self.db.commit()
        
        # 2. Invalidate cache
        cache_key = f"user:{user_id}"
        self.cache.delete(cache_key)
        
        print(f"Updated user {user_id} and invalidated cache")

# Usage
redis_client = redis.Redis(host='localhost', port=6379, decode_responses=True)
db = psycopg2.connect("dbname=myapp user=postgres password=secret")

cache = CacheAside(redis_client, db)

# First call - cache miss, queries DB
user = cache.get_user(123)

# Second call - cache hit, returns from cache
user = cache.get_user(123)

# Update - invalidates cache
cache.update_user(123, "New Name", "new@email.com")

# Next call - cache miss again
user = cache.get_user(123)
```

### Advantages

✅ **Only caches what's needed** - No wasted space
✅ **Simple to implement** - Easy to understand
✅ **Cache failures non-fatal** - Falls back to database

### Disadvantages

❌ **Cache miss penalty** - Three round trips on miss (check cache, query DB, write cache)
❌ **Data staleness** - Cache might have old data until invalidated

---

## 3. Read Through Cache

**Pattern:** Application always queries cache. Cache is responsible for loading from database on miss.

### Flow Diagram

```
App → Cache (abstraction layer)
       ↓
   Cache Hit? → Return data
       ↓
   Cache Miss? → Cache queries DB → Cache stores data → Return data
```

### Implementation

```java
import redis.clients.jedis.Jedis;
import java.sql.*;

public class ReadThroughCache {
    private Jedis cache;
    private Connection db;
    private int ttl = 3600;
    
    public ReadThroughCache(Jedis cache, Connection db) {
        this.cache = cache;
        this.db = db;
    }
    
    /**
     * Get method - cache handles DB loading automatically
     */
    public String getUser(int userId) throws SQLException {
        String cacheKey = "user:" + userId;
        
        // Query cache
        String cachedData = cache.get(cacheKey);
        
        if (cachedData != null) {
            System.out.println("Cache HIT for user " + userId);
            return cachedData;
        }
        
        System.out.println("Cache MISS for user " + userId);
        
        // Cache is responsible for loading from DB
        String userData = loadFromDatabase(userId);
        
        if (userData != null) {
            // Store in cache
            cache.setex(cacheKey, ttl, userData);
        }
        
        return userData;
    }
    
    private String loadFromDatabase(int userId) throws SQLException {
        PreparedStatement stmt = db.prepareStatement(
            "SELECT * FROM users WHERE id = ?"
        );
        stmt.setInt(1, userId);
        ResultSet rs = stmt.executeQuery();
        
        if (rs.next()) {
            return String.format(
                "{\"id\":%d,\"name\":\"%s\",\"email\":\"%s\"}",
                rs.getInt("id"),
                rs.getString("name"),
                rs.getString("email")
            );
        }
        
        return null;
    }
    
    /**
     * Write - updates DB and invalidates cache
     */
    public void updateUser(int userId, String name, String email) 
            throws SQLException {
        // Update database
        PreparedStatement stmt = db.prepareStatement(
            "UPDATE users SET name = ?, email = ? WHERE id = ?"
        );
        stmt.setString(1, name);
        stmt.setString(2, email);
        stmt.setInt(3, userId);
        stmt.executeUpdate();
        
        // Invalidate cache
        String cacheKey = "user:" + userId;
        cache.del(cacheKey);
    }
}

// Usage
Jedis cache = new Jedis("localhost", 6379);
Connection db = DriverManager.getConnection(
    "jdbc:postgresql://localhost/myapp", "postgres", "secret"
);

ReadThroughCache readThrough = new ReadThroughCache(cache, db);

// Application only talks to cache
String user = readThrough.getUser(123);
```

### Advantages

✅ **Simplified application code** - Cache handles loading
✅ **Consistent API** - Always query cache
✅ **Cache manages lifecycle** - Centralized logic

### Disadvantages

❌ **Cache miss penalty** - Still slow on first access
❌ **Complex cache implementation** - Cache needs DB access logic

---

## 4. Write Through Cache

**Pattern:** Data is written to cache and database simultaneously. Cache always has fresh data.

### Flow Diagram

```
App → Write to Cache and DB (synchronously)
       ↓
   Cache.set(key, value)
       ↓
   DB.write(data)
       ↓
   Return success (both completed)
```

### Implementation

```typescript
import { createClient } from 'redis';
import { Pool } from 'pg';

class WriteThroughCache {
    private cache: any;
    private db: Pool;
    private ttl: number = 3600;
    
    constructor(cache: any, db: Pool) {
        this.cache = cache;
        this.db = db;
    }
    
    async getUser(userId: number): Promise<any> {
        const cacheKey = `user:${userId}`;
        
        // Try cache first
        const cached = await this.cache.get(cacheKey);
        if (cached) {
            console.log(`Cache HIT for user ${userId}`);
            return JSON.parse(cached);
        }
        
        console.log(`Cache MISS for user ${userId}`);
        
        // Load from database
        const result = await this.db.query(
            'SELECT * FROM users WHERE id = $1',
            [userId]
        );
        
        if (result.rows.length === 0) {
            return null;
        }
        
        const user = result.rows[0];
        
        // Store in cache
        await this.cache.setEx(
            cacheKey,
            this.ttl,
            JSON.stringify(user)
        );
        
        return user;
    }
    
    async createUser(name: string, email: string): Promise<any> {
        const client = await this.db.connect();
        
        try {
            await client.query('BEGIN');
            
            // 1. Write to database
            const result = await client.query(
                'INSERT INTO users (name, email) VALUES ($1, $2) RETURNING *',
                [name, email]
            );
            
            const user = result.rows[0];
            
            // 2. Write to cache (synchronously)
            const cacheKey = `user:${user.id}`;
            await this.cache.setEx(
                cacheKey,
                this.ttl,
                JSON.stringify(user)
            );
            
            await client.query('COMMIT');
            
            console.log(`User ${user.id} written to both DB and cache`);
            return user;
            
        } catch (error) {
            await client.query('ROLLBACK');
            throw error;
        } finally {
            client.release();
        }
    }
    
    async updateUser(userId: number, name: string, email: string): Promise<void> {
        const client = await this.db.connect();
        
        try {
            await client.query('BEGIN');
            
            // 1. Update database
            await client.query(
                'UPDATE users SET name = $1, email = $2 WHERE id = $3',
                [name, email, userId]
            );
            
            // 2. Update cache
            const cacheKey = `user:${userId}`;
            const user = { id: userId, name, email };
            await this.cache.setEx(
                cacheKey,
                this.ttl,
                JSON.stringify(user)
            );
            
            await client.query('COMMIT');
            
            console.log(`User ${userId} updated in both DB and cache`);
            
        } catch (error) {
            await client.query('ROLLBACK');
            throw error;
        } finally {
            client.release();
        }
    }
}

// Usage
const cache = createClient();
await cache.connect();

const db = new Pool({
    host: 'localhost',
    database: 'myapp',
    user: 'postgres',
    password: 'secret'
});

const writeThrough = new WriteThroughCache(cache, db);

// Write - updates both cache and DB
await writeThrough.createUser('John Doe', 'john@example.com');

// Read - always has fresh data
const user = await writeThrough.getUser(1);
```

### Advantages

✅ **Cache always fresh** - No stale data
✅ **Read performance** - Cache always populated
✅ **Consistency** - Cache and DB in sync

### Disadvantages

❌ **Write latency** - Must write to both cache and DB
❌ **Write penalty** - Slower writes
❌ **Wasted writes** - Caches data that might never be read

---

## 5. Write Behind (Write Back) Cache

**Pattern:** Write to cache immediately, asynchronously write to database later.

### Flow Diagram

```
App → Write to Cache (fast)
       ↓
   Return immediately
       ↓
   Background process writes to DB (async)
```

### Implementation

```python
import redis
import psycopg2
import json
import threading
import time
from queue import Queue

class WriteBehindCache:
    def __init__(self, redis_client, db_connection):
        self.cache = redis_client
        self.db = db_connection
        self.write_queue = Queue()
        self.ttl = 3600
        
        # Start background worker
        self.worker_thread = threading.Thread(
            target=self._background_writer,
            daemon=True
        )
        self.worker_thread.start()
    
    def _background_writer(self):
        """Background thread that writes to database"""
        while True:
            try:
                # Get write operation from queue
                operation = self.write_queue.get(timeout=1)
                
                if operation['type'] == 'create':
                    self._write_to_db(operation['data'])
                elif operation['type'] == 'update':
                    self._update_in_db(operation['data'])
                
                self.write_queue.task_done()
                
            except:
                time.sleep(0.1)
    
    def _write_to_db(self, user_data):
        """Actually write to database"""
        cursor = self.db.cursor()
        cursor.execute(
            "INSERT INTO users (id, name, email) VALUES (%s, %s, %s)",
            (user_data['id'], user_data['name'], user_data['email'])
        )
        self.db.commit()
        print(f"Background: Wrote user {user_data['id']} to database")
    
    def _update_in_db(self, user_data):
        """Actually update in database"""
        cursor = self.db.cursor()
        cursor.execute(
            "UPDATE users SET name = %s, email = %s WHERE id = %s",
            (user_data['name'], user_data['email'], user_data['id'])
        )
        self.db.commit()
        print(f"Background: Updated user {user_data['id']} in database")
    
    def create_user(self, user_id, name, email):
        """Create user - writes to cache immediately"""
        user_data = {
            'id': user_id,
            'name': name,
            'email': email
        }
        
        # 1. Write to cache immediately (fast)
        cache_key = f"user:{user_id}"
        self.cache.setex(
            cache_key,
            self.ttl,
            json.dumps(user_data)
        )
        
        # 2. Queue for background DB write
        self.write_queue.put({
            'type': 'create',
            'data': user_data
        })
        
        print(f"User {user_id} written to cache, queued for DB")
        return user_data
    
    def update_user(self, user_id, name, email):
        """Update user - updates cache immediately"""
        user_data = {
            'id': user_id,
            'name': name,
            'email': email
        }
        
        # 1. Update cache immediately
        cache_key = f"user:{user_id}"
        self.cache.setex(
            cache_key,
            self.ttl,
            json.dumps(user_data)
        )
        
        # 2. Queue for background DB update
        self.write_queue.put({
            'type': 'update',
            'data': user_data
        })
        
        print(f"User {user_id} updated in cache, queued for DB")
    
    def get_user(self, user_id):
        """Get user from cache"""
        cache_key = f"user:{user_id}"
        cached_data = self.cache.get(cache_key)
        
        if cached_data:
            return json.loads(cached_data)
        
        # Fallback to database if not in cache
        cursor = self.db.cursor()
        cursor.execute("SELECT * FROM users WHERE id = %s", (user_id,))
        user = cursor.fetchone()
        
        if user:
            user_data = {
                'id': user[0],
                'name': user[1],
                'email': user[2]
            }
            self.cache.setex(
                cache_key,
                self.ttl,
                json.dumps(user_data)
            )
            return user_data
        
        return None

# Usage
redis_client = redis.Redis(host='localhost', port=6379, decode_responses=True)
db = psycopg2.connect("dbname=myapp user=postgres password=secret")

cache = WriteBehindCache(redis_client, db)

# Write - returns immediately, DB write happens in background
cache.create_user(123, "John Doe", "john@example.com")
print("Write returned immediately!")

# Read - served from cache (very fast)
user = cache.get_user(123)

# Wait for background writes to complete
time.sleep(2)
```

### Advantages

✅ **Very fast writes** - Only writes to cache
✅ **Better write throughput** - Batched DB writes
✅ **Reduced DB load** - Consolidate multiple writes

### Disadvantages

❌ **Data loss risk** - If cache fails before DB write
❌ **Complex implementation** - Need background workers
❌ **Consistency issues** - Cache and DB temporarily out of sync

---

## 6. Cache Invalidation Strategies

> "There are only two hard things in Computer Science: cache invalidation and naming things." - Phil Karlton

### 6.1 Time-To-Live (TTL)

**Strategy:** Cache entries expire after a fixed time.

```python
# Redis with TTL
cache.setex("user:123", 3600, user_data)  # Expires in 1 hour

# After 1 hour, cache automatically removes entry
# Next request will cache miss and reload from DB
```

**Pros:**
✅ Simple to implement
✅ Automatic cleanup
✅ Works for data that changes predictably

**Cons:**
❌ Stale data until expiry
❌ Cache misses after expiry (even if data unchanged)

### 6.2 Explicit Invalidation

**Strategy:** Manually delete cache when data changes.

```javascript
async function updateUser(userId, updates) {
    // 1. Update database
    await db.query(
        'UPDATE users SET name = $1, email = $2 WHERE id = $3',
        [updates.name, updates.email, userId]
    );
    
    // 2. Invalidate cache
    await cache.del(`user:${userId}`);
    
    // 3. Invalidate related caches
    await cache.del(`user:${userId}:posts`);
    await cache.del(`user:${userId}:comments`);
}
```

**Pros:**
✅ No stale data
✅ Cache updated immediately

**Cons:**
❌ Must track all related cache keys
❌ Complex with many dependencies

### 6.3 Tag-Based Invalidation

**Strategy:** Group related cache entries with tags, invalidate all at once.

```python
class TaggedCache:
    def __init__(self, redis_client):
        self.cache = redis_client
    
    def set_with_tags(self, key, value, tags, ttl=3600):
        """Store value with tags"""
        # Store the value
        self.cache.setex(key, ttl, value)
        
        # Add key to each tag set
        for tag in tags:
            tag_key = f"tag:{tag}"
            self.cache.sadd(tag_key, key)
    
    def invalidate_tag(self, tag):
        """Invalidate all keys with this tag"""
        tag_key = f"tag:{tag}"
        
        # Get all keys with this tag
        keys = self.cache.smembers(tag_key)
        
        if keys:
            # Delete all keys
            self.cache.delete(*keys)
            
            # Delete tag set
            self.cache.delete(tag_key)
            
            print(f"Invalidated {len(keys)} keys with tag '{tag}'")

# Usage
tagged_cache = TaggedCache(redis_client)

# Store user data with tags
tagged_cache.set_with_tags(
    "user:123",
    json.dumps(user_data),
    tags=["user", "user:123", "users:active"]
)

tagged_cache.set_with_tags(
    "user:123:posts",
    json.dumps(posts),
    tags=["user:123", "posts"]
)

# Invalidate all data for user 123
tagged_cache.invalidate_tag("user:123")
```

### 6.4 Event-Based Invalidation

**Strategy:** Publish events when data changes, listeners invalidate caches.

```typescript
import { EventEmitter } from 'events';

class EventBasedCache {
    private cache: any;
    private eventBus: EventEmitter;
    
    constructor(cache: any) {
        this.cache = cache;
        this.eventBus = new EventEmitter();
        
        // Subscribe to invalidation events
        this.eventBus.on('user:updated', (userId) => {
            this.invalidateUser(userId);
        });
        
        this.eventBus.on('post:created', (userId) => {
            this.invalidateUserPosts(userId);
        });
    }
    
    private async invalidateUser(userId: number): Promise<void> {
        await this.cache.del(`user:${userId}`);
        console.log(`Invalidated cache for user ${userId}`);
    }
    
    private async invalidateUserPosts(userId: number): Promise<void> {
        await this.cache.del(`user:${userId}:posts`);
        console.log(`Invalidated posts cache for user ${userId}`);
    }
    
    async updateUser(userId: number, updates: any): Promise<void> {
        // Update database
        await db.query('UPDATE users SET ... WHERE id = $1', [userId]);
        
        // Emit event (cache will be invalidated automatically)
        this.eventBus.emit('user:updated', userId);
    }
    
    async createPost(userId: number, content: string): Promise<void> {
        // Create post
        await db.query('INSERT INTO posts ...');
        
        // Emit event
        this.eventBus.emit('post:created', userId);
    }
}
```

---

## 7. CDN (Content Delivery Networks)

**Definition:** Geographically distributed servers that cache content close to users.

### How CDN Works

```
User in US:
User → CDN (San Francisco) → Cache Hit → Return
       [10ms]

User in Europe:
User → CDN (London) → Cache Hit → Return
       [10ms]

Without CDN:
User in Europe → Origin Server (US) → Return
                 [200ms]
```

### CDN Architecture

```
┌─────────────────────────────────────────────────┐
│              Origin Server (Your API)           │
└────────────────────┬────────────────────────────┘
                     │
                     ↓
          ┌──────────────────────┐
          │   CDN Network        │
          └──────────┬───────────┘
                     │
        ┌────────────┼────────────┐
        ↓            ↓             ↓
   ┌────────┐   ┌────────┐   ┌────────┐
   │CDN Edge│   │CDN Edge│   │CDN Edge│
   │US West │   │Europe  │   │Asia    │
   └────────┘   └────────┘   └────────┘
        ↑            ↑             ↑
        │            │             │
   Users US     Users EU      Users Asia
```

### CDN Configuration Example

```nginx
# nginx.conf with CDN headers
server {
    listen 80;
    server_name api.example.com;
    
    location /static/ {
        alias /var/www/static/;
        
        # Cache in CDN for 1 year
        expires 1y;
        add_header Cache-Control "public, immutable";
        
        # Add ETag for validation
        etag on;
    }
    
    location /api/products {
        proxy_pass http://backend;
        
        # Cache in CDN for 1 hour
        add_header Cache-Control "public, max-age=3600";
        
        # Vary by Accept-Language
        add_header Vary "Accept-Language";
    }
    
    location /api/user/profile {
        proxy_pass http://backend;
        
        # Don't cache private data
        add_header Cache-Control "private, no-cache";
    }
}
```

### Cache-Control Headers

```javascript
// Express.js - Setting cache headers
app.get('/static/logo.png', (req, res) => {
    // Cache for 1 year (immutable asset)
    res.set('Cache-Control', 'public, max-age=31536000, immutable');
    res.sendFile('logo.png');
});

app.get('/api/products', (req, res) => {
    // Cache for 5 minutes
    res.set('Cache-Control', 'public, max-age=300');
    res.json(products);
});

app.get('/api/user/profile', (req, res) => {
    // Don't cache (private data)
    res.set('Cache-Control', 'private, no-cache, no-store, must-revalidate');
    res.json(userProfile);
});

app.get('/api/products/:id', (req, res) => {
    // Cache, but revalidate with server
    res.set('Cache-Control', 'public, max-age=3600, must-revalidate');
    res.set('ETag', generateETag(product));
    res.json(product);
});
```

---

## 8. Implementation with Redis and Memcached

### 8.1 Redis Implementation

```python
import redis
import json
import hashlib

class RedisCache:
    def __init__(self, host='localhost', port=6379):
        self.client = redis.Redis(
            host=host,
            port=port,
            decode_responses=True
        )
    
    def get(self, key):
        """Get value from cache"""
        return self.client.get(key)
    
    def set(self, key, value, ttl=3600):
        """Set value in cache with TTL"""
        self.client.setex(key, ttl, value)
    
    def delete(self, key):
        """Delete key from cache"""
        self.client.delete(key)
    
    def get_json(self, key):
        """Get and deserialize JSON"""
        value = self.client.get(key)
        return json.loads(value) if value else None
    
    def set_json(self, key, value, ttl=3600):
        """Serialize and set JSON"""
        self.client.setex(key, ttl, json.dumps(value))
    
    def cache_function(self, ttl=3600):
        """Decorator to cache function results"""
        def decorator(func):
            def wrapper(*args, **kwargs):
                # Generate cache key from function name and arguments
                key_data = f"{func.__name__}:{args}:{kwargs}"
                cache_key = hashlib.md5(key_data.encode()).hexdigest()
                
                # Try cache
                cached = self.get(cache_key)
                if cached:
                    print(f"Cache hit for {func.__name__}")
                    return json.loads(cached)
                
                # Execute function
                print(f"Cache miss for {func.__name__}")
                result = func(*args, **kwargs)
                
                # Store in cache
                self.set(cache_key, json.dumps(result), ttl)
                
                return result
            return wrapper
        return decorator

# Usage
cache = RedisCache()

@cache.cache_function(ttl=600)
def get_user(user_id):
    """Expensive database query"""
    print(f"Querying database for user {user_id}")
    # Simulate DB query
    import time
    time.sleep(1)
    return {'id': user_id, 'name': 'John Doe'}

# First call - cache miss, queries DB (slow)
user = get_user(123)

# Second call - cache hit, returns from cache (fast)
user = get_user(123)
```

### 8.2 Memcached Implementation

```python
import memcache
import json
import hashlib

class MemcachedCache:
    def __init__(self, servers=['127.0.0.1:11211']):
        self.client = memcache.Client(servers)
    
    def get(self, key):
        """Get value from cache"""
        return self.client.get(key)
    
    def set(self, key, value, ttl=3600):
        """Set value in cache"""
        self.client.set(key, value, time=ttl)
    
    def delete(self, key):
        """Delete key"""
        self.client.delete(key)
    
    def get_multi(self, keys):
        """Get multiple keys at once"""
        return self.client.get_multi(keys)
    
    def set_multi(self, mapping, ttl=3600):
        """Set multiple keys at once"""
        self.client.set_multi(mapping, time=ttl)

# Usage
cache = MemcachedCache()

# Single operations
cache.set('user:123', json.dumps({'name': 'John'}))
user = cache.get('user:123')

# Batch operations (more efficient)
users = {
    'user:1': json.dumps({'name': 'Alice'}),
    'user:2': json.dumps({'name': 'Bob'}),
    'user:3': json.dumps({'name': 'Charlie'})
}
cache.set_multi(users)

# Retrieve multiple
results = cache.get_multi(['user:1', 'user:2', 'user:3'])
```

### Redis vs Memcached

| Feature | Redis | Memcached |
|---------|-------|-----------|
| **Data Structures** | Strings, Lists, Sets, Hashes, Sorted Sets | Strings only |
| **Persistence** | Yes (optional) | No |
| **Replication** | Yes | No (need separate tools) |
| **Max Value Size** | 512 MB | 1 MB |
| **Atomic Operations** | Many (INCR, LPUSH, etc.) | Limited |
| **Pub/Sub** | Yes | No |
| **Lua Scripting** | Yes | No |
| **Use Case** | Feature-rich caching, sessions, queues | Simple key-value caching |

---

## 9. Cache Eviction Policies

When cache is full, which entries should be removed?

### 9.1 LRU (Least Recently Used)

**Policy:** Remove the least recently accessed item.

```python
from collections import OrderedDict

class LRUCache:
    def __init__(self, capacity):
        self.cache = OrderedDict()
        self.capacity = capacity
    
    def get(self, key):
        if key not in self.cache:
            return None
        
        # Move to end (most recently used)
        self.cache.move_to_end(key)
        return self.cache[key]
    
    def put(self, key, value):
        if key in self.cache:
            # Update and move to end
            self.cache.move_to_end(key)
        
        self.cache[key] = value
        
        # Remove oldest if over capacity
        if len(self.cache) > self.capacity:
            oldest = next(iter(self.cache))
            del self.cache[oldest]
            print(f"Evicted {oldest} (LRU)")

# Usage
cache = LRUCache(capacity=3)

cache.put('a', 1)  # Cache: a
cache.put('b', 2)  # Cache: a, b
cache.put('c', 3)  # Cache: a, b, c

cache.get('a')     # Cache: b, c, a (a moved to end)

cache.put('d', 4)  # Cache: c, a, d (b evicted - LRU)
```

### 9.2 LFU (Least Frequently Used)

**Policy:** Remove the least frequently accessed item.

```python
from collections import defaultdict

class LFUCache:
    def __init__(self, capacity):
        self.capacity = capacity
        self.cache = {}
        self.freq = defaultdict(int)
        self.min_freq = 0
    
    def get(self, key):
        if key not in self.cache:
            return None
        
        # Increment frequency
        self.freq[key] += 1
        return self.cache[key]
    
    def put(self, key, value):
        if self.capacity <= 0:
            return
        
        if key in self.cache:
            self.cache[key] = value
            self.freq[key] += 1
            return
        
        # Evict if at capacity
        if len(self.cache) >= self.capacity:
            # Find key with minimum frequency
            min_key = min(self.freq, key=self.freq.get)
            del self.cache[min_key]
            del self.freq[min_key]
            print(f"Evicted {min_key} (LFU, freq={self.freq.get(min_key, 0)})")
        
        self.cache[key] = value
        self.freq[key] = 1
```

### 9.3 FIFO (First In First Out)

**Policy:** Remove oldest entry (queue behavior).

### 9.4 TTL (Time To Live)

**Policy:** Remove expired entries.

### 9.5 Random

**Policy:** Remove random entry (simple, surprisingly effective).

### Redis Eviction Policies

```conf
# redis.conf

# Maximum memory
maxmemory 2gb

# Eviction policy
maxmemory-policy allkeys-lru

# Options:
# - noeviction: Return error when memory limit reached
# - allkeys-lru: Remove least recently used keys
# - allkeys-lfu: Remove least frequently used keys
# - volatile-lru: Remove LRU keys with expire set
# - volatile-lfu: Remove LFU keys with expire set
# - allkeys-random: Remove random keys
# - volatile-random: Remove random keys with expire set
# - volatile-ttl: Remove keys with shortest TTL
```

---

## 10. Distributed Caching

### 10.1 Cache Stampede Problem

**Problem:** Cache expires, multiple requests hit database simultaneously.

```
Cache expires
    ↓
100 requests → Cache Miss → All query database simultaneously
                            (Database overload!)
```

**Solution 1: Lock-based (Mutex)**

```python
import redis
import time

class StampedeProtection:
    def __init__(self, redis_client):
        self.cache = redis_client
    
    def get_with_lock(self, key, fetch_func, ttl=3600, lock_ttl=10):
        """Get value with stampede protection"""
        # Try to get from cache
        value = self.cache.get(key)
        if value:
            return value
        
        # Try to acquire lock
        lock_key = f"lock:{key}"
        lock_acquired = self.cache.set(
            lock_key,
            "1",
            ex=lock_ttl,
            nx=True  # Only set if not exists
        )
        
        if lock_acquired:
            # This request rebuilds cache
            try:
                value = fetch_func()
                self.cache.setex(key, ttl, value)
                return value
            finally:
                self.cache.delete(lock_key)
        else:
            # Other request is rebuilding, wait
            for _ in range(50):  # Wait up to 5 seconds
                time.sleep(0.1)
                value = self.cache.get(key)
                if value:
                    return value
            
            # Timeout, fetch anyway
            return fetch_func()

# Usage
def expensive_query():
    print("Querying database...")
    time.sleep(2)  # Simulate slow query
    return "result"

protection = StampedeProtection(redis_client)

# Even if 100 requests come at once, only 1 queries database
result = protection.get_with_lock("user:123", expensive_query)
```

**Solution 2: Probabilistic Early Expiration**

```python
import random
import time

def get_with_early_expiration(key, fetch_func, ttl=3600):
    """Probabilistically refresh cache before expiration"""
    cached_data = cache.get(key)
    
    if cached_data:
        data, cached_at = json.loads(cached_data)
        age = time.time() - cached_at
        
        # Probabilistically refresh early
        # Probability increases as age approaches TTL
        refresh_probability = age / ttl
        
        if random.random() < refresh_probability:
            # Refresh in background
            threading.Thread(target=lambda: refresh_cache(key, fetch_func, ttl)).start()
        
        return data
    
    # Cache miss
    data = fetch_func()
    cache_data = json.dumps([data, time.time()])
    cache.setex(key, ttl, cache_data)
    return data
```

### 10.2 Consistent Hashing

**Problem:** When adding/removing cache servers, minimize cache invalidation.

```python
import hashlib
import bisect

class ConsistentHashing:
    def __init__(self, nodes, virtual_nodes=150):
        self.virtual_nodes = virtual_nodes
        self.ring = {}
        self.sorted_keys = []
        
        for node in nodes:
            self.add_node(node)
    
    def _hash(self, key):
        return int(hashlib.md5(key.encode()).hexdigest(), 16)
    
    def add_node(self, node):
        """Add a node to the ring"""
        for i in range(self.virtual_nodes):
            virtual_key = f"{node}:{i}"
            hash_val = self._hash(virtual_key)
            self.ring[hash_val] = node
            bisect.insort(self.sorted_keys, hash_val)
    
    def remove_node(self, node):
        """Remove a node from the ring"""
        for i in range(self.virtual_nodes):
            virtual_key = f"{node}:{i}"
            hash_val = self._hash(virtual_key)
            del self.ring[hash_val]
            self.sorted_keys.remove(hash_val)
    
    def get_node(self, key):
        """Get the node responsible for this key"""
        if not self.ring:
            return None
        
        hash_val = self._hash(key)
        
        # Find the first node clockwise from the hash
        idx = bisect.bisect_right(self.sorted_keys, hash_val)
        
        if idx == len(self.sorted_keys):
            idx = 0
        
        return self.ring[self.sorted_keys[idx]]

# Usage
nodes = ['cache1:11211', 'cache2:11211', 'cache3:11211']
ch = ConsistentHashing(nodes)

# Keys distributed across nodes
print(ch.get_node('user:123'))  # cache2:11211
print(ch.get_node('user:456'))  # cache1:11211
print(ch.get_node('user:789'))  # cache3:11211

# Add new node - only some keys redistributed
ch.add_node('cache4:11211')
```

---

## 11. Real-World Patterns

### Pattern 1: Multi-Level Caching

```typescript
class MultiLevelCache {
    private l1Cache: Map<string, any>;  // In-memory (fastest)
    private l2Cache: any;                // Redis (fast)
    private database: any;               // Database (slow)
    
    constructor(redisClient: any, db: any) {
        this.l1Cache = new Map();
        this.l2Cache = redisClient;
        this.database = db;
    }
    
    async get(key: string): Promise<any> {
        // Level 1: In-memory cache
        if (this.l1Cache.has(key)) {
            console.log('L1 Cache hit');
            return this.l1Cache.get(key);
        }
        
        // Level 2: Redis cache
        const l2Data = await this.l2Cache.get(key);
        if (l2Data) {
            console.log('L2 Cache hit');
            const data = JSON.parse(l2Data);
            // Populate L1
            this.l1Cache.set(key, data);
            return data;
        }
        
        // Level 3: Database
        console.log('Cache miss - querying database');
        const data = await this.database.query(key);
        
        // Populate both caches
        this.l1Cache.set(key, data);
        await this.l2Cache.setEx(key, 3600, JSON.stringify(data));
        
        return data;
    }
}
```

### Pattern 2: Cache Warming

```python
def warm_cache():
    """Pre-populate cache with frequently accessed data"""
    print("Warming cache...")
    
    # Get top 1000 most viewed products
    cursor = db.cursor()
    cursor.execute("""
        SELECT id, name, price 
        FROM products 
        ORDER BY view_count DESC 
        LIMIT 1000
    """)
    
    products = cursor.fetchall()
    
    # Pre-populate cache
    for product in products:
        cache_key = f"product:{product[0]}"
        cache.setex(
            cache_key,
            3600,
            json.dumps({
                'id': product[0],
                'name': product[1],
                'price': product[2]
            })
        )
    
    print(f"Warmed cache with {len(products)} products")

# Run on application startup
warm_cache()
```

### Pattern 3: Cache-Aside with Circuit Breaker

```typescript
class ResilientCache {
    private cache: any;
    private db: any;
    private failures: number = 0;
    private isOpen: boolean = false;
    private threshold: number = 5;
    private timeout: number = 60000; // 1 minute
    
    async get(key: string): Promise<any> {
        try {
            // Try cache
            const cached = await this.cache.get(key);
            if (cached) {
                this.onSuccess();
                return JSON.parse(cached);
            }
        } catch (error) {
            console.error('Cache error:', error);
            this.onFailure();
        }
        
        // If circuit is open, skip cache
        if (this.isOpen) {
            console.log('Circuit breaker open - bypassing cache');
            return await this.db.query(key);
        }
        
        // Cache miss - query DB
        const data = await this.db.query(key);
        
        // Try to store in cache
        try {
            await this.cache.setEx(key, 3600, JSON.stringify(data));
            this.onSuccess();
        } catch (error) {
            console.error('Failed to store in cache:', error);
            this.onFailure();
        }
        
        return data;
    }
    
    private onSuccess(): void {
        this.failures = 0;
        if (this.isOpen) {
            console.log('Circuit breaker closed');
            this.isOpen = false;
        }
    }
    
    private onFailure(): void {
        this.failures++;
        
        if (this.failures >= this.threshold && !this.isOpen) {
            console.log('Circuit breaker opened');
            this.isOpen = true;
            
            // Auto-close after timeout
            setTimeout(() => {
                console.log('Circuit breaker half-open');
                this.failures = 0;
            }, this.timeout);
        }
    }
}
```

---

## Chapter 11 Summary

### Key Concepts

1. **Caching** = Store frequently accessed data in fast storage
2. **Cache Strategies**:
   - Cache Aside (Lazy Loading)
   - Read Through
   - Write Through
   - Write Behind
3. **Cache Invalidation**: TTL, Explicit, Tag-based, Event-based
4. **CDN** = Distributed edge caching for static content
5. **Eviction Policies**: LRU, LFU, FIFO, TTL

### Strategy Comparison

| Strategy | Write Speed | Read Speed | Consistency | Complexity |
|----------|-------------|------------|-------------|------------|
| **Cache Aside** | Fast | Medium | Good | Low |
| **Read Through** | Fast | Medium | Good | Medium |
| **Write Through** | Slow | Fast | Excellent | Medium |
| **Write Behind** | Very Fast | Fast | Eventually | High |

### Redis vs Memcached

| Use Case | Choose |
|----------|--------|
| Simple key-value | Memcached |
| Rich data structures | Redis |
| Persistence needed | Redis |
| Pub/sub needed | Redis |
| Maximum simplicity | Memcached |

### Interview Tips

**Common Questions:**
1. "Explain cache-aside pattern"
2. "How do you invalidate cache?"
3. "What is cache stampede and how to prevent it?"
4. "Redis vs Memcached?"
5. "Explain CDN"

**How to Answer:**
- Draw flow diagrams
- Explain trade-offs
- Mention real examples (session storage, API responses)
- Discuss invalidation strategies
- Know when NOT to cache

### Best Practices

1. **Cache frequently accessed, rarely changing data**
2. **Set appropriate TTLs** - Balance freshness vs performance
3. **Monitor cache hit ratio** - Target > 80%
4. **Handle cache failures gracefully** - Always have fallback
5. **Use CDN for static assets** - Images, CSS, JS
6. **Invalidate proactively** - Don't rely solely on TTL
7. **Avoid cache stampede** - Use locks or probabilistic refresh

### Next Steps

Chapter 12 will cover **Database Architecture** - normalization, indexing, replication, sharding, and choosing between SQL and NoSQL databases.