# Chapter 26: Distributed Systems Concepts

## Table of Contents
1. Introduction to Distributed Systems
2. CAP Theorem
3. BASE vs ACID
4. Distributed Transactions (2PC, 3PC)
5. Consensus Algorithms (Paxos, Raft)
6. Leader Election
7. Distributed Locks
8. Clock Synchronization
9. Vector Clocks and Causality
10. Split-Brain and Network Partitions
11. Real-World Distributed Systems

---

## 1. Introduction to Distributed Systems

**Definition:** A distributed system is a collection of independent computers that appear to users as a single coherent system.

### Why Distributed Systems?

**1. Scalability**
```
Single server max: ~100K requests/second
Distributed: Millions of requests/second
```

**2. Fault Tolerance**
```
Single server fails = System down
Distributed: Other nodes continue
```

**3. Geographic Distribution**
```
Users worldwide get low latency
Data stored in their region
```

### Challenges

**1. Network is Unreliable**
```
Messages can be:
- Delayed
- Lost
- Duplicated
- Reordered
```

**2. No Global Clock**
```
Different nodes have different times
Hard to determine order of events
```

**3. Partial Failures**
```
Some nodes fail while others work
Hard to distinguish slow from failed
```

**4. Concurrency Issues**
```
Multiple nodes updating same data
Race conditions
Deadlocks
```

---

## 2. CAP Theorem

**Theorem:** In a distributed system experiencing a network partition, you must choose between Consistency and Availability. (Eric Brewer, 2000)

### The Three Guarantees

**Consistency (C):**
All nodes see the same data at the same time.

```
Write to Node A: data = "X"
Read from Node B: data = "X" (immediately)

No stale data
```

**Availability (A):**
Every request receives a response (success or failure).

```
Request → Some node always responds
(even if not most recent data)
```

**Partition Tolerance (P):**
System continues operating despite network failures.

```
Network split:
Node A ←╳→ Node B

System must still work on both sides
```

### You Can Only Pick 2

```
CP (Consistency + Partition Tolerance):
- Sacrifice Availability
- When network partitions, reject writes
- Examples: MongoDB, HBase, Redis Cluster

AP (Availability + Partition Tolerance):
- Sacrifice Consistency
- When network partitions, accept stale reads
- Examples: Cassandra, DynamoDB, Riak

CA (Consistency + Availability):
- Only works with no partitions
- Single-node databases
- Examples: Traditional RDBMS (single instance)
```

### Real Example: CAP in Action

```python
# ============================================
# CP System (MongoDB with Majority Write Concern)
# ============================================
from pymongo import MongoClient, WriteConcern

client = MongoClient('mongodb://localhost:27017/?replicaSet=myReplSet')
db = client.myapp

# CP: Require majority acknowledgment (Consistency)
collection = db.get_collection(
    'users',
    write_concern=WriteConcern(w='majority', j=True)
)

try:
    # Write must succeed on majority of nodes
    result = collection.insert_one({
        'name': 'John',
        'email': 'john@example.com'
    })
    
    print("Write successful (majority confirmed)")
    
except Exception as e:
    # If majority unavailable (partition), write fails
    # Sacrificing Availability for Consistency
    print(f"Write failed: {e}")
    # Return error to client (not available)

# ============================================
# AP System (Cassandra)
# ============================================
from cassandra.cluster import Cluster
from cassandra.query import SimpleStatement, ConsistencyLevel

cluster = Cluster(['localhost'])
session = cluster.connect('myapp')

# AP: ONE consistency (Availability over Consistency)
query = SimpleStatement(
    "INSERT INTO users (id, name, email) VALUES (?, ?, ?)",
    consistency_level=ConsistencyLevel.ONE  # Only need one node
)

try:
    session.execute(query, (uuid4(), 'John', 'john@example.com'))
    print("Write successful (at least one node)")
    # Might return stale data on read
    # But always available
    
except Exception as e:
    print(f"Write failed: {e}")

# Read with eventual consistency
read_query = SimpleStatement(
    "SELECT * FROM users WHERE id = ?",
    consistency_level=ConsistencyLevel.ONE
)

result = session.execute(read_query, (user_id,))
# Might get old data, but always gets a response
```

---

## 3. BASE vs ACID

### ACID (Traditional Databases)

```
A - Atomicity: All or nothing
C - Consistency: Valid state to valid state
I - Isolation: Transactions isolated
D - Durability: Committed data persists

Example:
BEGIN TRANSACTION
  UPDATE accounts SET balance = balance - 100 WHERE id = 1;
  UPDATE accounts SET balance = balance + 100 WHERE id = 2;
COMMIT

Both updates succeed or both fail (atomic)
```

### BASE (Distributed Systems)

```
BA - Basically Available: System appears to work most of the time
S  - Soft state: State may change without input (async replication)
E  - Eventual consistency: System becomes consistent over time

Example:
Node A writes: data = "X"
              ↓ (replication lag)
Node B reads: data = "Y" (old value)
              ↓ (after replication)
Node B reads: data = "X" (eventually consistent)
```

### Eventual Consistency Example

```javascript
// ============================================
// Eventual Consistency in Practice
// ============================================

class EventuallyConsistentStore {
  constructor() {
    this.nodes = [
      new DatabaseNode('node1'),
      new DatabaseNode('node2'),
      new DatabaseNode('node3')
    ];
  }
  
  async write(key, value) {
    // Write to one node immediately (fast)
    await this.nodes[0].write(key, value);
    
    // Replicate to others asynchronously
    this.replicateAsync(key, value);
    
    return { success: true };
  }
  
  async replicateAsync(key, value) {
    // Background replication
    for (let i = 1; i < this.nodes.length; i++) {
      try {
        await this.nodes[i].write(key, value);
      } catch (error) {
        console.error(`Replication to node${i} failed:`, error);
        // Retry later
      }
    }
  }
  
  async read(key) {
    // Read from any node (might be stale)
    const node = this.nodes[Math.floor(Math.random() * this.nodes.length)];
    return await node.read(key);
  }
  
  async readLatest(key) {
    // Read from all nodes, return latest (slower but more consistent)
    const results = await Promise.all(
      this.nodes.map(node => node.read(key))
    );
    
    // Return value with highest timestamp
    return results.reduce((latest, current) => 
      current.timestamp > latest.timestamp ? current : latest
    );
  }
}

// Timeline:
// T0: Write(key='user:1', value='Alice')
//     Node1: 'Alice' ✓
//     Node2: undefined (not yet replicated)
//     Node3: undefined

// T1 (10ms later):
//     Node1: 'Alice' ✓
//     Node2: 'Alice' ✓ (replicated)
//     Node3: undefined

// T2 (20ms later):
//     Node1: 'Alice' ✓
//     Node2: 'Alice' ✓
//     Node3: 'Alice' ✓ (eventually consistent!)
```

---

## 4. Distributed Transactions

### Two-Phase Commit (2PC)

**Definition:** Atomic commit protocol for distributed databases.

**Phases:**
1. **Prepare Phase:** Coordinator asks all participants to prepare
2. **Commit Phase:** If all prepared successfully, commit; otherwise abort

```
Coordinator:
    ↓
1. PREPARE
    ↓
┌───────┬───────┬───────┐
│Node A │Node B │Node C │
│  ✓    │  ✓    │  ✓    │ All vote YES
└───────┴───────┴───────┘
    ↓
2. COMMIT
    ↓
┌───────┬───────┬───────┐
│Node A │Node B │Node C │
│ Done  │ Done  │ Done  │
└───────┴───────┴───────┘

If any node votes NO:
→ Coordinator sends ABORT to all
```

### Implementation

```python
# ============================================
# Two-Phase Commit Implementation
# ============================================

class TwoPhaseCommitCoordinator:
    def __init__(self, participants):
        self.participants = participants
        self.transaction_id = None
    
    async def execute_transaction(self, operations):
        """Execute distributed transaction"""
        self.transaction_id = self.generate_transaction_id()
        
        print(f"Starting 2PC transaction: {self.transaction_id}")
        
        # PHASE 1: PREPARE
        print("Phase 1: PREPARE")
        
        prepare_results = []
        
        for participant in self.participants:
            try:
                # Ask participant to prepare
                result = await participant.prepare(
                    self.transaction_id,
                    operations.get(participant.name)
                )
                prepare_results.append(result)
                
                if result == 'YES':
                    print(f"  {participant.name}: YES")
                else:
                    print(f"  {participant.name}: NO")
                    
            except Exception as e:
                print(f"  {participant.name}: ERROR - {e}")
                prepare_results.append('NO')
        
        # PHASE 2: COMMIT or ABORT
        if all(result == 'YES' for result in prepare_results):
            print("Phase 2: COMMIT (all participants ready)")
            
            # Send COMMIT to all
            for participant in self.participants:
                try:
                    await participant.commit(self.transaction_id)
                    print(f"  {participant.name}: COMMITTED")
                except Exception as e:
                    # Participant failed to commit - problematic!
                    print(f"  {participant.name}: COMMIT FAILED - {e}")
                    # In real system: retry, alert, manual intervention
            
            return {'status': 'COMMITTED'}
            
        else:
            print("Phase 2: ABORT (not all participants ready)")
            
            # Send ABORT to all
            for participant in self.participants:
                try:
                    await participant.abort(self.transaction_id)
                    print(f"  {participant.name}: ABORTED")
                except Exception as e:
                    print(f"  {participant.name}: ABORT FAILED - {e}")
            
            return {'status': 'ABORTED'}

# ============================================
# Participant Node
# ============================================

class Participant:
    def __init__(self, name, db):
        self.name = name
        self.db = db
        self.prepared_transactions = {}
    
    async def prepare(self, transaction_id, operation):
        """Prepare phase - lock resources"""
        try:
            # Execute operation in transaction but don't commit
            await self.db.execute("BEGIN")
            await self.db.execute(operation['query'], operation['params'])
            
            # Lock acquired, ready to commit
            self.prepared_transactions[transaction_id] = True
            
            return 'YES'
            
        except Exception as e:
            # Cannot prepare
            await self.db.execute("ROLLBACK")
            return 'NO'
    
    async def commit(self, transaction_id):
        """Commit phase"""
        if transaction_id in self.prepared_transactions:
            await self.db.execute("COMMIT")
            del self.prepared_transactions[transaction_id]
        else:
            raise Exception("Transaction not prepared")
    
    async def abort(self, transaction_id):
        """Abort phase"""
        if transaction_id in self.prepared_transactions:
            await self.db.execute("ROLLBACK")
            del self.prepared_transactions[transaction_id]

# Usage
coordinator = TwoPhaseCommitCoordinator([
    Participant('BankA', db_connection_a),
    Participant('BankB', db_connection_b)
])

# Transfer money between banks
result = await coordinator.execute_transaction({
    'BankA': {
        'query': 'UPDATE accounts SET balance = balance - $1 WHERE id = $2',
        'params': [100, 'account_123']
    },
    'BankB': {
        'query': 'UPDATE accounts SET balance = balance + $1 WHERE id = $2',
        'params': [100, 'account_456']
    }
})

# Both banks commit or both abort (atomic across banks)
```

### 2PC Problems

```
Problem 1: Blocking
- Participants wait during prepare phase
- If coordinator crashes, participants stuck

Problem 2: Single Point of Failure
- Coordinator crashes → system blocked

Problem 3: Not fault-tolerant
- Can't handle coordinator failure well
```

---

## 5. Consensus Algorithms

### Raft Algorithm

**Purpose:** Elect leader and replicate logs across cluster.

**Key Concepts:**

**1. Leader Election**
```
Initial state: All followers

Follower timeout → Becomes candidate
                 → Requests votes
                 → Gets majority → Becomes leader

Leader sends heartbeats to maintain leadership
```

**2. Log Replication**
```
Client → Leader: Write request
Leader → Appends to log (uncommitted)
       → Replicates to followers
       → Waits for majority acknowledgment
       → Commits log entry
       → Returns to client
       → Tells followers to commit
```

### Raft Implementation (Simplified)

```javascript
// ============================================
// Raft Node Implementation
// ============================================

const NodeState = {
  FOLLOWER: 'FOLLOWER',
  CANDIDATE: 'CANDIDATE',
  LEADER: 'LEADER'
};

class RaftNode {
  constructor(nodeId, peers) {
    this.nodeId = nodeId;
    this.peers = peers;  // Other nodes
    
    // State
    this.state = NodeState.FOLLOWER;
    this.currentTerm = 0;
    this.votedFor = null;
    this.log = [];
    this.commitIndex = 0;
    
    // Timers
    this.electionTimeout = this.randomTimeout(150, 300);
    this.lastHeartbeat = Date.now();
    
    this.startElectionTimer();
  }
  
  randomTimeout(min, max) {
    return Math.floor(Math.random() * (max - min + 1)) + min;
  }
  
  startElectionTimer() {
    setInterval(() => {
      const timeSinceHeartbeat = Date.now() - this.lastHeartbeat;
      
      if (this.state === NodeState.FOLLOWER && 
          timeSinceHeartbeat > this.electionTimeout) {
        // No heartbeat from leader - start election
        this.startElection();
      }
    }, 10);
  }
  
  async startElection() {
    console.log(`[${this.nodeId}] Starting election`);
    
    this.state = NodeState.CANDIDATE;
    this.currentTerm++;
    this.votedFor = this.nodeId;  // Vote for self
    
    let votesReceived = 1;  // Self vote
    const votesNeeded = Math.floor(this.peers.length / 2) + 1;
    
    // Request votes from peers
    for (const peer of this.peers) {
      try {
        const response = await peer.requestVote({
          term: this.currentTerm,
          candidateId: this.nodeId,
          lastLogIndex: this.log.length - 1,
          lastLogTerm: this.log[this.log.length - 1]?.term || 0
        });
        
        if (response.voteGranted) {
          votesReceived++;
        }
        
        if (votesReceived >= votesNeeded) {
          this.becomeLeader();
          return;
        }
        
      } catch (error) {
        console.error(`Failed to get vote from ${peer.id}:`, error);
      }
    }
    
    // Didn't get majority, revert to follower
    this.state = NodeState.FOLLOWER;
  }
  
  becomeLeader() {
    console.log(`[${this.nodeId}] Became LEADER for term ${this.currentTerm}`);
    
    this.state = NodeState.LEADER;
    
    // Send heartbeats to maintain leadership
    this.sendHeartbeats();
  }
  
  sendHeartbeats() {
    if (this.state !== NodeState.LEADER) return;
    
    // Send heartbeat to all followers
    for (const peer of this.peers) {
      peer.appendEntries({
        term: this.currentTerm,
        leaderId: this.nodeId,
        entries: [],  // Empty for heartbeat
        commitIndex: this.commitIndex
      }).catch(() => {
        // Peer unreachable
      });
    }
    
    // Send heartbeats every 50ms
    setTimeout(() => this.sendHeartbeats(), 50);
  }
  
  async appendEntries(request) {
    /**
     * Receive heartbeat or log entries from leader
     */
    if (request.term >= this.currentTerm) {
      // Valid leader
      this.state = NodeState.FOLLOWER;
      this.currentTerm = request.term;
      this.lastHeartbeat = Date.now();
      
      // Append entries if any
      if (request.entries.length > 0) {
        this.log.push(...request.entries);
      }
      
      return { success: true };
    }
    
    return { success: false };
  }
  
  async requestVote(request) {
    /**
     * Handle vote request from candidate
     */
    if (request.term > this.currentTerm) {
      this.currentTerm = request.term;
      this.votedFor = null;
    }
    
    if (this.votedFor === null || this.votedFor === request.candidateId) {
      // Check if candidate's log is up-to-date
      const lastLogIndex = this.log.length - 1;
      const lastLogTerm = this.log[lastLogIndex]?.term || 0;
      
      if (request.lastLogTerm >= lastLogTerm &&
          request.lastLogIndex >= lastLogIndex) {
        // Grant vote
        this.votedFor = request.candidateId;
        this.lastHeartbeat = Date.now();
        
        return { voteGranted: true, term: this.currentTerm };
      }
    }
    
    return { voteGranted: false, term: this.currentTerm };
  }
}

// Create Raft cluster
const node1 = new RaftNode('node1', [node2, node3]);
const node2 = new RaftNode('node2', [node1, node3]);
const node3 = new RaftNode('node3', [node1, node2]);

// One node will become leader
// Others remain followers
// If leader fails, new election starts
```

---

## 6. Leader Election

**Purpose:** Select one node as coordinator/leader.

### Bully Algorithm

```python
# ============================================
# Bully Algorithm for Leader Election
# ============================================

class BullyAlgorithm:
    def __init__(self, node_id, all_nodes):
        self.node_id = node_id
        self.all_nodes = sorted(all_nodes)  # Sorted by ID
        self.leader = max(all_nodes)  # Highest ID is leader
        self.is_leader = (self.node_id == self.leader)
    
    async def start_election(self):
        """Start election when current leader fails"""
        print(f"[Node {self.node_id}] Starting election")
        
        # Send ELECTION message to all higher-ID nodes
        higher_nodes = [n for n in self.all_nodes if n > self.node_id]
        
        if not higher_nodes:
            # No higher nodes - I'm the leader!
            self.become_leader()
            return
        
        responses = []
        
        for node in higher_nodes:
            try:
                response = await self.send_election_message(node)
                responses.append(response)
            except Exception as e:
                # Node didn't respond (assumed dead)
                print(f"[Node {self.node_id}] Node {node} didn't respond")
        
        if any(responses):
            # Higher node responded - wait for them to become leader
            print(f"[Node {self.node_id}] Higher node responded, waiting...")
            
            # Wait for COORDINATOR message
            await self.wait_for_coordinator()
            
        else:
            # No higher node responded - I'm the leader!
            self.become_leader()
    
    def become_leader(self):
        """Declare self as leader"""
        self.is_leader = True
        self.leader = self.node_id
        
        print(f"[Node {self.node_id}] I am the leader!")
        
        # Send COORDINATOR message to all lower nodes
        lower_nodes = [n for n in self.all_nodes if n < self.node_id]
        
        for node in lower_nodes:
            self.send_coordinator_message(node)
    
    async def receive_election_message(self, from_node):
        """Handle election message from lower node"""
        if from_node < self.node_id:
            # I'm higher - respond and start my own election
            await self.send_ok_message(from_node)
            await self.start_election()
    
    async def receive_coordinator_message(self, leader_id):
        """Handle coordinator announcement"""
        self.leader = leader_id
        self.is_leader = False
        print(f"[Node {self.node_id}] Acknowledged leader: Node {leader_id}")

# Example:
# Nodes: [1, 2, 3, 4, 5]
# Leader: Node 5

# Node 5 crashes
# Node 4 detects failure
# Node 4 sends ELECTION to Node 5 (no response)
# Node 4 becomes leader
# Node 4 sends COORDINATOR to Nodes 1, 2, 3
```

---

## 7. Distributed Locks

**Purpose:** Coordinate access to shared resources across multiple nodes.

### Redis-Based Distributed Lock (Redlock)

```python
# ============================================
# Distributed Lock with Redis
# ============================================
import redis
import time
import uuid

class DistributedLock:
    def __init__(self, redis_client, lock_name, ttl=10):
        self.redis = redis_client
        self.lock_name = f"lock:{lock_name}"
        self.ttl = ttl  # seconds
        self.lock_id = str(uuid.uuid4())  # Unique ID for this lock holder
    
    def acquire(self, timeout=10):
        """Try to acquire lock"""
        end_time = time.time() + timeout
        
        while time.time() < end_time:
            # Try to set lock with NX (only if not exists)
            acquired = self.redis.set(
                self.lock_name,
                self.lock_id,
                ex=self.ttl,  # Expire after TTL
                nx=True       # Only set if not exists
            )
            
            if acquired:
                print(f"Lock acquired: {self.lock_name}")
                return True
            
            # Wait before retrying
            time.sleep(0.1)
        
        print(f"Failed to acquire lock: {self.lock_name}")
        return False
    
    def release(self):
        """Release lock (only if we own it)"""
        # Lua script for atomic check-and-delete
        lua_script = """
        if redis.call("get", KEYS[1]) == ARGV[1] then
            return redis.call("del", KEYS[1])
        else
            return 0
        end
        """
        
        result = self.redis.eval(
            lua_script,
            1,  # Number of keys
            self.lock_name,
            self.lock_id
        )
        
        if result:
            print(f"Lock released: {self.lock_name}")
        else:
            print(f"Lock not owned or already released: {self.lock_name}")
    
    def __enter__(self):
        """Context manager support"""
        if not self.acquire():
            raise Exception(f"Could not acquire lock: {self.lock_name}")
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.release()

# ============================================
# Usage
# ============================================

redis_client = redis.Redis(host='localhost', port=6379)

# Example: Only one worker should process this job
def process_job(job_id):
    lock = DistributedLock(redis_client, f"job:{job_id}")
    
    if lock.acquire(timeout=5):
        try:
            print(f"Processing job {job_id}")
            # Do work...
            time.sleep(2)
            print(f"Job {job_id} completed")
        finally:
            lock.release()
    else:
        print(f"Job {job_id} already being processed by another worker")

# Or use context manager
def process_job_with_context(job_id):
    try:
        with DistributedLock(redis_client, f"job:{job_id}"):
            print(f"Processing job {job_id}")
            # Do work...
    except Exception as e:
        print(f"Could not acquire lock: {e}")

# Run multiple workers
# Only one will process each job (distributed mutex)
```

---

## 8. Clock Synchronization

### The Problem

```
Node A: 10:00:00.000
Node B: 10:00:00.100 (100ms ahead)
Node C: 09:59:59.900 (100ms behind)

Event timeline unclear:
- Node A writes at 10:00:00.050
- Node C reads at 10:00:00.000 (its clock)
- Did read happen before or after write?
```

### Physical Clocks (NTP)

```bash
# Network Time Protocol synchronization
# Keeps clocks within ~1ms of each other

# Install NTP
apt-get install ntp

# Configure NTP
# /etc/ntp.conf
server 0.pool.ntp.org
server 1.pool.ntp.org
server 2.pool.ntp.org

# Start NTP service
systemctl start ntp

# Check synchronization
ntpq -p
```

### Logical Clocks (Lamport Timestamps)

**Purpose:** Order events without synchronized physical clocks.

```python
# ============================================
# Lamport Timestamps
# ============================================

class LamportClock:
    def __init__(self):
        self.time = 0
    
    def tick(self):
        """Increment clock on internal event"""
        self.time += 1
        return self.time
    
    def send_event(self):
        """Timestamp for sending message"""
        self.time += 1
        return self.time
    
    def receive_event(self, message_timestamp):
        """Update clock on receiving message"""
        self.time = max(self.time, message_timestamp) + 1
        return self.time

# ============================================
# Example
# ============================================

# Node A
clock_a = LamportClock()

# Node A: Internal event
t1 = clock_a.tick()  # t=1
print(f"Node A event at Lamport time {t1}")

# Node A: Send message to Node B
t2 = clock_a.send_event()  # t=2
print(f"Node A sends message at Lamport time {t2}")

# Node B
clock_b = LamportClock()

# Node B: Receive message
t3 = clock_b.receive_event(t2)  # t=max(0,2)+1=3
print(f"Node B receives message at Lamport time {t3}")

# Node B: Internal event
t4 = clock_b.tick()  # t=4
print(f"Node B event at Lamport time {t4}")

# Guarantees: If event A happened before event B,
#             then timestamp(A) < timestamp(B)
```

---

## 9. Vector Clocks

**Purpose:** Track causality in distributed systems.

```python
# ============================================
# Vector Clocks Implementation
# ============================================

class VectorClock:
    def __init__(self, node_id, num_nodes):
        self.node_id = node_id
        self.clock = [0] * num_nodes  # Vector of timestamps
    
    def tick(self):
        """Increment own clock"""
        self.clock[self.node_id] += 1
        return self.clock.copy()
    
    def send_event(self):
        """Get timestamp for sending message"""
        return self.tick()
    
    def receive_event(self, message_clock):
        """Update vector on receiving message"""
        # Take maximum of each component
        for i in range(len(self.clock)):
            self.clock[i] = max(self.clock[i], message_clock[i])
        
        # Increment own clock
        self.clock[self.node_id] += 1
        
        return self.clock.copy()
    
    def happens_before(self, other_clock):
        """Check if this clock happened before other"""
        # Happened before if: all components <= and at least one <
        all_less_or_equal = all(
            self.clock[i] <= other_clock[i] 
            for i in range(len(self.clock))
        )
        
        at_least_one_less = any(
            self.clock[i] < other_clock[i] 
            for i in range(len(self.clock))
        )
        
        return all_less_or_equal and at_least_one_less
    
    def concurrent(self, other_clock):
        """Check if clocks are concurrent (can't determine order)"""
        return not self.happens_before(other_clock) and \
               not VectorClock.static_happens_before(other_clock, self.clock)
    
    @staticmethod
    def static_happens_before(clock1, clock2):
        all_less_or_equal = all(
            clock1[i] <= clock2[i] 
            for i in range(len(clock1))
        )
        at_least_one_less = any(
            clock1[i] < clock2[i] 
            for i in range(len(clock1))
        )
        return all_less_or_equal and at_least_one_less

# ============================================
# Example with 3 nodes
# ============================================

# Node 0
vc0 = VectorClock(node_id=0, num_nodes=3)

# Node 1
vc1 = VectorClock(node_id=1, num_nodes=3)

# Node 2
vc2 = VectorClock(node_id=2, num_nodes=3)

# Node 0: Event
t0_1 = vc0.tick()  # [1, 0, 0]

# Node 0: Send to Node 1
msg_clock = vc0.send_event()  # [2, 0, 0]

# Node 1: Receive from Node 0
vc1.receive_event(msg_clock)  # [2, 1, 0]

# Node 1: Event
vc1.tick()  # [2, 2, 0]

# Node 1: Send to Node 2
msg_clock2 = vc1.send_event()  # [2, 3, 0]

# Node 2: Receive from Node 1
vc2.receive_event(msg_clock2)  # [2, 3, 1]

# Can now determine causality:
# [1,0,0] happened before [2,3,1] ✓
# [2,3,1] and [3,0,0] are concurrent (can't determine order)
```

---

## 10. Split-Brain and Network Partitions

### Split-Brain Problem

```
Cluster of 5 nodes:
[Node1, Node2, Node3, Node4, Node5]
     Leader: Node1

Network partition:
[Node1, Node2] ←╳→ [Node3, Node4, Node5]

Both sides elect leaders:
Partition A: Node1 is leader
Partition B: Node3 is leader

Two leaders! (split-brain)

Both accept writes → conflict when reconnected
```

### Solution: Quorum

```python
# ============================================
# Quorum-Based Consensus
# ============================================

class QuorumSystem:
    def __init__(self, nodes):
        self.nodes = nodes
        self.num_nodes = len(nodes)
        self.quorum_size = (self.num_nodes // 2) + 1  # Majority
    
    async def write(self, key, value):
        """Write succeeds if majority acknowledges"""
        success_count = 0
        
        for node in self.nodes:
            try:
                await node.write(key, value)
                success_count += 1
                
                if success_count >= self.quorum_size:
                    print(f"Write successful (quorum: {success_count}/{self.num_nodes})")
                    return True
                    
            except Exception as e:
                print(f"Node {node.id} failed: {e}")
        
        print(f"Write failed (only {success_count}/{self.quorum_size} required)")
        return False
    
    async def read(self, key):
        """Read from quorum, return latest"""
        values = []
        
        for node in self.nodes:
            try:
                value = await node.read(key)
                values.append(value)
                
                if len(values) >= self.quorum_size:
                    break
                    
            except Exception as e:
                print(f"Node {node.id} failed: {e}")
        
        if len(values) < self.quorum_size:
            raise Exception("Could not achieve read quorum")
        
        # Return most recent value (highest version)
        return max(values, key=lambda v: v['version'])

# With 5 nodes, quorum = 3
# Partition A (2 nodes): Can't achieve quorum (no writes)
# Partition B (3 nodes): Can achieve quorum (writes allowed)

# Only one partition can accept writes → no split-brain!
```

---

## 11. Real-World Distributed Systems

### Cassandra Architecture

```
Cassandra Cluster (Ring topology):

Node1 ←→ Node2 ←→ Node3
  ↑                ↓
Node6 ←→ Node5 ←→ Node4

Characteristics:
- No leader (peer-to-peer)
- Data replicated to N nodes
- Consistent hashing for distribution
- Tunable consistency

Write with consistency level QUORUM:
Write to 3 nodes (replication factor)
Must get 2 acknowledgments (quorum)
```

```python
# ============================================
# Cassandra Client
# ============================================
from cassandra.cluster import Cluster
from cassandra.query import SimpleStatement, ConsistencyLevel

cluster = Cluster(['node1', 'node2', 'node3'])
session = cluster.connect('myapp')

# Write with QUORUM consistency
query = SimpleStatement(
    "INSERT INTO users (id, name, email) VALUES (?, ?, ?)",
    consistency_level=ConsistencyLevel.QUORUM
)

session.execute(query, (user_id, 'John', 'john@example.com'))
# Waits for majority of replicas to acknowledge

# Read with QUORUM consistency
query = SimpleStatement(
    "SELECT * FROM users WHERE id = ?",
    consistency_level=ConsistencyLevel.QUORUM
)

result = session.execute(query, (user_id,))
# Reads from majority, returns latest value

# Trade-off:
# QUORUM = Stronger consistency, slower
# ONE = Weak consistency, faster
```

---

## Chapter 26 Summary

### Key Concepts

1. **CAP Theorem** - Consistency, Availability, Partition Tolerance (pick 2)
2. **BASE** - Basically Available, Soft state, Eventual consistency
3. **2PC** - Two-phase commit for distributed transactions
4. **Consensus** - Raft, Paxos for agreement
5. **Leader Election** - Choose coordinator
6. **Distributed Locks** - Coordinate access
7. **Clocks** - Lamport, Vector clocks for ordering
8. **Quorum** - Majority voting
9. **Split-Brain** - Two leaders problem

### Consistency Levels

| Level | Description | Use Case |
|-------|-------------|----------|
| **Strong** | All reads see latest write | Banking, inventory |
| **Eventual** | Reads eventually consistent | Social media, caching |
| **Causal** | Related events ordered | Messaging, comments |
| **Read-your-writes** | See own writes | User profiles |

### Distributed Systems Trade-offs

```
Consistency ←→ Availability
- More consistent = Less available during partitions
- More available = More eventual consistency

Consistency ←→ Performance
- Stronger consistency = Slower (wait for quorum)
- Weaker consistency = Faster (local reads)

Simplicity ←→ Scalability
- Simple (single node) = Limited scale
- Complex (distributed) = Unlimited scale
```

### Interview Tips

**Common Questions:**
1. "Explain CAP theorem with examples"
2. "How does Raft consensus work?"
3. "What is eventual consistency?"
4. "How do you handle distributed transactions?"
5. "Explain split-brain problem"

**How to Answer:**
- Draw diagrams (CAP triangle, Raft states)
- Give concrete examples (Cassandra for AP, MongoDB for CP)
- Explain trade-offs clearly
- Mention real systems (Raft in etcd, Paxos in Chubby)
- Discuss when you'd choose each

### Best Practices

1. **Design for partition tolerance** - Network failures will happen
2. **Choose consistency model** - Based on business requirements
3. **Use proven consensus** - Don't implement Paxos yourself!
4. **Quorum for critical operations** - Ensure correctness
5. **Eventual consistency for scale** - When acceptable
6. **Monitor partition detection** - Alert on split-brain

### Next Steps

Chapter 27 will cover **Data Consistency Models** - deep dive into consistency guarantees, linearizability, serializability, and how to reason about consistency in distributed systems.