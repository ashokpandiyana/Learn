# Chapter 29: Data Modeling

## Table of Contents
1. Introduction to Data Modeling
2. Entity-Relationship (ER) Diagrams
3. Star Schema (Data Warehouse)
4. Snowflake Schema
5. Data Lakes vs Data Warehouses
6. Dimensional Modeling
7. NoSQL Data Modeling
8. Document Model Design
9. Time-Series Data Modeling
10. Graph Data Modeling
11. Data Modeling Best Practices

---

## 1. Introduction to Data Modeling

**Definition:** Data modeling is the process of creating a visual representation of data structures and relationships to support business requirements.

### Why Data Modeling Matters

```
Good Data Model:
- Fast queries
- Easy to understand
- Maintains data integrity
- Scales well

Bad Data Model:
- Slow queries (missing indexes, bad joins)
- Confusing structure
- Data anomalies
- Doesn't scale
```

### Types of Data Models

**1. Conceptual Model**
- High-level business view
- Entities and relationships
- No technical details

**2. Logical Model**
- Detailed structure
- Attributes and keys
- Platform-independent

**3. Physical Model**
- Implementation details
- Data types, indexes
- Platform-specific

---

## 2. Entity-Relationship (ER) Diagrams

**Definition:** Visual representation of entities (things) and their relationships.

### ER Diagram Components

```
Entity: Rectangle
  ┌──────────┐
  │ Customer │
  └──────────┘

Relationship: Diamond
       ┌────────┐
  ─────│ Places │─────
       └────────┘

Attribute: Oval
       ┌────────┐
       │  Name  │
       └────────┘
```

### Complete ER Example: E-Commerce

```sql
-- ============================================
-- E-Commerce Database Schema
-- ============================================

-- Entities

CREATE TABLE customers (
    customer_id SERIAL PRIMARY KEY,
    email VARCHAR(255) UNIQUE NOT NULL,
    first_name VARCHAR(100) NOT NULL,
    last_name VARCHAR(100) NOT NULL,
    phone VARCHAR(20),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE products (
    product_id SERIAL PRIMARY KEY,
    sku VARCHAR(50) UNIQUE NOT NULL,
    name VARCHAR(200) NOT NULL,
    description TEXT,
    price DECIMAL(10, 2) NOT NULL,
    category_id INTEGER REFERENCES categories(category_id),
    stock_quantity INTEGER DEFAULT 0,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE categories (
    category_id SERIAL PRIMARY KEY,
    name VARCHAR(100) NOT NULL,
    parent_category_id INTEGER REFERENCES categories(category_id),
    description TEXT
);

CREATE TABLE orders (
    order_id SERIAL PRIMARY KEY,
    customer_id INTEGER NOT NULL REFERENCES customers(customer_id),
    order_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    total_amount DECIMAL(10, 2) NOT NULL,
    status VARCHAR(20) DEFAULT 'pending',
    shipping_address_id INTEGER REFERENCES addresses(address_id),
    billing_address_id INTEGER REFERENCES addresses(address_id)
);

CREATE TABLE order_items (
    order_item_id SERIAL PRIMARY KEY,
    order_id INTEGER NOT NULL REFERENCES orders(order_id) ON DELETE CASCADE,
    product_id INTEGER NOT NULL REFERENCES products(product_id),
    quantity INTEGER NOT NULL CHECK (quantity > 0),
    price_at_purchase DECIMAL(10, 2) NOT NULL,
    UNIQUE(order_id, product_id)
);

CREATE TABLE addresses (
    address_id SERIAL PRIMARY KEY,
    customer_id INTEGER NOT NULL REFERENCES customers(customer_id),
    address_type VARCHAR(20), -- 'shipping' or 'billing'
    street_address VARCHAR(255) NOT NULL,
    city VARCHAR(100) NOT NULL,
    state VARCHAR(50),
    postal_code VARCHAR(20),
    country VARCHAR(100) NOT NULL
);

CREATE TABLE reviews (
    review_id SERIAL PRIMARY KEY,
    product_id INTEGER NOT NULL REFERENCES products(product_id),
    customer_id INTEGER NOT NULL REFERENCES customers(customer_id),
    rating INTEGER CHECK (rating BETWEEN 1 AND 5),
    comment TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    UNIQUE(product_id, customer_id)  -- One review per customer per product
);

-- Indexes for common queries
CREATE INDEX idx_orders_customer ON orders(customer_id);
CREATE INDEX idx_orders_date ON orders(order_date DESC);
CREATE INDEX idx_products_category ON products(category_id);
CREATE INDEX idx_reviews_product ON reviews(product_id);
CREATE INDEX idx_addresses_customer ON addresses(customer_id);
```

### Relationships

**One-to-Many (1:N)**
```sql
-- One customer can have many orders
customers (1) ←→ (N) orders

-- Implementation: Foreign key in "many" side
CREATE TABLE orders (
    order_id SERIAL PRIMARY KEY,
    customer_id INTEGER REFERENCES customers(customer_id)  -- Foreign key
);
```

**Many-to-Many (M:N)**
```sql
-- Many products can be in many orders
-- Need junction table

products (M) ←→ order_items ←→ (N) orders

-- Implementation: Junction table with two foreign keys
CREATE TABLE order_items (
    order_item_id SERIAL PRIMARY KEY,
    order_id INTEGER REFERENCES orders(order_id),
    product_id INTEGER REFERENCES products(product_id),
    quantity INTEGER,
    PRIMARY KEY (order_id, product_id)  -- Composite key
);
```

**One-to-One (1:1)**
```sql
-- One user has one profile
users (1) ←→ (1) user_profiles

-- Implementation: Foreign key with UNIQUE constraint
CREATE TABLE user_profiles (
    profile_id SERIAL PRIMARY KEY,
    user_id INTEGER UNIQUE REFERENCES users(user_id),  -- UNIQUE makes it 1:1
    bio TEXT,
    avatar_url VARCHAR(255)
);
```

---

## 3. Star Schema (Data Warehouse)

**Definition:** Dimensional model with central fact table connected to dimension tables.

### Structure

```
          Dimension Tables
              (Who, What, When, Where)
                    ↓
        ┌──────────────────────┐
        │                      │
    ┌───▼────┐          ┌──────▼───┐
    │  Time  │          │ Product  │
    │  Dim   │          │   Dim    │
    └───┬────┘          └──────┬───┘
        │                      │
        │   ┌──────────┐      │
        └──→│  Sales   │←─────┘
            │  Fact    │
        ┌──→│  Table   │←─────┐
        │   └──────────┘      │
        │                      │
    ┌───┴────┐          ┌──────┴───┐
    │Customer│          │ Location │
    │  Dim   │          │   Dim    │
    └────────┘          └──────────┘
    
Central fact table stores measurements
Dimension tables store descriptive attributes
```

### Implementation

```sql
-- ============================================
-- Star Schema Implementation
-- ============================================

-- Fact Table (Measures/Metrics)
CREATE TABLE fact_sales (
    sale_id BIGSERIAL PRIMARY KEY,
    
    -- Foreign keys to dimensions
    time_key INTEGER NOT NULL REFERENCES dim_time(time_key),
    product_key INTEGER NOT NULL REFERENCES dim_product(product_key),
    customer_key INTEGER NOT NULL REFERENCES dim_customer(customer_key),
    location_key INTEGER NOT NULL REFERENCES dim_location(location_key),
    
    -- Measures (numerical facts)
    quantity INTEGER NOT NULL,
    unit_price DECIMAL(10, 2) NOT NULL,
    total_amount DECIMAL(10, 2) NOT NULL,
    discount_amount DECIMAL(10, 2) DEFAULT 0,
    tax_amount DECIMAL(10, 2) DEFAULT 0,
    profit DECIMAL(10, 2),
    
    -- Metadata
    transaction_id VARCHAR(50),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Dimension Table: Time
CREATE TABLE dim_time (
    time_key SERIAL PRIMARY KEY,
    
    -- Date components
    full_date DATE NOT NULL UNIQUE,
    year INTEGER NOT NULL,
    quarter INTEGER NOT NULL,
    month INTEGER NOT NULL,
    month_name VARCHAR(20) NOT NULL,
    week INTEGER NOT NULL,
    day_of_month INTEGER NOT NULL,
    day_of_week INTEGER NOT NULL,
    day_name VARCHAR(20) NOT NULL,
    
    -- Business attributes
    is_weekend BOOLEAN,
    is_holiday BOOLEAN,
    holiday_name VARCHAR(100),
    fiscal_year INTEGER,
    fiscal_quarter INTEGER
);

-- Dimension Table: Product
CREATE TABLE dim_product (
    product_key SERIAL PRIMARY KEY,
    
    -- Natural key
    product_id VARCHAR(50) UNIQUE NOT NULL,
    
    -- Attributes
    product_name VARCHAR(200) NOT NULL,
    brand VARCHAR(100),
    category VARCHAR(100),
    subcategory VARCHAR(100),
    unit_cost DECIMAL(10, 2),
    supplier VARCHAR(100),
    
    -- Slowly Changing Dimension (SCD Type 2)
    effective_date DATE NOT NULL,
    expiration_date DATE,
    is_current BOOLEAN DEFAULT TRUE
);

-- Dimension Table: Customer
CREATE TABLE dim_customer (
    customer_key SERIAL PRIMARY KEY,
    customer_id VARCHAR(50) UNIQUE NOT NULL,
    
    first_name VARCHAR(100),
    last_name VARCHAR(100),
    email VARCHAR(255),
    phone VARCHAR(20),
    
    -- Demographics
    age_group VARCHAR(20),
    gender VARCHAR(10),
    income_bracket VARCHAR(50),
    
    -- Segmentation
    customer_segment VARCHAR(50),  -- 'Premium', 'Regular', 'New'
    loyalty_tier VARCHAR(20),
    
    -- SCD Type 2
    effective_date DATE NOT NULL,
    expiration_date DATE,
    is_current BOOLEAN DEFAULT TRUE
);

-- Dimension Table: Location
CREATE TABLE dim_location (
    location_key SERIAL PRIMARY KEY,
    
    store_id VARCHAR(50),
    store_name VARCHAR(200),
    
    street_address VARCHAR(255),
    city VARCHAR(100),
    state VARCHAR(50),
    postal_code VARCHAR(20),
    country VARCHAR(100),
    
    region VARCHAR(100),
    territory VARCHAR(100),
    
    store_size VARCHAR(20),  -- 'Small', 'Medium', 'Large'
    store_type VARCHAR(50)   -- 'Retail', 'Warehouse', 'Online'
);

-- ============================================
-- Analytical Queries (Fast with Star Schema)
-- ============================================

-- Total sales by month
SELECT 
    t.year,
    t.month_name,
    SUM(f.total_amount) as total_sales,
    COUNT(*) as transaction_count
FROM fact_sales f
JOIN dim_time t ON f.time_key = t.time_key
WHERE t.year = 2024
GROUP BY t.year, t.month, t.month_name
ORDER BY t.month;

-- Top products by category
SELECT 
    p.category,
    p.product_name,
    SUM(f.quantity) as units_sold,
    SUM(f.total_amount) as revenue
FROM fact_sales f
JOIN dim_product p ON f.product_key = p.product_key
WHERE p.is_current = TRUE
GROUP BY p.category, p.product_name
ORDER BY revenue DESC
LIMIT 10;

-- Customer segment analysis
SELECT 
    c.customer_segment,
    COUNT(DISTINCT f.customer_key) as customer_count,
    SUM(f.total_amount) as total_revenue,
    AVG(f.total_amount) as avg_order_value
FROM fact_sales f
JOIN dim_customer c ON f.customer_key = c.customer_key
WHERE c.is_current = TRUE
  AND f.time_key >= (SELECT time_key FROM dim_time WHERE full_date = '2024-01-01')
GROUP BY c.customer_segment
ORDER BY total_revenue DESC;

-- Sales by region and quarter
SELECT 
    l.region,
    t.year,
    t.quarter,
    SUM(f.total_amount) as total_sales
FROM fact_sales f
JOIN dim_location l ON f.location_key = l.location_key
JOIN dim_time t ON f.time_key = t.time_key
GROUP BY l.region, t.year, t.quarter
ORDER BY l.region, t.year, t.quarter;
```

### Star Schema Benefits

✅ **Simple queries** - Easy joins
✅ **Fast aggregations** - Optimized for analytics
✅ **Denormalized** - No complex joins
✅ **BI tool friendly** - Tools understand star schema

---

## 4. Snowflake Schema

**Definition:** Normalized version of star schema where dimension tables are normalized into multiple related tables.

### Snowflake vs Star

```
Star Schema (Denormalized):
Fact Table → Product Dimension (all product attributes in one table)

Snowflake Schema (Normalized):
Fact Table → Product Dimension → Category → Subcategory
                              → Brand
                              → Supplier
```

### Implementation

```sql
-- ============================================
-- Snowflake Schema Implementation
-- ============================================

-- Fact Table (same as star schema)
CREATE TABLE fact_sales (
    sale_id BIGSERIAL PRIMARY KEY,
    time_key INTEGER REFERENCES dim_time(time_key),
    product_key INTEGER REFERENCES dim_product(product_key),
    customer_key INTEGER REFERENCES dim_customer(customer_key),
    location_key INTEGER REFERENCES dim_location(location_key),
    quantity INTEGER,
    total_amount DECIMAL(10, 2)
);

-- Normalized Product Dimension
CREATE TABLE dim_product (
    product_key SERIAL PRIMARY KEY,
    product_id VARCHAR(50) UNIQUE,
    product_name VARCHAR(200),
    brand_key INTEGER REFERENCES dim_brand(brand_key),  -- Normalized
    category_key INTEGER REFERENCES dim_category(category_key),  -- Normalized
    supplier_key INTEGER REFERENCES dim_supplier(supplier_key)  -- Normalized
);

-- Brand dimension (normalized out)
CREATE TABLE dim_brand (
    brand_key SERIAL PRIMARY KEY,
    brand_name VARCHAR(100),
    brand_country VARCHAR(100)
);

-- Category dimension (normalized out)
CREATE TABLE dim_category (
    category_key SERIAL PRIMARY KEY,
    category_name VARCHAR(100),
    parent_category_key INTEGER REFERENCES dim_category(category_key)  -- Hierarchy
);

-- Supplier dimension (normalized out)
CREATE TABLE dim_supplier (
    supplier_key SERIAL PRIMARY KEY,
    supplier_name VARCHAR(100),
    supplier_country VARCHAR(100),
    supplier_rating DECIMAL(3, 2)
);

-- Query (requires more joins)
SELECT 
    cat.category_name,
    brand.brand_name,
    SUM(f.total_amount) as revenue
FROM fact_sales f
JOIN dim_product p ON f.product_key = p.product_key
JOIN dim_brand brand ON p.brand_key = brand.brand_key
JOIN dim_category cat ON p.category_key = cat.category_key
GROUP BY cat.category_name, brand.brand_name;
```

### Star vs Snowflake Trade-offs

| Aspect | Star Schema | Snowflake Schema |
|--------|-------------|------------------|
| **Query Speed** | Faster (fewer joins) | Slower (more joins) |
| **Storage** | More (denormalized) | Less (normalized) |
| **Maintenance** | Harder (update redundant data) | Easier (update once) |
| **Complexity** | Simpler | More complex |
| **Use Case** | Most data warehouses | Storage-constrained |

---

## 5. Data Lakes vs Data Warehouses

### Data Lake

**Definition:** Store raw data in native format until needed.

**Characteristics:**
- **Schema-on-read** - Structure applied when reading
- **All data types** - Structured, semi-structured, unstructured
- **Flexible** - Can store anything
- **Cheaper** - Use object storage (S3)

```python
# ============================================
# Data Lake Architecture (AWS)
# ============================================

import boto3
import json

class DataLake:
    def __init__(self, bucket_name):
        self.s3 = boto3.client('s3')
        self.bucket = bucket_name
    
    def ingest_raw_data(self, source, data, metadata=None):
        """Store raw data in data lake"""
        
        # Organize by source and date
        from datetime import datetime
        date_path = datetime.now().strftime('%Y/%m/%d')
        
        key = f"raw/{source}/{date_path}/{metadata.get('filename', 'data.json')}"
        
        # Store raw data (no transformation)
        self.s3.put_object(
            Bucket=self.bucket,
            Key=key,
            Body=json.dumps(data) if isinstance(data, dict) else data,
            Metadata=metadata or {}
        )
        
        print(f"Ingested to data lake: s3://{self.bucket}/{key}")
        
        return key
    
    def catalog_data(self, key, schema):
        """Catalog data for querying (AWS Glue)"""
        glue = boto3.client('glue')
        
        # Create table in Glue Data Catalog
        glue.create_table(
            DatabaseName='datalake',
            TableInput={
                'Name': 'raw_events',
                'StorageDescriptor': {
                    'Columns': schema,
                    'Location': f's3://{self.bucket}/raw/events/',
                    'InputFormat': 'org.apache.hadoop.mapred.TextInputFormat',
                    'OutputFormat': 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat',
                    'SerdeInfo': {
                        'SerializationLibrary': 'org.openx.data.jsonserde.JsonSerDe'
                    }
                },
                'PartitionKeys': [
                    {'Name': 'year', 'Type': 'int'},
                    {'Name': 'month', 'Type': 'int'},
                    {'Name': 'day', 'Type': 'int'}
                ]
            }
        )
    
    def query_with_athena(self, sql_query):
        """Query data lake using Athena (serverless SQL)"""
        athena = boto3.client('athena')
        
        response = athena.start_query_execution(
            QueryString=sql_query,
            QueryExecutionContext={'Database': 'datalake'},
            ResultConfiguration={
                'OutputLocation': f's3://{self.bucket}/query-results/'
            }
        )
        
        query_execution_id = response['QueryExecutionId']
        
        # Wait for query to complete
        while True:
            status = athena.get_query_execution(
                QueryExecutionId=query_execution_id
            )
            
            state = status['QueryExecution']['Status']['State']
            
            if state == 'SUCCEEDED':
                break
            elif state in ['FAILED', 'CANCELLED']:
                raise Exception(f"Query failed: {state}")
            
            time.sleep(1)
        
        # Get results
        results = athena.get_query_results(
            QueryExecutionId=query_execution_id
        )
        
        return results

# Usage
lake = DataLake('my-data-lake')

# Ingest raw clickstream data
lake.ingest_raw_data(
    source='clickstream',
    data={'user_id': '123', 'page': '/products', 'timestamp': '2024-01-15T10:30:00Z'},
    metadata={'filename': 'clicks_20240115.json'}
)

# Query raw data with SQL
results = lake.query_with_athena("""
    SELECT 
        page,
        COUNT(*) as page_views
    FROM raw_events
    WHERE year = 2024 AND month = 1
    GROUP BY page
    ORDER BY page_views DESC
    LIMIT 10
""")
```

### Data Warehouse

**Definition:** Structured, processed data optimized for analytics.

**Characteristics:**
- **Schema-on-write** - Structure defined before loading
- **Structured data** - Tables, rows, columns
- **Optimized** - For complex queries
- **Expensive** - Compute and storage

```python
# ============================================
# Data Warehouse (Amazon Redshift)
# ============================================

import psycopg2

class DataWarehouse:
    def __init__(self):
        self.conn = psycopg2.connect(
            host='mywarehouse.redshift.amazonaws.com',
            port=5439,
            database='analytics',
            user='admin',
            password='secret'
        )
    
    def load_from_s3(self, table_name, s3_path):
        """Load data from S3 into Redshift"""
        cursor = self.conn.cursor()
        
        # COPY command (optimized bulk load)
        copy_command = f"""
            COPY {table_name}
            FROM '{s3_path}'
            IAM_ROLE 'arn:aws:iam::123456789:role/RedshiftRole'
            FORMAT AS JSON 'auto'
            TIMEFORMAT 'auto'
            REGION 'us-east-1'
        """
        
        cursor.execute(copy_command)
        self.conn.commit()
        
        print(f"Loaded data into {table_name}")
    
    def create_aggregate_table(self):
        """Create aggregated table for fast queries"""
        cursor = self.conn.cursor()
        
        # Pre-aggregate for common queries
        cursor.execute("""
            CREATE TABLE sales_by_month AS
            SELECT 
                DATE_TRUNC('month', order_date) as month,
                product_category,
                SUM(total_amount) as total_sales,
                COUNT(*) as order_count,
                AVG(total_amount) as avg_order_value
            FROM fact_sales
            JOIN dim_time ON fact_sales.time_key = dim_time.time_key
            JOIN dim_product ON fact_sales.product_key = dim_product.product_key
            GROUP BY month, product_category
        """)
        
        self.conn.commit()
        
        # Now queries on sales_by_month are instant!

# Usage
warehouse = DataWarehouse()

# Load processed data from S3
warehouse.load_from_s3(
    'fact_sales',
    's3://my-bucket/processed/sales/2024/'
)

# Create aggregate tables for fast queries
warehouse.create_aggregate_table()
```

### Data Lake vs Data Warehouse

| Aspect | Data Lake | Data Warehouse |
|--------|-----------|----------------|
| **Data Type** | Raw, all formats | Processed, structured |
| **Schema** | Schema-on-read | Schema-on-write |
| **Users** | Data scientists, ML | Business analysts |
| **Purpose** | Exploration, ML | Reporting, BI |
| **Cost** | Low (storage) | High (compute + storage) |
| **Query Speed** | Slower | Faster |
| **Flexibility** | High | Low |
| **Example** | S3 + Athena | Redshift, Snowflake |

---

## 6. Dimensional Modeling

### Slowly Changing Dimensions (SCD)

**Problem:** Dimension attributes change over time.

**Type 1: Overwrite (No history)**
```sql
-- Customer moves to new address
-- Old: 123 Main St
-- New: 456 Oak Ave

UPDATE dim_customer
SET address = '456 Oak Ave'
WHERE customer_id = 'C123';

-- Previous address lost
-- Can't analyze historical data with old address
```

**Type 2: Add New Row (Keep history)**
```sql
-- Keep both versions with date ranges

-- Current record
customer_key | customer_id | address      | effective_date | expiration_date | is_current
1           | C123        | 123 Main St  | 2020-01-01     | 2024-01-14      | FALSE
2           | C123        | 456 Oak Ave  | 2024-01-15     | 9999-12-31      | TRUE

-- Query for current address
SELECT * FROM dim_customer 
WHERE customer_id = 'C123' AND is_current = TRUE;

-- Query for historical data
SELECT * FROM dim_customer 
WHERE customer_id = 'C123' 
  AND '2023-06-01' BETWEEN effective_date AND expiration_date;
```

**Type 3: Add New Column**
```sql
-- Track only current and previous value

customer_key | customer_id | current_address | previous_address
1           | C123        | 456 Oak Ave     | 123 Main St
```

### Implementation

```python
# ============================================
# SCD Type 2 Implementation
# ============================================

class SlowlyChangingDimension:
    def __init__(self, db):
        self.db = db
    
    def update_customer_scd2(self, customer_id, new_attributes):
        """Update customer dimension (SCD Type 2)"""
        
        # 1. Get current record
        current = self.db.query("""
            SELECT * FROM dim_customer
            WHERE customer_id = %s AND is_current = TRUE
        """, [customer_id])
        
        if not current:
            # New customer - insert
            self.db.execute("""
                INSERT INTO dim_customer 
                (customer_id, name, address, effective_date, is_current)
                VALUES (%s, %s, %s, CURRENT_DATE, TRUE)
            """, [customer_id, new_attributes['name'], new_attributes['address']])
            return
        
        # 2. Check if attributes changed
        changed = (
            current['name'] != new_attributes['name'] or
            current['address'] != new_attributes['address']
        )
        
        if not changed:
            # No change - do nothing
            return
        
        # 3. Expire current record
        self.db.execute("""
            UPDATE dim_customer
            SET is_current = FALSE,
                expiration_date = CURRENT_DATE - INTERVAL '1 day'
            WHERE customer_key = %s
        """, [current['customer_key']])
        
        # 4. Insert new record
        self.db.execute("""
            INSERT INTO dim_customer
            (customer_id, name, address, effective_date, is_current)
            VALUES (%s, %s, %s, CURRENT_DATE, TRUE)
        """, [customer_id, new_attributes['name'], new_attributes['address']])
        
        print(f"Customer {customer_id} updated (SCD Type 2)")

# Usage
scd = SlowlyChangingDimension(db)

# Customer moves
scd.update_customer_scd2('C123', {
    'name': 'John Doe',
    'address': '456 Oak Ave'  # Changed from 123 Main St
})

# Now have historical record of both addresses
```

---

## 7. NoSQL Data Modeling

### Document Model (MongoDB)

**Principle:** Model data how you query it.

```javascript
// ============================================
// MongoDB Document Model
// ============================================

// E-Commerce Order Document (Embedded)
{
  _id: ObjectId("507f1f77bcf86cd799439011"),
  orderNumber: "ORD-2024-001",
  orderDate: ISODate("2024-01-15T10:30:00Z"),
  status: "shipped",
  
  // Embedded customer info (denormalized)
  customer: {
    customerId: "C123",
    name: "John Doe",
    email: "john@example.com"
  },
  
  // Embedded items array
  items: [
    {
      productId: "P456",
      productName: "Laptop",
      sku: "LAP-001",
      quantity: 1,
      price: 999.99
    },
    {
      productId: "P789",
      productName: "Mouse",
      sku: "MOU-001",
      quantity: 2,
      price: 25.00
    }
  ],
  
  // Embedded shipping address
  shippingAddress: {
    street: "123 Main St",
    city: "New York",
    state: "NY",
    zip: "10001"
  },
  
  // Calculated fields
  totalAmount: 1049.99,
  itemCount: 3,
  
  // Embedded payment info
  payment: {
    method: "credit_card",
    last4: "4242",
    transactionId: "txn_123456"
  },
  
  // Embedded tracking
  tracking: {
    carrier: "UPS",
    trackingNumber: "1Z999AA10123456784",
    shippedDate: ISODate("2024-01-16T08:00:00Z"),
    estimatedDelivery: ISODate("2024-01-20T17:00:00Z")
  }
}

// Single query gets everything!
db.orders.findOne({ _id: ObjectId("507f1f77bcf86cd799439011") });

// No joins needed
// Fast reads
// All order data in one document
```

### When to Embed vs Reference

```javascript
// ============================================
// Embed (Denormalize)
// ============================================

// Use when:
// - Data is always accessed together
// - One-to-few relationship
// - Data doesn't change often

// Example: Order items (always shown with order)
{
  orderId: "ORD-001",
  items: [  // Embedded
    { productId: "P1", quantity: 2 },
    { productId: "P2", quantity: 1 }
  ]
}

// ============================================
// Reference (Normalize)
// ============================================

// Use when:
// - Data is large
// - One-to-many relationship
// - Data changes frequently
// - Data accessed independently

// Example: User's orders (hundreds of orders)
{
  userId: "U123",
  name: "John Doe",
  // Don't embed all orders!
}

// Separate collection
{
  orderId: "ORD-001",
  userId: "U123",  // Reference
  total: 100
}

// Query:
db.orders.find({ userId: "U123" });
```

---

## 8. Document Model Design

### Access Pattern-Driven Design

```javascript
// ============================================
// Design Based on Access Patterns
// ============================================

// Access Pattern 1: Display user profile with recent posts
// Solution: Embed recent posts, reference older ones

{
  _id: ObjectId("..."),
  userId: "U123",
  name: "John Doe",
  email: "john@example.com",
  
  // Embed recent posts (last 10)
  recentPosts: [
    {
      postId: "P1",
      title: "My latest thoughts",
      content: "...",
      createdAt: ISODate("2024-01-15"),
      likes: 42,
      comments: 5
    },
    // ... 9 more recent posts
  ],
  
  // Stats
  totalPosts: 250,
  followerCount: 1500
}

// Older posts in separate collection
db.posts.find({ userId: "U123" }).sort({ createdAt: -1 });

// Access Pattern 2: Display post with all comments
// Solution: Embed comments if few, paginate if many

{
  _id: ObjectId("..."),
  postId: "P1",
  userId: "U123",
  title: "My post",
  content: "...",
  
  // If < 100 comments: Embed
  comments: [
    {
      commentId: "C1",
      userId: "U456",
      userName: "Jane",
      text: "Great post!",
      createdAt: ISODate("2024-01-15")
    }
  ],
  
  commentCount: 15
}

// If > 100 comments: Separate collection with pagination
```

### Polymorphic Pattern

```javascript
// ============================================
// Polymorphic Documents (Different structures in same collection)
// ============================================

// Products collection with different types

// Electronic Product
{
  _id: ObjectId("..."),
  type: "electronic",
  name: "Laptop",
  price: 999,
  
  // Electronic-specific fields
  brand: "Dell",
  processor: "Intel i7",
  ram: "16GB",
  storage: "512GB SSD",
  warranty: "3 years"
}

// Clothing Product
{
  _id: ObjectId("..."),
  type: "clothing",
  name: "T-Shirt",
  price: 25,
  
  // Clothing-specific fields
  brand: "Nike",
  sizes: ["S", "M", "L", "XL"],
  colors: ["Red", "Blue", "Black"],
  material: "Cotton"
}

// Book Product
{
  _id: ObjectId("..."),
  type: "book",
  name: "Design Patterns",
  price: 45,
  
  // Book-specific fields
  author: "Gang of Four",
  isbn: "978-0201633610",
  pages: 395,
  publisher: "Addison-Wesley"
}

// Query by type
db.products.find({ type: "electronic", "ram": "16GB" });

// Flexible schema allows different product types in same collection
```

---

## 9. Time-Series Data Modeling

**Definition:** Data points indexed by time, optimized for time-based queries.

### Time-Series Schema

```sql
-- ============================================
-- Time-Series Table (PostgreSQL + TimescaleDB)
-- ============================================

-- Create hypertable
CREATE TABLE sensor_data (
    time TIMESTAMPTZ NOT NULL,
    sensor_id INTEGER NOT NULL,
    temperature DOUBLE PRECISION,
    humidity DOUBLE PRECISION,
    pressure DOUBLE PRECISION,
    location VARCHAR(100)
);

-- Convert to hypertable (TimescaleDB)
SELECT create_hypertable('sensor_data', 'time');

-- Automatically partitions by time
-- Each partition (chunk) contains data for a time range

-- Insert data
INSERT INTO sensor_data (time, sensor_id, temperature, humidity, pressure, location)
VALUES 
    (NOW(), 1, 22.5, 65.0, 1013.25, 'Room A'),
    (NOW(), 2, 23.1, 68.5, 1012.80, 'Room B');

-- Time-series queries (optimized)

-- Average temperature per hour (last 24 hours)
SELECT 
    time_bucket('1 hour', time) AS hour,
    sensor_id,
    AVG(temperature) as avg_temp,
    MAX(temperature) as max_temp,
    MIN(temperature) as min_temp
FROM sensor_data
WHERE time > NOW() - INTERVAL '24 hours'
GROUP BY hour, sensor_id
ORDER BY hour DESC;

-- Continuous aggregate (materialized view that updates automatically)
CREATE MATERIALIZED VIEW sensor_hourly
WITH (timescaledb.continuous) AS
SELECT 
    time_bucket('1 hour', time) AS hour,
    sensor_id,
    AVG(temperature) as avg_temp,
    AVG(humidity) as avg_humidity
FROM sensor_data
GROUP BY hour, sensor_id;

-- Refresh policy (auto-update)
SELECT add_continuous_aggregate_policy('sensor_hourly',
    start_offset => INTERVAL '3 hours',
    end_offset => INTERVAL '1 hour',
    schedule_interval => INTERVAL '1 hour'
);

-- Query is instant (pre-aggregated)
SELECT * FROM sensor_hourly
WHERE hour > NOW() - INTERVAL '7 days'
ORDER BY hour DESC;
```

### InfluxDB (Purpose-Built Time-Series DB)

```python
# ============================================
# InfluxDB Time-Series
# ============================================

from influxdb_client import InfluxDBClient, Point
from influxdb_client.client.write_api import SYNCHRONOUS

client = InfluxDBClient(
    url="http://localhost:8086",
    token="my-token",
    org="my-org"
)

write_api = client.write_api(write_options=SYNCHRONOUS)
query_api = client.query_api()

# Write data point
point = Point("sensor_data") \
    .tag("sensor_id", "sensor_1") \
    .tag("location", "room_a") \
    .field("temperature", 22.5) \
    .field("humidity", 65.0) \
    .time(datetime.utcnow())

write_api.write(bucket="sensors", record=point)

# Query (Flux language)
query = '''
from(bucket: "sensors")
  |> range(start: -24h)
  |> filter(fn: (r) => r["_measurement"] == "sensor_data")
  |> filter(fn: (r) => r["sensor_id"] == "sensor_1")
  |> aggregateWindow(every: 1h, fn: mean)
'''

result = query_api.query(query)

# Process results
for table in result:
    for record in table.records:
        print(f"{record.get_time()}: {record.get_value()}")
```

---

## 10. Graph Data Modeling

**Definition:** Model data as nodes and relationships (edges).

### Graph Model Example

```cypher
-- ============================================
-- Neo4j Graph Database
-- ============================================

-- Create nodes
CREATE (alice:Person {name: 'Alice', age: 30, city: 'New York'})
CREATE (bob:Person {name: 'Bob', age: 25, city: 'San Francisco'})
CREATE (charlie:Person {name: 'Charlie', age: 35, city: 'New York'})

CREATE (python:Skill {name: 'Python', category: 'Programming'})
CREATE (java:Skill {name: 'Java', category: 'Programming'})
CREATE (ml:Skill {name: 'Machine Learning', category: 'Data Science'})

CREATE (companyA:Company {name: 'Tech Corp', industry: 'Technology'})

-- Create relationships
MATCH (alice:Person {name: 'Alice'}), (bob:Person {name: 'Bob'})
CREATE (alice)-[:FRIENDS_WITH {since: date('2020-01-15')}]->(bob)

MATCH (alice:Person {name: 'Alice'}), (python:Skill {name: 'Python'})
CREATE (alice)-[:KNOWS {level: 'expert', years: 5}]->(python)

MATCH (alice:Person {name: 'Alice'}), (companyA:Company {name: 'Tech Corp'})
CREATE (alice)-[:WORKS_AT {position: 'Senior Engineer', since: date('2019-06-01')}]->(companyA)

-- Queries that shine with graph databases

-- 1. Find friends of friends (2 hops)
MATCH (me:Person {name: 'Alice'})-[:FRIENDS_WITH]->()-[:FRIENDS_WITH]->(fof)
WHERE NOT (me)-[:FRIENDS_WITH]->(fof) AND me <> fof
RETURN fof.name

-- 2. Recommend skills (what friends know that I don't)
MATCH (me:Person {name: 'Alice'})-[:FRIENDS_WITH]->(friend)-[:KNOWS]->(skill)
WHERE NOT (me)-[:KNOWS]->(skill)
RETURN skill.name, COUNT(friend) as friend_count
ORDER BY friend_count DESC

-- 3. Find shortest path between two people
MATCH path = shortestPath(
  (alice:Person {name: 'Alice'})-[:FRIENDS_WITH*]-(charlie:Person {name: 'Charlie'})
)
RETURN path

-- 4. Find colleagues (people who work at same company)
MATCH (me:Person {name: 'Alice'})-[:WORKS_AT]->(company)<-[:WORKS_AT]-(colleague)
WHERE me <> colleague
RETURN colleague.name, colleague.position

-- 5. Recommend connections (friends of friends who share skills)
MATCH (me:Person {name: 'Alice'})-[:FRIENDS_WITH]->()-[:FRIENDS_WITH]->(potential)
WHERE NOT (me)-[:FRIENDS_WITH]->(potential) AND me <> potential
MATCH (me)-[:KNOWS]->(skill)<-[:KNOWS]-(potential)
RETURN potential.name, COUNT(skill) as shared_skills
ORDER BY shared_skills DESC
```

### Graph Model in Application

```python
# ============================================
# Python with Neo4j
# ============================================

from neo4j import GraphDatabase

class SocialNetworkGraph:
    def __init__(self, uri, user, password):
        self.driver = GraphDatabase.driver(uri, auth=(user, password))
    
    def close(self):
        self.driver.close()
    
    def add_friendship(self, person1, person2):
        """Create bidirectional friendship"""
        with self.driver.session() as session:
            result = session.run("""
                MATCH (a:Person {name: $person1})
                MATCH (b:Person {name: $person2})
                CREATE (a)-[:FRIENDS_WITH {since: date()}]->(b)
                CREATE (b)-[:FRIENDS_WITH {since: date()}]->(a)
                RETURN a, b
            """, person1=person1, person2=person2)
            
            return result.single()
    
    def recommend_friends(self, person_name, limit=10):
        """Friend recommendations (friends of friends)"""
        with self.driver.session() as session:
            result = session.run("""
                MATCH (me:Person {name: $name})-[:FRIENDS_WITH]->()-[:FRIENDS_WITH]->(recommended)
                WHERE NOT (me)-[:FRIENDS_WITH]->(recommended) AND me <> recommended
                WITH recommended, COUNT(*) as mutual_friends
                MATCH (recommended)-[:KNOWS]->(skill)<-[:KNOWS]-(me)
                WITH recommended, mutual_friends, COUNT(skill) as shared_skills
                RETURN 
                    recommended.name as name,
                    mutual_friends,
                    shared_skills,
                    recommended.city as city
                ORDER BY mutual_friends DESC, shared_skills DESC
                LIMIT $limit
            """, name=person_name, limit=limit)
            
            return [dict(record) for record in result]
    
    def get_network_stats(self, person_name):
        """Get network statistics"""
        with self.driver.session() as session:
            result = session.run("""
                MATCH (me:Person {name: $name})
                OPTIONAL MATCH (me)-[:FRIENDS_WITH]->(friend)
                OPTIONAL MATCH (me)-[:KNOWS]->(skill)
                OPTIONAL MATCH (me)-[:WORKS_AT]->(company)
                RETURN 
                    COUNT(DISTINCT friend) as friend_count,
                    COUNT(DISTINCT skill) as skill_count,
                    company.name as employer
            """, name=person_name)
            
            return dict(result.single())

# Usage
graph = SocialNetworkGraph("bolt://localhost:7687", "neo4j", "password")

# Add friendship
graph.add_friendship("Alice", "Bob")

# Get recommendations
recommendations = graph.recommend_friends("Alice", limit=5)
for rec in recommendations:
    print(f"{rec['name']}: {rec['mutual_friends']} mutual friends, {rec['shared_skills']} shared skills")

# Network stats
stats = graph.get_network_stats("Alice")
print(f"Friends: {stats['friend_count']}, Skills: {stats['skill_count']}")

graph.close()
```

---

## 11. Data Modeling Best Practices

### Normalization vs Denormalization

```python
# ============================================
# When to Normalize
# ============================================

# Normalized (Good for writes, data integrity)
CREATE TABLE users (
    user_id SERIAL PRIMARY KEY,
    name VARCHAR(100),
    email VARCHAR(255)
);

CREATE TABLE posts (
    post_id SERIAL PRIMARY KEY,
    user_id INTEGER REFERENCES users(user_id),
    title VARCHAR(200),
    content TEXT
);

# Query requires JOIN
SELECT posts.title, users.name
FROM posts
JOIN users ON posts.user_id = users.id;

# Pros:
# - No duplicate data (user name stored once)
# - Easy to update user name (single place)
# - Data integrity (foreign key constraint)

# Cons:
# - JOIN required for every query (slower)
# - More complex queries

# ============================================
# When to Denormalize
# ============================================

# Denormalized (Good for reads, performance)
CREATE TABLE posts (
    post_id SERIAL PRIMARY KEY,
    user_id INTEGER,
    user_name VARCHAR(100),  -- Denormalized
    user_email VARCHAR(255), -- Denormalized
    title VARCHAR(200),
    content TEXT
);

# Query is simple (no JOIN)
SELECT post_id, title, user_name FROM posts;

# Pros:
# - Fast queries (no JOINs)
# - Simple queries
# - Good for read-heavy workloads

# Cons:
# - Duplicate data (user name in every post)
# - Update complexity (must update all posts if user changes name)
# - More storage
```

### Design Checklist

```python
DATA_MODELING_CHECKLIST = {
    'Requirements': [
        'Identified access patterns',
        'Query frequency known',
        'Read vs write ratio understood',
        'Data volume estimated',
        'Growth projection made'
    ],
    
    'Design': [
        'Entities identified',
        'Relationships defined',
        'Keys chosen (natural vs surrogate)',
        'Normalization level decided',
        'Indexes planned'
    ],
    
    'Optimization': [
        'Common queries optimized',
        'Appropriate data types chosen',
        'Partitioning strategy (if needed)',
        'Archiving strategy defined',
        'Caching opportunities identified'
    ],
    
    'Validation': [
        'Sample queries tested',
        'Performance benchmarked',
        'Data integrity constraints added',
        'Migration path planned'
    ]
}
```

---

## Chapter 29 Summary

### Key Concepts

1. **ER Diagrams** - Visual data modeling
2. **Star Schema** - Central fact + dimension tables
3. **Snowflake Schema** - Normalized dimensions
4. **Data Lake** - Raw data, schema-on-read
5. **Data Warehouse** - Structured, schema-on-write
6. **SCD** - Slowly changing dimensions (Type 1, 2, 3)
7. **Document Model** - Embed vs reference decisions
8. **Time-Series** - Optimized for temporal queries
9. **Graph Model** - Nodes and relationships

### Schema Design Patterns

| Data Type | Best Model | Example |
|-----------|------------|---------|
| **Transactional** | Normalized relational | PostgreSQL |
| **Analytical** | Star/Snowflake schema | Redshift |
| **Documents** | Document store | MongoDB |
| **Time-series** | Time-series DB | InfluxDB, TimescaleDB |
| **Relationships** | Graph database | Neo4j |
| **Cache** | Key-value | Redis |

### Design Decision Framework

```
Questions to ask:

1. Access patterns?
   - Read-heavy → Denormalize
   - Write-heavy → Normalize

2. Query complexity?
   - Simple lookups → NoSQL
   - Complex joins → SQL

3. Data relationships?
   - Hierarchical → Document
   - Network → Graph
   - Tabular → Relational

4. Consistency needs?
   - Strong → SQL
   - Eventual → NoSQL

5. Scale requirements?
   - < 1TB → Single database
   - > 1TB → Distributed/Sharding
```

### Interview Tips

**Common Questions:**
1. "Design a database schema for [domain]"
2. "Star schema vs Snowflake schema?"
3. "When to use NoSQL vs SQL?"
4. "Explain slowly changing dimensions"

**How to Answer:**
- Draw ER diagrams
- Show table structures with keys
- Explain normalization decisions
- Discuss indexing strategy
- Mention trade-offs

### Next Steps

Chapter 30 will cover **Data Pipeline Architecture** - ETL vs ELT, batch vs stream processing, and building robust data pipelines.