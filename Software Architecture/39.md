# Chapter 39: Classic System Design Problems - In-Depth Solutions

## Introduction

This chapter covers the most frequently asked system design interview questions with complete solutions. Each problem includes requirements analysis, capacity estimation, API design, architecture diagrams, and detailed component design.

---

## 1. Design URL Shortener (like bit.ly)

### Requirements

**Functional:**
- Create short URL from long URL
- Redirect short URL to original URL
- Custom aliases (optional)
- Expiration time (optional)
- Analytics (click tracking)

**Non-Functional:**
- 100M URLs generated per month
- 10:1 read to write ratio (1B redirects/month)
- Low latency (<100ms for redirect)
- High availability (99.9%+)
- Store URLs for 5 years

### Capacity Estimation

```python
# Storage calculation
URLS_PER_MONTH = 100_000_000
MONTHS_TO_STORE = 60  # 5 years
AVERAGE_URL_LENGTH = 500  # bytes

total_urls = URLS_PER_MONTH * MONTHS_TO_STORE
storage_needed = total_urls * AVERAGE_URL_LENGTH

print(f"Total URLs: {total_urls:,}")  # 6 billion
print(f"Storage: {storage_needed / (1024**3):.2f} GB")  # 2,793 GB = 2.8 TB

# QPS calculation
SECONDS_PER_MONTH = 30 * 24 * 60 * 60

write_qps = URLS_PER_MONTH / SECONDS_PER_MONTH
read_qps = write_qps * 10

print(f"Write QPS: {write_qps:.0f}")  # 39 writes/sec
print(f"Read QPS: {read_qps:.0f}")    # 386 reads/sec
```

### API Design

```javascript
// 1. Create short URL
POST /api/v1/shorten
{
  "long_url": "https://www.example.com/very/long/path",
  "custom_alias": "mylink",  // optional
  "expiration_date": "2025-12-31"  // optional
}

Response 201:
{
  "short_url": "https://short.ly/abc123",
  "long_url": "https://www.example.com/very/long/path",
  "created_at": "2024-01-15T10:30:00Z"
}

// 2. Redirect
GET /{short_code}
Response 302:
Location: https://www.example.com/very/long/path

// 3. Get analytics
GET /api/v1/analytics/{short_code}
Response 200:
{
  "total_clicks": 1523,
  "unique_clicks": 892,
  "clicks_by_country": {...}
}
```

### Hash Generation Strategy

```python
"""
Key challenge: Generate unique short codes

Requirements:
- Short codes should be 6-7 characters
- Characters: [a-z, A-Z, 0-9] = 62 characters
- 62^6 = 56 billion possible combinations
"""

import hashlib
import base64

class URLShortener:
    def __init__(self):
        self.counter = 0  # For base62 encoding method
        
    # METHOD 1: MD5 Hash + Truncate
    def hash_method(self, long_url):
        """
        Pros: Simple, deterministic
        Cons: Collision possible, need to handle duplicates
        """
        # Generate MD5 hash
        hash_obj = hashlib.md5(long_url.encode())
        hash_hex = hash_obj.hexdigest()
        
        # Take first 6 characters
        short_code = hash_hex[:6]
        
        # If collision, append counter and rehash
        return short_code
    
    # METHOD 2: Base62 Encoding (RECOMMENDED)
    def base62_encode(self, num):
        """
        Pros: No collisions, shorter codes
        Cons: Need distributed counter
        """
        characters = "0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ"
        base = len(characters)
        
        if num == 0:
            return characters[0]
        
        result = []
        while num:
            remainder = num % base
            result.append(characters[remainder])
            num //= base
        
        return ''.join(reversed(result))
    
    def generate_short_code(self):
        """
        Use auto-increment ID and convert to base62
        """
        self.counter += 1
        return self.base62_encode(self.counter)

# Example usage
shortener = URLShortener()

# Generate short codes
for i in range(1, 1000000):
    code = shortener.base62_encode(i)
    if i in [1, 62, 3844, 238328]:
        print(f"{i} -> {code}")

# Output:
# 1 -> 1
# 62 -> 10
# 3844 -> 100
# 238328 -> 1000
```

### High-Level Architecture

```
┌────────────────────────────────────────────────────────────┐
│                        CLIENTS                             │
└────────────────────────────────────────────────────────────┘
                           │
                           ▼
┌────────────────────────────────────────────────────────────┐
│                    LOAD BALANCER                           │
└────────────────────────────────────────────────────────────┘
                           │
           ┌───────────────┼────────────────┐
           ▼               ▼                ▼
    ┌───────────┐   ┌───────────┐   ┌───────────┐
    │    API    │   │    API    │   │    API    │
    │  Server 1 │   │  Server 2 │   │  Server N │
    └───────────┘   └───────────┘   └───────────┘
           │               │                │
           └───────────────┼────────────────┘
                           │
           ┌───────────────┼────────────────┐
           ▼               ▼                ▼
    ┌───────────┐   ┌───────────┐   ┌───────────┐
    │   CACHE   │   │ DATABASE  │   │  COUNTER  │
    │  (Redis)  │   │(PostgreSQL│   │ SERVICE   │
    │           │   │ Cassandra)│   │(ZooKeeper)│
    └───────────┘   └───────────┘   └───────────┘
```

### Database Schema

```sql
-- URLs Table
CREATE TABLE urls (
    id BIGSERIAL PRIMARY KEY,
    short_code VARCHAR(7) UNIQUE NOT NULL,
    long_url TEXT NOT NULL,
    user_id BIGINT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    expiration_date TIMESTAMP,
    click_count BIGINT DEFAULT 0,
    INDEX idx_short_code (short_code),
    INDEX idx_user_id (user_id)
);

-- Analytics Table (for detailed tracking)
CREATE TABLE url_clicks (
    id BIGSERIAL PRIMARY KEY,
    short_code VARCHAR(7) NOT NULL,
    clicked_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    ip_address VARCHAR(45),
    user_agent TEXT,
    referrer TEXT,
    country VARCHAR(2),
    INDEX idx_short_code (short_code),
    INDEX idx_clicked_at (clicked_at)
);
```

### Detailed Design: Redirection Flow

```python
"""
URL Redirection with caching
"""

class URLRedirectionService:
    def __init__(self, cache, database):
        self.cache = cache
        self.database = database
    
    def redirect(self, short_code):
        """
        Redirection flow with 3-level caching
        """
        # 1. Check browser cache (client-side)
        # Handled by HTTP Cache-Control headers
        
        # 2. Check Redis cache
        long_url = self.cache.get(short_code)
        if long_url:
            # Cache hit
            self.increment_click_count_async(short_code)
            return {
                "status": 302,
                "location": long_url,
                "cache_control": "public, max-age=3600"  # 1 hour
            }
        
        # 3. Check database
        url_record = self.database.get_url(short_code)
        if not url_record:
            return {"status": 404, "message": "URL not found"}
        
        # Check expiration
        if url_record['expiration_date'] and \
           url_record['expiration_date'] < datetime.now():
            return {"status": 410, "message": "URL expired"}
        
        # Store in cache for future requests
        self.cache.set(
            short_code, 
            url_record['long_url'],
            expiry=3600  # 1 hour TTL
        )
        
        # Async update click count
        self.increment_click_count_async(short_code)
        
        return {
            "status": 302,
            "location": url_record['long_url']
        }
    
    def increment_click_count_async(self, short_code):
        """
        Increment click count asynchronously
        """
        # Option 1: Message queue
        message_queue.publish({
            "type": "click",
            "short_code": short_code,
            "timestamp": datetime.now()
        })
        
        # Option 2: Redis counter
        self.cache.incr(f"clicks:{short_code}")
```

### Scaling Considerations

```python
"""
Scaling URL Shortener
"""

# 1. Database Sharding
class ShardedDatabase:
    def get_shard(self, short_code):
        """
        Shard by short_code hash
        """
        shard_count = 10
        shard_id = hash(short_code) % shard_count
        return self.shards[shard_id]

# 2. Rate Limiting
class RateLimiter:
    def check_rate_limit(self, user_id):
        """
        Limit users to 100 URLs per hour
        """
        key = f"rate_limit:{user_id}:{datetime.now().hour}"
        count = self.cache.incr(key)
        
        if count == 1:
            self.cache.expire(key, 3600)  # 1 hour
        
        return count <= 100

# 3. Analytics Processing
class AnalyticsProcessor:
    def process_clicks(self):
        """
        Batch process click events
        """
        # Collect clicks from message queue
        clicks = message_queue.fetch_batch(1000)
        
        # Batch insert to analytics database
        self.analytics_db.bulk_insert(clicks)
        
        # Update aggregated metrics
        self.update_metrics(clicks)
```

---

## 2. Design Instagram / Photo Sharing

### Requirements

**Functional:**
- Upload photos
- Follow users
- View photo feed
- Like and comment on photos
- Search users

**Non-Functional:**
- 500M daily active users
- 100M photos uploaded per day
- Average photo size: 3 MB
- Read-heavy (100:1 read to write)

### Capacity Estimation

```python
# Storage calculation
PHOTOS_PER_DAY = 100_000_000
PHOTO_SIZE = 3 * 1024 * 1024  # 3 MB
YEARS_TO_STORE = 5

# Daily storage
daily_storage = PHOTOS_PER_DAY * PHOTO_SIZE
print(f"Daily storage: {daily_storage / (1024**4):.2f} TB")  # 272 TB/day

# 5-year storage
total_storage = daily_storage * 365 * YEARS_TO_STORE
print(f"5-year storage: {total_storage / (1024**5):.2f} PB")  # 484 PB

# QPS calculation
write_qps = PHOTOS_PER_DAY / 86400
read_qps = write_qps * 100

print(f"Write QPS: {write_qps:.0f}")  # 1,157 writes/sec
print(f"Read QPS: {read_qps:,.0f}")   # 115,740 reads/sec
```

### API Design

```javascript
// 1. Upload photo
POST /api/v1/photos
Content-Type: multipart/form-data

{
  "image": <binary>,
  "caption": "Beautiful sunset!",
  "location": "San Francisco, CA",
  "tags": ["sunset", "nature"]
}

Response 201:
{
  "photo_id": "photo123",
  "url": "https://cdn.instagram.com/photo123.jpg",
  "thumbnail_url": "https://cdn.instagram.com/photo123_thumb.jpg"
}

// 2. Get user feed
GET /api/v1/feed?user_id=user123&limit=20&cursor=cursor_token

Response 200:
{
  "photos": [
    {
      "photo_id": "photo456",
      "user_id": "user789",
      "username": "johndoe",
      "url": "https://cdn.instagram.com/photo456.jpg",
      "caption": "Amazing view!",
      "likes_count": 523,
      "created_at": "2024-01-15T10:30:00Z"
    }
  ],
  "next_cursor": "cursor_token_2"
}

// 3. Like a photo
POST /api/v1/photos/{photo_id}/like
{
  "user_id": "user123"
}
```

### High-Level Architecture

```
                    ┌──────────────┐
                    │    USERS     │
                    └──────┬───────┘
                           │
                           ▼
              ┌────────────────────────┐
              │    CDN (Images)        │
              └────────────────────────┘
                           │
                           ▼
              ┌────────────────────────┐
              │   Load Balancer        │
              └────────────────────────┘
                           │
        ┌──────────────────┼────────────────────┐
        ▼                  ▼                    ▼
┌──────────────┐   ┌──────────────┐   ┌──────────────┐
│   Upload     │   │    Feed      │   │   Social     │
│   Service    │   │   Service    │   │   Graph      │
└──────────────┘   └──────────────┘   └──────────────┘
        │                  │                    │
        │                  │                    │
┌───────▼────────┐ ┌───────▼───────┐  ┌────────▼─────┐
│  Object Store  │ │ Feed Cache    │  │  Graph DB    │
│  (S3/GCS)      │ │ (Redis)       │  │ (Neo4j)      │
└────────────────┘ └───────────────┘  └──────────────┘
        │                  │
┌───────▼────────┐ ┌───────▼───────┐
│  Metadata DB   │ │  Metrics DB   │
│  (Cassandra)   │ │  (ClickHouse) │
└────────────────┘ └───────────────┘
```

### Photo Upload Flow

```python
"""
Detailed photo upload process
"""

class PhotoUploadService:
    def __init__(self, object_store, metadata_db, cdn):
        self.object_store = object_store
        self.metadata_db = metadata_db
        self.cdn = cdn
        self.image_processor = ImageProcessor()
    
    async def upload_photo(self, user_id, image_data, metadata):
        """
        Complete photo upload flow
        """
        photo_id = self.generate_photo_id()
        
        # 1. Validate image
        if not self.validate_image(image_data):
            raise ValidationError("Invalid image format")
        
        # 2. Generate multiple sizes
        sizes = await self.image_processor.generate_sizes(image_data)
        # sizes = {
        #   'original': (4096x4096),
        #   'large': (1080x1080),
        #   'medium': (640x640),
        #   'thumbnail': (150x150)
        # }
        
        # 3. Upload to object storage
        urls = {}
        for size_name, image_bytes in sizes.items():
            key = f"photos/{user_id}/{photo_id}_{size_name}.jpg"
            url = await self.object_store.upload(key, image_bytes)
            urls[size_name] = url
        
        # 4. Store metadata in database
        photo_metadata = {
            'photo_id': photo_id,
            'user_id': user_id,
            'original_url': urls['original'],
            'large_url': urls['large'],
            'medium_url': urls['medium'],
            'thumbnail_url': urls['thumbnail'],
            'caption': metadata.get('caption'),
            'location': metadata.get('location'),
            'tags': metadata.get('tags', []),
            'created_at': datetime.now(),
            'likes_count': 0,
            'comments_count': 0
        }
        
        await self.metadata_db.insert('photos', photo_metadata)
        
        # 5. Fan out to followers' feeds
        await self.fanout_to_followers(user_id, photo_id)
        
        # 6. Return CDN URLs
        return {
            'photo_id': photo_id,
            'url': self.cdn.get_url(urls['large']),
            'thumbnail_url': self.cdn.get_url(urls['thumbnail'])
        }

class ImageProcessor:
    async def generate_sizes(self, image_data):
        """
        Generate multiple image sizes
        Uses PIL/Pillow or similar library
        """
        from PIL import Image
        import io
        
        img = Image.open(io.BytesIO(image_data))
        sizes = {}
        
        # Original
        sizes['original'] = image_data
        
        # Large (1080x1080)
        large = img.copy()
        large.thumbnail((1080, 1080), Image.LANCZOS)
        sizes['large'] = self.image_to_bytes(large)
        
        # Medium (640x640)
        medium = img.copy()
        medium.thumbnail((640, 640), Image.LANCZOS)
        sizes['medium'] = self.image_to_bytes(medium)
        
        # Thumbnail (150x150)
        thumb = img.copy()
        thumb.thumbnail((150, 150), Image.LANCZOS)
        sizes['thumbnail'] = self.image_to_bytes(thumb)
        
        return sizes
    
    def image_to_bytes(self, image):
        buffer = io.BytesIO()
        image.save(buffer, format='JPEG', quality=85)
        return buffer.getvalue()
```

### Feed Generation

```python
"""
News feed generation for Instagram
"""

class FeedService:
    def __init__(self, cache, graph_db, photo_db):
        self.cache = cache
        self.graph_db = graph_db
        self.photo_db = photo_db
    
    async def get_feed(self, user_id, limit=20, cursor=None):
        """
        Hybrid approach: Pre-computed + on-demand
        """
        # 1. Try cache first
        cached_feed = await self.cache.get(f"feed:{user_id}")
        if cached_feed and not cursor:
            return self.parse_cached_feed(cached_feed, limit)
        
        # 2. Get users being followed
        following = await self.graph_db.get_following(user_id)
        
        # 3. For regular users: Pre-computed feed
        if len(following) < 1000:  # Not following too many
            return await self.get_precomputed_feed(user_id, limit, cursor)
        
        # 4. For power users: On-demand aggregation
        return await self.get_aggregated_feed(following, limit, cursor)
    
    async def get_precomputed_feed(self, user_id, limit, cursor):
        """
        Feed is pre-computed when photos are uploaded
        """
        feed_key = f"feed:{user_id}"
        
        # Redis sorted set with timestamp scores
        if cursor:
            max_score = cursor
        else:
            max_score = '+inf'
        
        # Get photo IDs from sorted set
        photo_ids = await self.cache.zrevrangebyscore(
            feed_key,
            max_score,
            '-inf',
            start=0,
            num=limit
        )
        
        # Fetch photo details
        photos = await self.photo_db.get_photos(photo_ids)
        
        return {
            'photos': photos,
            'next_cursor': photos[-1]['created_at'] if photos else None
        }
    
    async def fanout_to_followers(self, author_id, photo_id):
        """
        When user uploads photo, add to all followers' feeds
        """
        followers = await self.graph_db.get_followers(author_id)
        
        # For each follower, add to their feed cache
        timestamp = datetime.now().timestamp()
        
        tasks = []
        for follower_id in followers:
            # Add to Redis sorted set
            task = self.cache.zadd(
                f"feed:{follower_id}",
                {photo_id: timestamp}
            )
            tasks.append(task)
        
        await asyncio.gather(*tasks)
        
        # Trim old entries (keep last 1000)
        for follower_id in followers:
            await self.cache.zremrangebyrank(
                f"feed:{follower_id}",
                0,
                -1001  # Keep last 1000
            )
```

### Database Schema

```sql
-- Photos Table (Cassandra)
CREATE TABLE photos (
    photo_id UUID PRIMARY KEY,
    user_id UUID,
    original_url TEXT,
    large_url TEXT,
    medium_url TEXT,
    thumbnail_url TEXT,
    caption TEXT,
    location TEXT,
    tags LIST<TEXT>,
    created_at TIMESTAMP,
    likes_count COUNTER,
    comments_count COUNTER
);

CREATE INDEX ON photos(user_id);

-- User Photos (for profile page)
CREATE TABLE user_photos (
    user_id UUID,
    created_at TIMESTAMP,
    photo_id UUID,
    PRIMARY KEY (user_id, created_at, photo_id)
) WITH CLUSTERING ORDER BY (created_at DESC);

-- Likes Table
CREATE TABLE photo_likes (
    photo_id UUID,
    user_id UUID,
    created_at TIMESTAMP,
    PRIMARY KEY (photo_id, user_id)
);

-- Social Graph (Neo4j)
(:User {user_id, username})-[:FOLLOWS]->(:User)
(:User)-[:POSTED]->(:Photo {photo_id})
(:User)-[:LIKES]->(:Photo)
```

---

## 3. Design Twitter / Social Media Feed

### Requirements & Capacity Estimation

```python
# Twitter scale
DAU = 200_000_000  # 200M daily active users
TWEETS_PER_DAY = 400_000_000  # 400M tweets
AVERAGE_FOLLOWERS = 200

# Storage
TWEET_SIZE = 300  # bytes
daily_storage = TWEETS_PER_DAY * TWEET_SIZE / (1024**3)
print(f"Daily storage: {daily_storage:.2f} GB")  # 112 GB/day

# QPS
write_qps = TWEETS_PER_DAY / 86400
read_qps = write_qps * 100  # Read-heavy
print(f"Write QPS: {write_qps:.0f}")  # 4,630/sec
print(f"Read QPS: {read_qps:,.0f}")   # 463,000/sec
```

### Architecture

```
┌─────────────────────────────────────────────────────┐
│                     CLIENTS                         │
└─────────────────────────────────────────────────────┘
                        │
                        ▼
┌─────────────────────────────────────────────────────┐
│              API GATEWAY / LOAD BALANCER            │
└─────────────────────────────────────────────────────┘
                        │
        ┌───────────────┼────────────────┐
        ▼               ▼                ▼
┌─────────────┐ ┌─────────────┐ ┌─────────────┐
│   Tweet     │ │  Timeline   │ │   Search    │
│  Service    │ │  Service    │ │  Service    │
└─────────────┘ └─────────────┘ └─────────────┘
        │               │                │
        │               │                │
┌───────▼──────┐ ┌──────▼─────┐  ┌──────▼──────┐
│  Tweet DB    │ │ Timeline   │  │ElasticSearch│
│ (Cassandra)  │ │Cache(Redis)│  │             │
└──────────────┘ └────────────┘  └─────────────┘
        │
        ▼
┌──────────────┐
│Message Queue │
│   (Kafka)    │
└──────────────┘
        │
        ▼
┌──────────────┐
│  Fan-out     │
│  Workers     │
└──────────────┘
```

---

## 4. Design Uber / Ride Sharing

### Core Challenges

1. **Real-time location tracking**
2. **Matching drivers to riders**
3. **ETA calculation**
4. **Dynamic pricing**
5. **High availability**

### Capacity Estimation

```python
# Uber scale
DAILY_RIDES = 15_000_000  # 15M rides per day
ACTIVE_DRIVERS = 3_000_000  # 3M drivers
LOCATION_UPDATE_INTERVAL = 4  # seconds

# Location updates per day
updates_per_driver = 86400 / LOCATION_UPDATE_INTERVAL
total_updates = ACTIVE_DRIVERS * updates_per_driver

print(f"Location updates per day: {total_updates:,.0f}")  # 64.8 billion

# QPS
update_qps = total_updates / 86400
print(f"Location update QPS: {update_qps:,.0f}")  # 750,000/sec
```

### API Design

```javascript
// 1. Update driver location
POST /api/v1/drivers/{driver_id}/location
{
  "latitude": 37.7749,
  "longitude": -122.4194,
  "timestamp": "2024-01-15T10:30:00Z"
}

// 2. Request ride
POST /api/v1/rides/request
{
  "rider_id": "rider123",
  "pickup_location": {
    "latitude": 37.7749,
    "longitude": -122.4194
  },
  "dropoff_location": {
    "latitude": 37.7849,
    "longitude": -122.4094
  }
}

Response 200:
{
  "ride_id": "ride456",
  "driver_id": "driver789",
  "eta_seconds": 300,
  "estimated_fare": 15.50
}

// 3. Get nearby drivers
GET /api/v1/drivers/nearby?lat=37.7749&lon=-122.4194&radius=5
```

### Location Storage & Querying

```python
"""
Geospatial indexing for finding nearby drivers
"""

class LocationService:
    def __init__(self, redis_client):
        self.redis = redis_client
    
    def update_driver_location(self, driver_id, latitude, longitude):
        """
        Store driver location using Redis Geospatial
        """
        # GEOADD key longitude latitude member
        self.redis.geoadd(
            'drivers:locations',
            longitude,
            latitude,
            driver_id
        )
        
        # Set TTL to remove inactive drivers
        self.redis.expire(f'driver:{driver_id}:location', 300)  # 5 min
    
    def get_nearby_drivers(self, latitude, longitude, radius_km=5):
        """
        Find drivers within radius
        """
        # GEORADIUS key longitude latitude radius km
        drivers = self.redis.georadius(
            'drivers:locations',
            longitude,
            latitude,
            radius_km,
            unit='km',
            withdist=True,
            sort='ASC'
        )
        
        return [
            {
                'driver_id': driver_id,
                'distance_km': distance
            }
            for driver_id, distance in drivers
        ]

# Alternative: QuadTree for in-memory search
class QuadTree:
    """
    Divide map into grid for efficient spatial queries
    """
    def __init__(self, boundary, capacity=4):
        self.boundary = boundary  # (min_lat, max_lat, min_lon, max_lon)
        self.capacity = capacity
        self.points = []
        self.divided = False
        
    def insert(self, point):
        """Insert driver location"""
        if not self.boundary.contains(point):
            return False
        
        if len(self.points) < self.capacity:
            self.points.append(point)
            return True
        
        if not self.divided:
            self.subdivide()
        
        return (self.northeast.insert(point) or
                self.northwest.insert(point) or
                self.southeast.insert(point) or
                self.southwest.insert(point))
    
    def query_range(self, range_boundary):
        """Find all drivers in range"""
        found = []
        
        if not self.boundary.intersects(range_boundary):
            return found
        
        for point in self.points:
            if range_boundary.contains(point):
                found.append(point)
        
        if self.divided:
            found.extend(self.northeast.query_range(range_boundary))
            found.extend(self.northwest.query_range(range_boundary))
            found.extend(self.southeast.query_range(range_boundary))
            found.extend(self.southwest.query_range(range_boundary))
        
        return found
```

### Driver-Rider Matching Algorithm

```python
"""
Smart matching algorithm
"""

class RideMatchingService:
    def __init__(self, location_service, pricing_service):
        self.location_service = location_service
        self.pricing_service = pricing_service
    
    def find_best_driver(self, ride_request):
        """
        Match rider with best available driver
        
        Criteria:
        1. Distance to rider
        2. Driver rating
        3. Car type match
        4. Acceptance rate
        """
        pickup_location = ride_request['pickup_location']
        
        # 1. Get nearby drivers (within 5km)
        nearby_drivers = self.location_service.get_nearby_drivers(
            pickup_location['latitude'],
            pickup_location['longitude'],
            radius_km=5
        )
        
        if not nearby_drivers:
            # Expand search radius
            nearby_drivers = self.location_service.get_nearby_drivers(
                pickup_location['latitude'],
                pickup_location['longitude'],
                radius_km=10
            )
        
        # 2. Filter by availability and car type
        available_drivers = []
        for driver in nearby_drivers:
            driver_info = self.get_driver_info(driver['driver_id'])
            
            if (driver_info['status'] == 'available' and
                driver_info['car_type'] == ride_request.get('car_type', 'standard')):
                available_drivers.append({
                    **driver,
                    **driver_info
                })
        
        if not available_drivers:
            return None
        
        # 3. Score and rank drivers
        scored_drivers = []
        for driver in available_drivers:
            score = self.calculate_driver_score(driver, ride_request)
            scored_drivers.append((score, driver))
        
        # 4. Return best driver
        scored_drivers.sort(reverse=True)
        best_driver = scored_drivers[0][1]
        
        return best_driver
    
    def calculate_driver_score(self, driver, ride_request):
        """
        Scoring formula:
        score = (rating * 0.4) + (acceptance_rate * 0.3) + 
                (proximity_score * 0.3)
        """
        # Rating (0-5 -> 0-1)
        rating_score = driver['rating'] / 5.0
        
        # Acceptance rate (0-1)
        acceptance_score = driver['acceptance_rate']
        
        # Proximity (closer is better)
        # 0-1km: 1.0, 1-3km: 0.7, 3-5km: 0.4
        distance = driver['distance_km']
        if distance < 1:
            proximity_score = 1.0
        elif distance < 3:
            proximity_score = 0.7
        else:
            proximity_score = 0.4
        
        # Weighted sum
        score = (rating_score * 0.4 + 
                acceptance_score * 0.3 + 
                proximity_score * 0.3)
        
        return score
```

---

## 5. Design Rate Limiter

### Requirements

- Limit API requests per user/IP
- Different limits for different endpoints
- Distributed system support
- Low latency overhead
- Accurate counting

### Algorithms Comparison

```python
"""
Rate Limiting Algorithms
"""

# 1. TOKEN BUCKET
class TokenBucket:
    """
    Pros: Smooth rate limiting, allows bursts
    Cons: More complex implementation
    """
    def __init__(self, capacity, refill_rate):
        self.capacity = capacity
        self.tokens = capacity
        self.refill_rate = refill_rate  # tokens per second
        self.last_refill = time.time()
    
    def allow_request(self):
        # Refill tokens
        now = time.time()
        elapsed = now - self.last_refill
        tokens_to_add = elapsed * self.refill_rate
        
        self.tokens = min(self.capacity, self.tokens + tokens_to_add)
        self.last_refill = now
        
        # Check if request allowed
        if self.tokens >= 1:
            self.tokens -= 1
            return True
        return False

# 2. SLIDING WINDOW LOG
class SlidingWindowLog:
    """
    Pros: Most accurate
    Cons: Memory intensive (stores all timestamps)
    """
    def __init__(self, limit, window_seconds):
        self.limit = limit
        self.window = window_seconds
        self.requests = []  # List of timestamps
    
    def allow_request(self):
        now = time.time()
        
        # Remove old requests
        cutoff = now - self.window
        self.requests = [req for req in self.requests if req > cutoff]
        
        # Check limit
        if len(self.requests) < self.limit:
            self.requests.append(now)
            return True
        return False

# 3. SLIDING WINDOW COUNTER (REDIS)
class SlidingWindowCounter:
    """
    Pros: Memory efficient, accurate enough
    Cons: Requires Redis
    
    Best for production!
    """
    def __init__(self, redis_client, limit, window_seconds):
        self.redis = redis_client
        self.limit = limit
        self.window = window_seconds
    
    def allow_request(self, user_id):
        key = f"rate_limit:{user_id}"
        now = time.time()
        
        # Use Redis sorted set with timestamps as scores
        pipe = self.redis.pipeline()
        
        # Remove old entries
        pipe.zremrangebyscore(key, 0, now - self.window)
        
        # Count requests in window
        pipe.zcard(key)
        
        # Add current request
        pipe.zadd(key, {str(now): now})
        
        # Set expiry
        pipe.expire(key, self.window)
        
        results = pipe.execute()
        request_count = results[1]
        
        return request_count < self.limit

# 4. FIXED WINDOW COUNTER (Simplest)
class FixedWindowCounter:
    """
    Pros: Very simple, memory efficient
    Cons: Can allow 2x requests at window boundary
    """
    def __init__(self, redis_client, limit, window_seconds):
        self.redis = redis_client
        self.limit = limit
        self.window = window_seconds
    
    def allow_request(self, user_id):
        # Key includes current time window
        window_start = int(time.time() / self.window) * self.window
        key = f"rate_limit:{user_id}:{window_start}"
        
        # Increment counter
        count = self.redis.incr(key)
        
        # Set expiry on first request
        if count == 1:
            self.redis.expire(key, self.window * 2)
        
        return count <= self.limit
```

### Production Implementation

```python
"""
Complete rate limiter with multiple tiers
"""

class RateLimiterMiddleware:
    def __init__(self, redis_client):
        self.redis = redis_client
        self.limits = {
            'free': {'requests': 100, 'window': 3600},      # 100/hour
            'premium': {'requests': 1000, 'window': 3600},  # 1000/hour
            'enterprise': {'requests': 10000, 'window': 3600}
        }
    
    def check_rate_limit(self, user_id, tier='free'):
        """Check if request is allowed"""
        config = self.limits[tier]
        limiter = SlidingWindowCounter(
            self.redis,
            config['requests'],
            config['window']
        )
        
        if not limiter.allow_request(user_id):
            # Return 429 Too Many Requests
            remaining_time = self.get_retry_after(user_id, tier)
            raise RateLimitExceeded(
                f"Rate limit exceeded. Retry after {remaining_time} seconds"
            )
        
        # Add headers
        return {
            'X-RateLimit-Limit': config['requests'],
            'X-RateLimit-Remaining': self.get_remaining(user_id, tier),
            'X-RateLimit-Reset': self.get_reset_time(user_id, tier)
        }
    
    def get_remaining(self, user_id, tier):
        """Get remaining requests in current window"""
        config = self.limits[tier]
        key = f"rate_limit:{user_id}"
        now = time.time()
        
        count = self.redis.zcount(
            key,
            now - config['window'],
            now
        )
        
        return max(0, config['requests'] - count)
```

---

## Key Patterns Across All Designs

### 1. Always Start With Requirements
```
Clarify → Estimate → Design → Deep Dive
```

### 2. Common Architecture Components
```
- Load Balancer
- API Servers
- Cache Layer (Redis/Memcached)
- Database (SQL/NoSQL)
- Object Storage (S3)
- Message Queue (Kafka/RabbitMQ)
- CDN
```

### 3. Scale Using
```
- Horizontal scaling
- Caching strategies
- Database sharding
- Replication
- Async processing
```

### 4. Trade-offs to Discuss
```
- Consistency vs Availability
- Latency vs Throughput
- Cost vs Performance
- Complexity vs Maintainability
```

Remember: **There's no perfect solution**, only appropriate trade-offs for the given requirements!