# Chapter 17: Performance Optimization

## Table of Contents
1. Introduction to Performance Optimization
2. Profiling and Benchmarking
3. N+1 Query Problem
4. Lazy Loading vs Eager Loading
5. Asynchronous Processing
6. Compression Techniques
7. Code-Level Optimizations
8. Memory Management
9. Database Query Optimization
10. Caching Strategies
11. Real-World Performance Tuning

---

## 1. Introduction to Performance Optimization

**Definition:** Performance optimization is the process of making a system run faster, use fewer resources, and handle more load.

### Performance Metrics

**1. Latency** - Time to complete a single operation
```
Good: < 100ms
Acceptable: 100-500ms
Poor: > 500ms
```

**2. Throughput** - Number of operations per time unit
```
Low: < 100 requests/second
Medium: 100-1,000 requests/second
High: > 1,000 requests/second
```

**3. Resource Utilization**
```
CPU: Should be < 70% average
Memory: Should be < 80% to avoid swapping
Disk I/O: Monitor IOPS and throughput
Network: Monitor bandwidth usage
```

### The Golden Rule

> **"Premature optimization is the root of all evil."** - Donald Knuth

**Process:**
1. **Measure first** - Identify actual bottlenecks
2. **Optimize** - Fix the bottleneck
3. **Measure again** - Verify improvement
4. **Repeat** - Move to next bottleneck

---

## 2. Profiling and Benchmarking

### Application Profiling

**Node.js Profiling:**

```javascript
// ============================================
// Built-in Profiler
// ============================================

// Start with --prof flag
// node --prof app.js

// Generates isolate-*.log file
// Process with: node --prof-process isolate-*.log

// ============================================
// Code-Level Profiling
// ============================================

class Profiler {
  static measure(name, fn) {
    const start = performance.now();
    const result = fn();
    const end = performance.now();
    console.log(`${name} took ${(end - start).toFixed(2)}ms`);
    return result;
  }
  
  static async measureAsync(name, fn) {
    const start = performance.now();
    const result = await fn();
    const end = performance.now();
    console.log(`${name} took ${(end - start).toFixed(2)}ms`);
    return result;
  }
}

// Usage
const users = Profiler.measure('Get Users', () => {
  return database.query('SELECT * FROM users');
});

await Profiler.measureAsync('Process Order', async () => {
  return await processOrder(orderId);
});

// ============================================
// Detailed Performance Tracking
// ============================================

class PerformanceTracker {
  constructor() {
    this.metrics = new Map();
  }
  
  start(label) {
    this.metrics.set(label, {
      start: performance.now(),
      end: null,
      duration: null
    });
  }
  
  end(label) {
    const metric = this.metrics.get(label);
    if (!metric) return;
    
    metric.end = performance.now();
    metric.duration = metric.end - metric.start;
    
    console.log(`[PERF] ${label}: ${metric.duration.toFixed(2)}ms`);
  }
  
  getMetrics() {
    const results = {};
    this.metrics.forEach((value, key) => {
      results[key] = value.duration;
    });
    return results;
  }
}

// Usage
const perf = new PerformanceTracker();

async function handleRequest(req, res) {
  perf.start('total');
  
  perf.start('auth');
  const user = await authenticate(req);
  perf.end('auth');
  
  perf.start('db_query');
  const data = await database.query('SELECT * FROM orders WHERE user_id = ?', [user.id]);
  perf.end('db_query');
  
  perf.start('processing');
  const processed = processData(data);
  perf.end('processing');
  
  perf.start('render');
  const response = renderResponse(processed);
  perf.end('render');
  
  perf.end('total');
  
  console.log('Performance metrics:', perf.getMetrics());
  
  res.json(response);
}

// Output:
// [PERF] auth: 45.23ms
// [PERF] db_query: 123.45ms
// [PERF] processing: 12.34ms
// [PERF] render: 5.67ms
// [PERF] total: 186.69ms
```

**Python Profiling:**

```python
# ============================================
# cProfile - Built-in profiler
# ============================================
import cProfile
import pstats

def expensive_function():
    result = []
    for i in range(1000000):
        result.append(i * i)
    return result

# Profile function
profiler = cProfile.Profile()
profiler.enable()

expensive_function()

profiler.disable()

# Print stats
stats = pstats.Stats(profiler)
stats.sort_stats('cumulative')
stats.print_stats(10)  # Top 10 functions

# ============================================
# line_profiler - Line-by-line profiling
# ============================================
# Install: pip install line_profiler

from line_profiler import LineProfiler

def process_data(items):
    result = []
    for item in items:
        # Slow operation
        processed = heavy_computation(item)
        result.append(processed)
    return result

# Profile
lp = LineProfiler()
lp.add_function(process_data)
lp.enable()

process_data(range(1000))

lp.disable()
lp.print_stats()

# Output shows time per line:
# Line #  Hits  Time    Per Hit   % Time  Line Contents
# ====================================================
#      2     1    10.0     10.0      0.0      result = []
#      3  1000   500.0      0.5      5.0      for item in items:
#      4  1000  9000.0      9.0     90.0          processed = heavy_computation(item)
#      5  1000   500.0      0.5      5.0          result.append(processed)
```

### Benchmarking

```python
import time

class Benchmark:
    @staticmethod
    def compare(*functions):
        """Compare performance of multiple functions"""
        results = {}
        
        for func in functions:
            start = time.time()
            
            # Run function 1000 times
            for _ in range(1000):
                func()
            
            end = time.time()
            duration = (end - start) * 1000  # Convert to ms
            
            results[func.__name__] = {
                'total_ms': duration,
                'avg_ms': duration / 1000
            }
        
        # Print comparison
        print("Benchmark Results:")
        print("-" * 50)
        for name, metrics in sorted(results.items(), key=lambda x: x[1]['total_ms']):
            print(f"{name:30s} {metrics['avg_ms']:8.3f}ms avg")
        
        return results

# Example
def approach1():
    return [i * 2 for i in range(1000)]

def approach2():
    result = []
    for i in range(1000):
        result.append(i * 2)
    return result

def approach3():
    return list(map(lambda x: x * 2, range(1000)))

Benchmark.compare(approach1, approach2, approach3)

# Output:
# Benchmark Results:
# --------------------------------------------------
# approach1                          0.045ms avg
# approach3                          0.052ms avg
# approach2                          0.067ms avg
```

---

## 3. N+1 Query Problem

**The Problem:** Making N+1 database queries when 1 query would suffice.

### Example Problem

```python
# ❌ BAD: N+1 queries
def get_users_with_orders():
    # Query 1: Get all users
    users = db.query("SELECT * FROM users LIMIT 10")
    
    # Queries 2-11: Get orders for each user (N queries)
    for user in users:
        user['orders'] = db.query(
            "SELECT * FROM orders WHERE user_id = ?",
            [user['id']]
        )
    
    return users

# Total: 1 + 10 = 11 queries!
# With 100 users = 101 queries!
# With 1000 users = 1001 queries!
```

### Solution 1: JOIN

```python
# ✅ GOOD: Single query with JOIN
def get_users_with_orders():
    query = """
        SELECT 
            users.id,
            users.name,
            users.email,
            orders.id as order_id,
            orders.total,
            orders.created_at as order_date
        FROM users
        LEFT JOIN orders ON orders.user_id = users.id
        WHERE users.id IN (SELECT id FROM users LIMIT 10)
    """
    
    results = db.query(query)
    
    # Group by user
    users_dict = {}
    for row in results:
        user_id = row['id']
        
        if user_id not in users_dict:
            users_dict[user_id] = {
                'id': user_id,
                'name': row['name'],
                'email': row['email'],
                'orders': []
            }
        
        if row['order_id']:
            users_dict[user_id]['orders'].append({
                'id': row['order_id'],
                'total': row['total'],
                'created_at': row['order_date']
            })
    
    return list(users_dict.values())

# Total: 1 query regardless of number of users!
```

### Solution 2: Eager Loading (ORM)

```python
# ============================================
# SQLAlchemy with Eager Loading
# ============================================
from sqlalchemy import Column, Integer, String, ForeignKey, create_engine
from sqlalchemy.orm import relationship, sessionmaker, joinedload
from sqlalchemy.ext.declarative import declarative_base

Base = declarative_base()

class User(Base):
    __tablename__ = 'users'
    
    id = Column(Integer, primary_key=True)
    name = Column(String)
    email = Column(String)
    
    # Relationship
    orders = relationship('Order', back_populates='user')

class Order(Base):
    __tablename__ = 'orders'
    
    id = Column(Integer, primary_key=True)
    user_id = Column(Integer, ForeignKey('users.id'))
    total = Column(Integer)
    
    user = relationship('User', back_populates='orders')

# Create session
engine = create_engine('postgresql://localhost/myapp')
Session = sessionmaker(bind=engine)
session = Session()

# ❌ BAD: Lazy loading (N+1 queries)
users = session.query(User).limit(10).all()
for user in users:
    print(user.orders)  # Each access triggers a query!

# ✅ GOOD: Eager loading (1 query)
users = session.query(User).options(
    joinedload(User.orders)
).limit(10).all()

for user in users:
    print(user.orders)  # No additional queries!

# Generated SQL:
# SELECT users.*, orders.*
# FROM users
# LEFT JOIN orders ON orders.user_id = users.id
# LIMIT 10
```

### Solution 3: Batch Loading (DataLoader)

```javascript
// ============================================
// DataLoader (Facebook)
// ============================================
const DataLoader = require('dataloader');

// Batch function - loads multiple users at once
async function batchLoadUsers(userIds) {
  console.log(`Loading users: ${userIds}`);
  
  const results = await db.query(
    'SELECT * FROM users WHERE id = ANY($1)',
    [userIds]
  );
  
  // Return in same order as requested
  const userMap = new Map(results.rows.map(u => [u.id, u]));
  return userIds.map(id => userMap.get(id));
}

// Create DataLoader
const userLoader = new DataLoader(batchLoadUsers);

// ❌ Without DataLoader (N+1 queries)
async function getOrdersWithUsers(orderIds) {
  const orders = await db.query(
    'SELECT * FROM orders WHERE id = ANY($1)',
    [orderIds]
  );
  
  for (const order of orders) {
    // Each call triggers a query
    order.user = await db.query(
      'SELECT * FROM users WHERE id = $1',
      [order.user_id]
    );
  }
  
  return orders;
}

// ✅ With DataLoader (2 queries total)
async function getOrdersWithUsers(orderIds) {
  const orders = await db.query(
    'SELECT * FROM orders WHERE id = ANY($1)',
    [orderIds]
  );
  
  for (const order of orders) {
    // Batched automatically
    order.user = await userLoader.load(order.user_id);
  }
  
  return orders;
}

// DataLoader batches all userLoader.load() calls
// SELECT * FROM users WHERE id IN (1, 2, 3, 4, 5)
// Just 1 query for all users!
```

---

## 4. Lazy Loading vs Eager Loading

### Lazy Loading (Load on Demand)

**Definition:** Load related data only when accessed.

```python
# ============================================
# Lazy Loading
# ============================================
class User:
    def __init__(self, id, name, email, db):
        self.id = id
        self.name = name
        self.email = email
        self._db = db
        self._orders = None  # Not loaded yet
    
    @property
    def orders(self):
        """Load orders on first access (lazy)"""
        if self._orders is None:
            print(f"Loading orders for user {self.id}...")
            self._orders = self._db.query(
                'SELECT * FROM orders WHERE user_id = ?',
                [self.id]
            )
        return self._orders

# Usage
user = User(123, "John", "john@example.com", db)
print(user.name)     # No query for orders
print(user.email)    # Still no query
print(user.orders)   # NOW queries orders (lazy)
```

**Pros:**
- Don't load unnecessary data
- Saves memory
- Faster initial load

**Cons:**
- N+1 query problem
- Unpredictable performance
- Additional queries later

### Eager Loading (Load Upfront)

**Definition:** Load all related data immediately.

```python
# ============================================
# Eager Loading
# ============================================
class User:
    def __init__(self, id, name, email, orders):
        self.id = id
        self.name = name
        self.email = email
        self.orders = orders  # Already loaded

def get_users_eager():
    """Eager load users with orders"""
    # Single query with JOIN
    results = db.query("""
        SELECT 
            users.*,
            orders.id as order_id,
            orders.total
        FROM users
        LEFT JOIN orders ON orders.user_id = users.id
    """)
    
    # Build user objects with orders
    users_dict = {}
    for row in results:
        if row['id'] not in users_dict:
            users_dict[row['id']] = User(
                id=row['id'],
                name=row['name'],
                email=row['email'],
                orders=[]
            )
        
        if row['order_id']:
            users_dict[row['id']].orders.append({
                'id': row['order_id'],
                'total': row['total']
            })
    
    return list(users_dict.values())

# Usage
users = get_users_eager()
for user in users:
    print(user.orders)  # No additional queries!
```

**Pros:**
- Predictable performance
- No N+1 problem
- Single query

**Cons:**
- Loads unnecessary data sometimes
- Higher memory usage
- Slower initial query

### When to Use Each

```
Use Lazy Loading when:
✅ Related data rarely needed
✅ Related data is large
✅ Loading one entity at a time

Use Eager Loading when:
✅ Related data always needed
✅ Loading multiple entities
✅ Performance is critical
```

---

## 5. Asynchronous Processing

### Blocking vs Non-Blocking

```javascript
// ============================================
// ❌ Blocking (Synchronous) - Slow
// ============================================
function handleRequest(req, res) {
  // Each operation waits for previous
  const user = getUserFromDB(req.userId);          // 50ms
  const orders = getOrdersFromDB(user.id);         // 100ms
  const recommendations = getRecommendations(user.id); // 200ms
  
  res.json({ user, orders, recommendations });
  // Total: 350ms
}

// ============================================
// ✅ Non-Blocking (Asynchronous) - Fast
// ============================================
async function handleRequest(req, res) {
  // All operations run in parallel
  const [user, orders, recommendations] = await Promise.all([
    getUserFromDB(req.userId),           // 50ms
    getOrdersFromDB(req.userId),         // 100ms
    getRecommendations(req.userId)       // 200ms
  ]);
  
  res.json({ user, orders, recommendations });
  // Total: 200ms (slowest operation)
  // Improvement: 43% faster!
}
```

### Async Patterns

**Pattern 1: Promise.all for Parallel Execution**

```typescript
async function getProductDetails(productId: string) {
  // ❌ Sequential (slow)
  const product = await db.getProduct(productId);       // 50ms
  const reviews = await db.getReviews(productId);       // 100ms
  const recommendations = await getRecommendations(productId); // 150ms
  // Total: 300ms
  
  return { product, reviews, recommendations };
}

async function getProductDetailsFast(productId: string) {
  // ✅ Parallel (fast)
  const [product, reviews, recommendations] = await Promise.all([
    db.getProduct(productId),              // 50ms
    db.getReviews(productId),              // 100ms
    getRecommendations(productId)          // 150ms
  ]);
  // Total: 150ms (parallel execution)
  
  return { product, reviews, recommendations };
}
```

**Pattern 2: Background Processing**

```python
# ============================================
# Celery Background Tasks
# ============================================
from celery import Celery
from flask import Flask, jsonify

app = Flask(__name__)
celery = Celery('tasks', broker='redis://localhost:6379/0')

@celery.task
def generate_report(user_id):
    """Long-running task (5 minutes)"""
    print(f"Generating report for user {user_id}")
    # Complex computation...
    time.sleep(300)  # 5 minutes
    return f"Report for user {user_id}"

@app.route('/reports', methods=['POST'])
def create_report():
    user_id = request.json['user_id']
    
    # Start task asynchronously
    task = generate_report.delay(user_id)
    
    # Return immediately (don't wait 5 minutes!)
    return jsonify({
        'task_id': task.id,
        'status': 'processing',
        'message': 'Report generation started'
    }), 202

@app.route('/reports/<task_id>')
def get_report_status(task_id):
    """Check task status"""
    task = generate_report.AsyncResult(task_id)
    
    if task.state == 'PENDING':
        return jsonify({'status': 'pending'})
    elif task.state == 'SUCCESS':
        return jsonify({'status': 'completed', 'result': task.result})
    elif task.state == 'FAILURE':
        return jsonify({'status': 'failed', 'error': str(task.info)})
    
    return jsonify({'status': task.state})
```

---

## 6. Compression Techniques

### HTTP Compression (Gzip)

```javascript
// ============================================
// Express.js with Compression
// ============================================
const express = require('express');
const compression = require('compression');

const app = express();

// Enable gzip compression
app.use(compression({
  level: 6,  // Compression level (1-9, 6 is default)
  threshold: 1024,  // Only compress if > 1KB
  filter: (req, res) => {
    // Don't compress if client doesn't support it
    if (req.headers['x-no-compression']) {
      return false;
    }
    return compression.filter(req, res);
  }
}));

app.get('/api/users', (req, res) => {
  const users = getLargeUserList();  // 500KB uncompressed
  res.json(users);
  // Sent as ~50KB (90% compression for JSON)
});

// Response headers:
// Content-Encoding: gzip
// Content-Length: 51200 (compressed size)
```

### Database Result Compression

```python
import gzip
import json

def compress_results(data):
    """Compress large query results"""
    json_str = json.dumps(data)
    compressed = gzip.compress(json_str.encode('utf-8'))
    
    print(f"Original size: {len(json_str)} bytes")
    print(f"Compressed size: {len(compressed)} bytes")
    print(f"Compression ratio: {len(compressed) / len(json_str):.2%}")
    
    return compressed

def decompress_results(compressed):
    """Decompress results"""
    decompressed = gzip.decompress(compressed)
    return json.loads(decompressed.decode('utf-8'))

# Usage
large_dataset = db.query("SELECT * FROM orders")  # 10MB

compressed = compress_results(large_dataset)
# Cache compressed version
cache.set('orders', compressed)

# Later
compressed_data = cache.get('orders')
data = decompress_results(compressed_data)
```

---

## 7. Code-Level Optimizations

### Algorithm Optimization

```python
# ============================================
# Example: Find duplicates
# ============================================

# ❌ O(n²) - Slow
def find_duplicates_slow(items):
    duplicates = []
    for i in range(len(items)):
        for j in range(i + 1, len(items)):
            if items[i] == items[j]:
                duplicates.append(items[i])
    return duplicates

# ✅ O(n) - Fast
def find_duplicates_fast(items):
    seen = set()
    duplicates = set()
    
    for item in items:
        if item in seen:
            duplicates.add(item)
        seen.add(item)
    
    return list(duplicates)

# Benchmark
items = list(range(1000)) + list(range(500))

import time

start = time.time()
find_duplicates_slow(items)
print(f"Slow: {(time.time() - start) * 1000:.2f}ms")  # ~200ms

start = time.time()
find_duplicates_fast(items)
print(f"Fast: {(time.time() - start) * 1000:.2f}ms")  # ~0.5ms

# 400x faster!
```

### Data Structure Choice

```javascript
// ============================================
// Array vs Set vs Map
// ============================================

// ❌ Array lookup - O(n)
const users = ['user1', 'user2', 'user3', ...];  // 10,000 users
const exists = users.includes('user5000');  // Scans array
// Time: ~5ms

// ✅ Set lookup - O(1)
const usersSet = new Set(['user1', 'user2', 'user3', ...]);
const exists = usersSet.has('user5000');  // Hash lookup
// Time: ~0.001ms

// ✅ Map for key-value - O(1)
const usersMap = new Map([
  ['user1', { name: 'John' }],
  ['user2', { name: 'Jane' }],
  // ...
]);
const user = usersMap.get('user5000');
// Time: ~0.001ms

// Performance comparison for 10,000 items
// Array indexOf: ~5ms
// Set has: ~0.001ms
// Map get: ~0.001ms
// Set/Map are 5000x faster!
```

### Loop Optimization

```python
# ============================================
# Loop Optimizations
# ============================================

# ❌ Slow - Function call in loop condition
items = range(1000000)
result = []
for i in range(len(items)):  # len() called every iteration
    result.append(items[i] * 2)

# ✅ Fast - Cache length
items = range(1000000)
result = []
length = len(items)  # Called once
for i in range(length):
    result.append(items[i] * 2)

# ✅ Fastest - List comprehension (C-optimized)
items = range(1000000)
result = [item * 2 for item in items]

# ✅ Even faster - Use built-in functions
import numpy as np
items = np.arange(1000000)
result = items * 2  # Vectorized operation

# Benchmark:
# Loop with len(): 250ms
# Loop cached: 180ms
# List comprehension: 120ms
# NumPy vectorized: 5ms
```

---

## 8. Memory Management

### Object Pooling

**Definition:** Reuse objects instead of creating new ones.

```java
// ============================================
// Object Pool Implementation
// ============================================
import java.util.concurrent.*;

public class ObjectPool<T> {
    private BlockingQueue<T> pool;
    private ObjectFactory<T> factory;
    private int maxSize;
    
    public ObjectPool(ObjectFactory<T> factory, int maxSize) {
        this.factory = factory;
        this.maxSize = maxSize;
        this.pool = new LinkedBlockingQueue<>(maxSize);
        
        // Pre-populate pool
        for (int i = 0; i < maxSize / 2; i++) {
            pool.offer(factory.create());
        }
    }
    
    public T acquire() throws InterruptedException {
        T object = pool.poll();
        
        if (object == null) {
            // Pool empty - create new object
            object = factory.create();
        }
        
        return object;
    }
    
    public void release(T object) {
        if (pool.size() < maxSize) {
            factory.reset(object);  // Clean before reuse
            pool.offer(object);
        }
        // else: discard (pool full)
    }
}

interface ObjectFactory<T> {
    T create();
    void reset(T object);
}

// Example: StringBuilder pool
class StringBuilderFactory implements ObjectFactory<StringBuilder> {
    public StringBuilder create() {
        return new StringBuilder(1024);
    }
    
    public void reset(StringBuilder sb) {
        sb.setLength(0);  // Clear content
    }
}

// Usage
ObjectPool<StringBuilder> pool = new ObjectPool<>(
    new StringBuilderFactory(),
    100  // Pool size
);

// Instead of: new StringBuilder() each time
StringBuilder sb = pool.acquire();
try {
    sb.append("Hello");
    sb.append(" World");
    String result = sb.toString();
} finally {
    pool.release(sb);  // Return to pool
}

// Benefit: Reduced GC pressure, faster allocation
```

### Memory Leak Prevention

```javascript
// ============================================
// Common Memory Leaks
// ============================================

// ❌ Leak: Global variables accumulate
let globalCache = {};  // Never cleared!

function cacheData(key, value) {
  globalCache[key] = value;  // Memory leak - grows forever
}

// ✅ Fix: Use LRU cache with size limit
const LRU = require('lru-cache');

const cache = new LRU({
  max: 500,  // Max 500 items
  maxAge: 1000 * 60 * 60  // 1 hour
});

function cacheData(key, value) {
  cache.set(key, value);  // Old entries automatically evicted
}

// ❌ Leak: Event listeners not removed
class DataFetcher {
  constructor() {
    this.eventEmitter = new EventEmitter();
    
    // Leak - listeners accumulate
    setInterval(() => {
      this.eventEmitter.on('data', this.handleData);
    }, 1000);
  }
  
  handleData(data) {
    console.log(data);
  }
}

// ✅ Fix: Remove listeners
class DataFetcher {
  constructor() {
    this.eventEmitter = new EventEmitter();
    this.interval = null;
  }
  
  start() {
    this.interval = setInterval(() => {
      this.eventEmitter.on('data', this.handleData);
    }, 1000);
  }
  
  stop() {
    clearInterval(this.interval);
    this.eventEmitter.removeAllListeners('data');
  }
}
```

---

## 9. Database Query Optimization

### Use EXPLAIN to Analyze Queries

```sql
-- ============================================
-- Analyze query performance
-- ============================================

-- Slow query
EXPLAIN ANALYZE
SELECT * FROM orders
WHERE user_id = 123
AND created_at > '2024-01-01';

-- Output:
-- Seq Scan on orders (cost=0.00..10000.00 rows=500 width=100)
--   Filter: (user_id = 123 AND created_at > '2024-01-01')
-- Planning Time: 0.5ms
-- Execution Time: 450.2ms  ← SLOW!

-- Add index
CREATE INDEX idx_orders_user_created ON orders(user_id, created_at);

-- Now much faster
EXPLAIN ANALYZE
SELECT * FROM orders
WHERE user_id = 123
AND created_at > '2024-01-01';

-- Output:
-- Index Scan using idx_orders_user_created (cost=0.42..8.44 rows=500 width=100)
--   Index Cond: (user_id = 123 AND created_at > '2024-01-01')
-- Planning Time: 0.2ms
-- Execution Time: 2.3ms  ← FAST! (200x improvement)
```

### Query Optimization Techniques

**1. Select Only Needed Columns**

```sql
-- ❌ Bad - retrieves all columns
SELECT * FROM users WHERE id = 123;

-- ✅ Good - only needed columns
SELECT id, name, email FROM users WHERE id = 123;

-- Benefit: Less data transferred, faster query
```

**2. Use Covering Index**

```sql
-- Query
SELECT name, email FROM users WHERE id = 123;

-- Covering index (includes all needed columns)
CREATE INDEX idx_users_id_name_email ON users(id, name, email);

-- Query can be satisfied entirely from index
-- No need to access table (index-only scan)
```

**3. Avoid SELECT DISTINCT When Possible**

```sql
-- ❌ Slow - requires sorting/hashing
SELECT DISTINCT category FROM products;

-- ✅ Faster - use GROUP BY with index
CREATE INDEX idx_products_category ON products(category);
SELECT category FROM products GROUP BY category;
```

**4. Batch Inserts**

```python
# ❌ Slow - 1000 individual inserts
for user in users:
    db.execute(
        "INSERT INTO users (name, email) VALUES (?, ?)",
        [user['name'], user['email']]
    )
# Time: ~2000ms

# ✅ Fast - Single batch insert
values = [(user['name'], user['email']) for user in users]
db.executemany(
    "INSERT INTO users (name, email) VALUES (?, ?)",
    values
)
# Time: ~50ms (40x faster!)
```

### Query Result Caching

```typescript
import Redis from 'ioredis';
import crypto from 'crypto';

class QueryCache {
  private redis: Redis;
  
  constructor() {
    this.redis = new Redis();
  }
  
  private getCacheKey(query: string, params: any[]): string {
    const data = JSON.stringify({ query, params });
    return crypto.createHash('md5').update(data).digest('hex');
  }
  
  async query(query: string, params: any[] = []): Promise<any> {
    // Generate cache key
    const cacheKey = this.getCacheKey(query, params);
    
    // Check cache
    const cached = await this.redis.get(cacheKey);
    if (cached) {
      console.log('Cache hit');
      return JSON.parse(cached);
    }
    
    console.log('Cache miss - querying database');
    
    // Execute query
    const result = await db.query(query, params);
    
    // Cache result (5 minutes)
    await this.redis.setex(cacheKey, 300, JSON.stringify(result.rows));
    
    return result.rows;
  }
  
  async invalidate(pattern: string): Promise<void> {
    /**
     * Invalidate cached queries matching pattern
     * Example: invalidate('SELECT * FROM users%')
     */
    const keys = await this.redis.keys(`*${pattern}*`);
    if (keys.length > 0) {
      await this.redis.del(...keys);
    }
  }
}

// Usage
const queryCache = new QueryCache();

// First call - queries database
const users1 = await queryCache.query(
  'SELECT * FROM users WHERE active = ?',
  [true]
);

// Second call - from cache (100x faster)
const users2 = await queryCache.query(
  'SELECT * FROM users WHERE active = ?',
  [true]
);
```

---

## 10. Caching Strategies

### Multi-Level Caching

```typescript
class MultiLevelCache {
  private l1: Map<string, any>;  // Memory (fastest)
  private l2: Redis;              // Redis (fast)
  private db: Database;           // Database (slow)
  
  constructor(redis: Redis, db: Database) {
    this.l1 = new Map();
    this.l2 = redis;
    this.db = db;
    
    // Clear L1 cache every 5 minutes
    setInterval(() => {
      this.l1.clear();
    }, 5 * 60 * 1000);
  }
  
  async get(key: string): Promise<any> {
    // L1: In-memory (< 1ms)
    if (this.l1.has(key)) {
      console.log('L1 cache hit');
      return this.l1.get(key);
    }
    
    // L2: Redis (~2ms)
    const l2Data = await this.l2.get(key);
    if (l2Data) {
      console.log('L2 cache hit');
      const data = JSON.parse(l2Data);
      this.l1.set(key, data);  // Populate L1
      return data;
    }
    
    // L3: Database (~50ms)
    console.log('Cache miss - querying database');
    const data = await this.db.query(key);
    
    // Populate both caches
    this.l1.set(key, data);
    await this.l2.setex(key, 3600, JSON.stringify(data));
    
    return data;
  }
  
  async set(key: string, value: any): Promise<void> {
    // Write to both caches
    this.l1.set(key, value);
    await this.l2.setex(key, 3600, JSON.stringify(value));
  }
  
  async invalidate(key: string): Promise<void> {
    // Invalidate all levels
    this.l1.delete(key);
    await this.l2.del(key);
  }
}

// Performance:
// L1 hit: 0.1ms
// L2 hit: 2ms
// DB query: 50ms
```

---

## 11. Real-World Performance Tuning

### Case Study: Optimizing API Response Time

```python
# ============================================
# Before Optimization
# ============================================
def get_user_dashboard(user_id):
    # 1. Get user (50ms)
    user = db.query("SELECT * FROM users WHERE id = ?", [user_id])
    
    # 2. Get orders - N+1 problem (100ms × 10 = 1000ms)
    orders = []
    order_ids = db.query("SELECT id FROM orders WHERE user_id = ?", [user_id])
    for order_id in order_ids:
        order = db.query("SELECT * FROM orders WHERE id = ?", [order_id])
        order['items'] = db.query("SELECT * FROM order_items WHERE order_id = ?", [order_id])
        orders.append(order)
    
    # 3. Get recommendations (200ms)
    recommendations = get_recommendations(user_id)
    
    # 4. Get notifications (150ms)
    notifications = db.query("SELECT * FROM notifications WHERE user_id = ?", [user_id])
    
    return {
        'user': user,
        'orders': orders,
        'recommendations': recommendations,
        'notifications': notifications
    }

# Total time: 50 + 1000 + 200 + 150 = 1400ms ❌

# ============================================
# After Optimization
# ============================================
import asyncio

async def get_user_dashboard_optimized(user_id):
    # Use async and parallel execution
    
    # 1. Parallel queries
    user_task = db.query_async("SELECT * FROM users WHERE id = ?", [user_id])
    
    # 2. Fix N+1 with JOIN (single query instead of N)
    orders_task = db.query_async("""
        SELECT 
            orders.*,
            json_agg(order_items.*) as items
        FROM orders
        LEFT JOIN order_items ON order_items.order_id = orders.id
        WHERE orders.user_id = ?
        GROUP BY orders.id
    """, [user_id])
    
    # 3. Cache recommendations (check cache first)
    async def get_cached_recommendations():
        cached = await cache.get(f'recommendations:{user_id}')
        if cached:
            return json.loads(cached)
        
        recs = await get_recommendations(user_id)
        await cache.setex(f'recommendations:{user_id}', 300, json.dumps(recs))
        return recs
    
    recommendations_task = get_cached_recommendations()
    
    # 4. Limit notifications (paginate)
    notifications_task = db.query_async(
        "SELECT * FROM notifications WHERE user_id = ? ORDER BY created_at DESC LIMIT 10",
        [user_id]
    )
    
    # Execute all in parallel
    user, orders, recommendations, notifications = await asyncio.gather(
        user_task,
        orders_task,
        recommendations_task,
        notifications_task
    )
    
    return {
        'user': user[0],
        'orders': orders,
        'recommendations': recommendations,
        'notifications': notifications
    }

# Total time: max(50, 100, 50, 30) = 100ms ✅
# Improvement: 14x faster! (1400ms → 100ms)
```

### Performance Checklist

```python
def optimize_endpoint():
    """Performance optimization checklist"""
    
    # ✅ 1. Add caching
    @cache.memoize(timeout=300)
    def expensive_operation():
        pass
    
    # ✅ 2. Use connection pooling
    db_pool = ConnectionPool(min=5, max=20)
    
    # ✅ 3. Fix N+1 queries
    # Use JOINs or eager loading
    
    # ✅ 4. Add database indexes
    # CREATE INDEX idx_users_email ON users(email)
    
    # ✅ 5. Compress responses
    # Enable gzip compression
    
    # ✅ 6. Use async/parallel processing
    results = await asyncio.gather(query1(), query2(), query3())
    
    # ✅ 7. Paginate large results
    # LIMIT and OFFSET
    
    # ✅ 8. Denormalize if needed
    # Store computed values
    
    # ✅ 9. Use CDN for static assets
    # Images, CSS, JS on CDN
    
    # ✅ 10. Monitor and profile
    # Track slow queries, bottlenecks
```

---

## Chapter 17 Summary

### Key Concepts

1. **Profiling** - Measure before optimizing
2. **N+1 Problem** - Solve with JOINs, eager loading, DataLoader
3. **Lazy vs Eager** - Trade-offs between loading strategies
4. **Async Processing** - Parallel execution, background tasks
5. **Compression** - Reduce data transfer
6. **Code Optimization** - Algorithm choice, data structures
7. **Memory Management** - Object pooling, leak prevention
8. **Query Optimization** - Indexes, EXPLAIN, batching
9. **Caching** - Multi-level caching

### Performance Wins

| Optimization | Typical Improvement |
|--------------|---------------------|
| **Add index** | 10-100x faster queries |
| **Fix N+1** | 10-50x faster |
| **Add caching** | 50-100x faster |
| **Async processing** | 2-5x faster |
| **Compression** | 5-10x less bandwidth |
| **Object pooling** | 2-3x less GC |
| **Batch operations** | 10-50x faster |

### Optimization Priority

1. **Database** - Usually biggest bottleneck (indexes, N+1)
2. **Caching** - Biggest bang for buck
3. **Async processing** - Easy win for parallel operations
4. **Code optimization** - Last resort (measure first!)

### Interview Tips

**Common Questions:**
1. "How do you identify performance bottlenecks?"
2. "Explain the N+1 query problem"
3. "How do you optimize a slow API endpoint?"
4. "Lazy loading vs eager loading?"

**How to Answer:**
- Always measure first
- Give specific examples with numbers
- Show before/after comparisons
- Mention tools (profilers, EXPLAIN)
- Discuss trade-offs

### Tools to Know

- **Profiling**: Chrome DevTools, New Relic, DataDog
- **Database**: EXPLAIN, slow query log, pg_stat_statements
- **Load Testing**: Apache JMeter, k6, Locust
- **Monitoring**: Prometheus, Grafana, CloudWatch

### Next Steps

Chapter 18 will cover **Content Delivery** - CDNs, edge computing, and optimizing content delivery globally.