# Chapter 30: Data Pipeline Architecture

## Table of Contents
1. Introduction to Data Pipelines
2. ETL (Extract, Transform, Load)
3. ELT (Extract, Load, Transform)
4. Batch Processing vs Stream Processing
5. Apache Airflow
6. Data Quality and Validation
7. Data Lineage
8. Error Handling and Retries
9. Monitoring Data Pipelines
10. Real-World Pipeline Examples
11. Best Practices

---

## 1. Introduction to Data Pipelines

**Definition:** A data pipeline is a series of data processing steps where data is ingested from sources, processed, and delivered to destinations.

### Pipeline Components

```
Sources → Ingestion → Processing → Storage → Consumption
          (Extract)   (Transform)   (Load)     (Analytics)

Sources:
- Databases
- APIs
- Files (CSV, JSON, Parquet)
- Event streams
- SaaS applications

Processing:
- Cleaning (remove nulls, fix formats)
- Enriching (add calculated fields)
- Aggregating (sum, count, average)
- Joining (combine multiple sources)

Destinations:
- Data warehouse (Redshift, Snowflake)
- Data lake (S3, GCS)
- Database (PostgreSQL, MongoDB)
- Analytics tools (Tableau, Looker)
```

---

## 2. ETL (Extract, Transform, Load)

**Process:** Extract from sources → Transform in pipeline → Load to destination

### Characteristics

```
Traditional ETL:
1. Extract: Pull data from sources
2. Transform: Process in ETL tool (Python, Spark)
3. Load: Insert into data warehouse

Pros:
- Clean data before loading
- Less load on warehouse
- Can use specialized tools

Cons:
- Transformation bottleneck
- Complex transformations slow
- Need powerful ETL servers
```

### Python ETL Example

```python
# ============================================
# ETL Pipeline with Pandas
# ============================================

import pandas as pd
import psycopg2
from sqlalchemy import create_engine
import requests

class ETLPipeline:
    def __init__(self, source_db_url, target_db_url):
        self.source_engine = create_engine(source_db_url)
        self.target_engine = create_engine(target_db_url)
    
    def extract(self):
        """Extract data from multiple sources"""
        print("Extracting data...")
        
        # Source 1: PostgreSQL database
        df_orders = pd.read_sql(
            "SELECT * FROM orders WHERE order_date >= CURRENT_DATE - INTERVAL '1 day'",
            self.source_engine
        )
        
        # Source 2: CSV file
        df_products = pd.read_csv('products.csv')
        
        # Source 3: REST API
        response = requests.get('https://api.example.com/customers')
        df_customers = pd.DataFrame(response.json())
        
        print(f"Extracted {len(df_orders)} orders, {len(df_products)} products, {len(df_customers)} customers")
        
        return df_orders, df_products, df_customers
    
    def transform(self, df_orders, df_products, df_customers):
        """Transform data"""
        print("Transforming data...")
        
        # 1. Clean data
        # Remove nulls
        df_orders = df_orders.dropna(subset=['customer_id', 'total'])
        
        # Fix data types
        df_orders['order_date'] = pd.to_datetime(df_orders['order_date'])
        df_orders['total'] = pd.to_numeric(df_orders['total'])
        
        # 2. Enrich data
        # Add derived columns
        df_orders['year'] = df_orders['order_date'].dt.year
        df_orders['month'] = df_orders['order_date'].dt.month
        df_orders['day_of_week'] = df_orders['order_date'].dt.dayofweek
        
        # 3. Join data
        # Add customer information to orders
        df_enriched = df_orders.merge(
            df_customers[['customer_id', 'customer_name', 'customer_segment']],
            on='customer_id',
            how='left'
        )
        
        # 4. Aggregate
        # Create summary table
        df_daily_summary = df_enriched.groupby(['year', 'month', 'day_of_week']).agg({
            'order_id': 'count',
            'total': 'sum',
            'customer_id': 'nunique'
        }).reset_index()
        
        df_daily_summary.columns = ['year', 'month', 'day_of_week', 
                                      'order_count', 'total_revenue', 'unique_customers']
        
        # 5. Filter
        # Remove test orders
        df_enriched = df_enriched[df_enriched['total'] > 0]
        
        print(f"Transformed to {len(df_enriched)} enriched records")
        
        return df_enriched, df_daily_summary
    
    def load(self, df_enriched, df_summary):
        """Load data to data warehouse"""
        print("Loading data...")
        
        # Load enriched orders
        df_enriched.to_sql(
            'fact_orders',
            self.target_engine,
            if_exists='append',
            index=False,
            method='multi',  # Batch insert
            chunksize=1000
        )
        
        # Load daily summary
        df_summary.to_sql(
            'daily_sales_summary',
            self.target_engine,
            if_exists='replace',  # Replace existing
            index=False
        )
        
        print("Data loaded successfully")
    
    def run(self):
        """Execute full ETL pipeline"""
        try:
            # Extract
            df_orders, df_products, df_customers = self.extract()
            
            # Transform
            df_enriched, df_summary = self.transform(
                df_orders, df_products, df_customers
            )
            
            # Load
            self.load(df_enriched, df_summary)
            
            print("ETL pipeline completed successfully")
            
        except Exception as e:
            print(f"ETL pipeline failed: {e}")
            raise

# Usage
pipeline = ETLPipeline(
    source_db_url='postgresql://source-db:5432/production',
    target_db_url='postgresql://warehouse:5432/analytics'
)

pipeline.run()
```

---

## 3. ELT (Extract, Load, Transform)

**Process:** Extract from sources → Load raw to warehouse → Transform in warehouse

### Characteristics

```
Modern ELT:
1. Extract: Pull data from sources
2. Load: Load raw data to warehouse
3. Transform: Use warehouse's SQL engine

Pros:
- Leverage warehouse compute power
- Raw data available for ad-hoc analysis
- Easier to reprocess (data in warehouse)
- Simpler pipeline (less ETL logic)

Cons:
- Warehouse stores raw data (more storage)
- Complex transformations in SQL
- Warehouse must be powerful
```

### ELT with dbt (Data Build Tool)

```sql
-- ============================================
-- dbt Models (SQL transformations in warehouse)
-- ============================================

-- models/staging/stg_orders.sql
-- Stage 1: Clean raw data
{{
  config(
    materialized='view'
  )
}}

SELECT
    order_id,
    customer_id,
    CAST(order_date AS DATE) as order_date,
    CAST(total AS DECIMAL(10,2)) as total,
    LOWER(TRIM(status)) as status
FROM {{ source('raw', 'orders') }}
WHERE total > 0  -- Filter out test orders

-- models/staging/stg_customers.sql
{{
  config(
    materialized='view'
  )
}}

SELECT
    customer_id,
    TRIM(CONCAT(first_name, ' ', last_name)) as customer_name,
    LOWER(TRIM(email)) as email,
    customer_segment
FROM {{ source('raw', 'customers') }}

-- models/marts/fct_orders.sql
-- Stage 2: Create fact table
{{
  config(
    materialized='table',
    indexes=[
      {'columns': ['customer_id']},
      {'columns': ['order_date']}
    ]
  )
}}

SELECT
    o.order_id,
    o.customer_id,
    c.customer_name,
    c.customer_segment,
    o.order_date,
    EXTRACT(YEAR FROM o.order_date) as year,
    EXTRACT(MONTH FROM o.order_date) as month,
    EXTRACT(DOW FROM o.order_date) as day_of_week,
    o.total,
    o.status
FROM {{ ref('stg_orders') }} o
LEFT JOIN {{ ref('stg_customers') }} c
    ON o.customer_id = c.customer_id

-- models/marts/daily_sales_summary.sql
-- Stage 3: Create aggregate
{{
  config(
    materialized='table'
  )
}}

SELECT
    order_date,
    customer_segment,
    COUNT(*) as order_count,
    SUM(total) as total_revenue,
    AVG(total) as avg_order_value,
    COUNT(DISTINCT customer_id) as unique_customers
FROM {{ ref('fct_orders') }}
GROUP BY order_date, customer_segment
```

```bash
# Run dbt transformations
dbt run

# dbt automatically:
# - Executes models in dependency order
# - Creates tables/views in warehouse
# - Handles incremental updates
# - Tests data quality

# Result:
# Raw data → Staging views → Fact tables → Aggregates
# All transformations in SQL (warehouse does the work)
```

---

## 4. Batch Processing vs Stream Processing

### Batch Processing

**Definition:** Process large volumes of data at scheduled intervals.

```
Characteristics:
- Scheduled (hourly, daily, weekly)
- Large datasets (GB to TB)
- High latency (minutes to hours)
- High throughput

Use cases:
- Daily reports
- Monthly aggregations
- Historical analysis
- ETL jobs
```

**Apache Spark Batch Example:**

```python
# ============================================
# PySpark Batch Processing
# ============================================

from pyspark.sql import SparkSession
from pyspark.sql.functions import *

# Initialize Spark
spark = SparkSession.builder \
    .appName("Daily Sales Report") \
    .config("spark.sql.warehouse.dir", "/user/hive/warehouse") \
    .enableHiveSupport() \
    .getOrCreate()

# Read data from multiple sources
df_orders = spark.read \
    .format("jdbc") \
    .option("url", "jdbc:postgresql://db:5432/production") \
    .option("dbtable", "orders") \
    .option("user", "spark") \
    .option("password", "secret") \
    .load()

df_products = spark.read.parquet("s3://data-lake/products/")
df_customers = spark.read.json("s3://data-lake/customers/")

# Transform
df_enriched = df_orders \
    .join(df_customers, "customer_id") \
    .join(df_products, "product_id") \
    .filter(col("order_date") == current_date()) \
    .withColumn("revenue", col("quantity") * col("price")) \
    .withColumn("month", month(col("order_date"))) \
    .withColumn("year", year(col("order_date")))

# Aggregate
df_summary = df_enriched \
    .groupBy("year", "month", "product_category") \
    .agg(
        sum("revenue").alias("total_revenue"),
        count("order_id").alias("order_count"),
        avg("revenue").alias("avg_order_value")
    )

# Write results
df_summary.write \
    .mode("overwrite") \
    .partitionBy("year", "month") \
    .parquet("s3://data-warehouse/sales_summary/")

# Stop Spark
spark.stop()

# Run daily via scheduler
# cron: 0 2 * * * python batch_job.py
```

### Stream Processing

**Definition:** Process data in real-time as it arrives.

```
Characteristics:
- Continuous processing
- Small batches (micro-batches)
- Low latency (milliseconds to seconds)
- Lower throughput per event

Use cases:
- Real-time dashboards
- Fraud detection
- Alerting
- Live recommendations
```

**Kafka Streams Example:**

```java
// ============================================
// Kafka Streams Real-Time Processing
// ============================================

import org.apache.kafka.streams.*;
import org.apache.kafka.streams.kstream.*;
import java.time.Duration;

public class RealTimeAnalytics {
    public static void main(String[] args) {
        Properties props = new Properties();
        props.put(StreamsConfig.APPLICATION_ID_CONFIG, "realtime-analytics");
        props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        
        StreamsBuilder builder = new StreamsBuilder();
        
        // Input stream: Click events
        KStream<String, ClickEvent> clicks = builder.stream("clicks");
        
        // Transform: Parse and enrich
        KStream<String, EnrichedClick> enrichedClicks = clicks
            .mapValues(click -> {
                // Parse event
                EnrichedClick enriched = new EnrichedClick(click);
                
                // Enrich with user data (from state store or external call)
                enriched.setUserSegment(getUserSegment(click.getUserId()));
                
                return enriched;
            });
        
        // Aggregate: Count clicks per page (1-minute windows)
        KTable<Windowed<String>, Long> clicksPerPage = enrichedClicks
            .groupBy((key, value) -> value.getPage())
            .windowedBy(TimeWindows.of(Duration.ofMinutes(1)))
            .count();
        
        // Convert to stream and send to output
        clicksPerPage
            .toStream()
            .map((key, count) -> new KeyValue<>(
                key.key(),  // Page URL
                new PageStats(key.key(), count, key.window().start())
            ))
            .to("page-stats");
        
        // Filter: Detect high-value users
        enrichedClicks
            .filter((key, click) -> click.getUserSegment().equals("premium"))
            .to("premium-user-clicks");
        
        // Alert: Detect anomalies
        enrichedClicks
            .groupBy((key, value) -> value.getUserId())
            .windowedBy(TimeWindows.of(Duration.ofMinutes(5)))
            .count()
            .filter((key, count) -> count > 100)  // > 100 clicks in 5 min
            .toStream()
            .to("anomaly-alerts");
        
        // Build and start
        KafkaStreams streams = new KafkaStreams(builder.build(), props);
        streams.start();
        
        // Shutdown hook
        Runtime.getRuntime().addShutdownHook(new Thread(streams::close));
    }
}

// Processing happens in real-time:
// Click → Parse → Enrich → Aggregate → Output
// Latency: < 1 second
```

---

## 5. Apache Airflow

**Definition:** Platform to programmatically author, schedule, and monitor workflows.

### DAG (Directed Acyclic Graph)

```python
# ============================================
# Airflow DAG Definition
# ============================================

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.providers.postgres.operators.postgres import PostgresOperator
from datetime import datetime, timedelta

# Default arguments
default_args = {
    'owner': 'data-team',
    'depends_on_past': False,
    'email': ['alerts@example.com'],
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 3,
    'retry_delay': timedelta(minutes=5)
}

# Define DAG
dag = DAG(
    'daily_sales_etl',
    default_args=default_args,
    description='Daily sales ETL pipeline',
    schedule_interval='0 2 * * *',  # Run daily at 2 AM
    start_date=datetime(2024, 1, 1),
    catchup=False,
    tags=['etl', 'sales']
)

# Task 1: Extract orders
def extract_orders(**context):
    """Extract orders from source database"""
    import psycopg2
    import pandas as pd
    
    conn = psycopg2.connect("postgresql://source:5432/production")
    
    df = pd.read_sql("""
        SELECT * FROM orders 
        WHERE order_date = CURRENT_DATE - INTERVAL '1 day'
    """, conn)
    
    # Save to XCom for next task
    context['ti'].xcom_push(key='orders_df', value=df.to_json())
    
    print(f"Extracted {len(df)} orders")
    
    conn.close()

extract_task = PythonOperator(
    task_id='extract_orders',
    python_callable=extract_orders,
    dag=dag
)

# Task 2: Extract customers
def extract_customers(**context):
    """Extract customers from API"""
    import requests
    
    response = requests.get('https://api.example.com/customers')
    customers = response.json()
    
    context['ti'].xcom_push(key='customers', value=customers)
    
    print(f"Extracted {len(customers)} customers")

extract_customers_task = PythonOperator(
    task_id='extract_customers',
    python_callable=extract_customers,
    dag=dag
)

# Task 3: Transform data
def transform_data(**context):
    """Transform and enrich data"""
    import pandas as pd
    import json
    
    # Get data from previous tasks (XCom)
    orders_json = context['ti'].xcom_pull(key='orders_df', task_ids='extract_orders')
    customers = context['ti'].xcom_pull(key='customers', task_ids='extract_customers')
    
    df_orders = pd.read_json(orders_json)
    df_customers = pd.DataFrame(customers)
    
    # Join and transform
    df_enriched = df_orders.merge(df_customers, on='customer_id', how='left')
    
    # Add calculated fields
    df_enriched['revenue'] = df_enriched['quantity'] * df_enriched['price']
    
    # Save to XCom
    context['ti'].xcom_push(key='transformed_data', value=df_enriched.to_json())
    
    print(f"Transformed {len(df_enriched)} records")

transform_task = PythonOperator(
    task_id='transform_data',
    python_callable=transform_data,
    dag=dag
)

# Task 4: Load to warehouse
load_task = PostgresOperator(
    task_id='load_to_warehouse',
    postgres_conn_id='warehouse_db',
    sql="""
        INSERT INTO fact_sales (order_id, customer_id, revenue, order_date)
        SELECT order_id, customer_id, revenue, order_date
        FROM staging.temp_sales
    """,
    dag=dag
)

# Task 5: Data quality check
def validate_data(**context):
    """Validate loaded data"""
    import psycopg2
    
    conn = psycopg2.connect("postgresql://warehouse:5432/analytics")
    cursor = conn.cursor()
    
    # Check 1: Row count
    cursor.execute("SELECT COUNT(*) FROM fact_sales WHERE order_date = CURRENT_DATE - INTERVAL '1 day'")
    count = cursor.fetchone()[0]
    
    if count == 0:
        raise ValueError("No data loaded for yesterday!")
    
    # Check 2: No nulls in critical columns
    cursor.execute("SELECT COUNT(*) FROM fact_sales WHERE customer_id IS NULL AND order_date = CURRENT_DATE - INTERVAL '1 day'")
    null_count = cursor.fetchone()[0]
    
    if null_count > 0:
        raise ValueError(f"Found {null_count} records with null customer_id!")
    
    # Check 3: Revenue is positive
    cursor.execute("SELECT COUNT(*) FROM fact_sales WHERE revenue < 0 AND order_date = CURRENT_DATE - INTERVAL '1 day'")
    negative_count = cursor.fetchone()[0]
    
    if negative_count > 0:
        raise ValueError(f"Found {negative_count} records with negative revenue!")
    
    print(f"✓ Data quality checks passed ({count} records)")
    
    conn.close()

validate_task = PythonOperator(
    task_id='validate_data',
    python_callable=validate_data,
    dag=dag
)

# Task 6: Send success notification
notify_task = BashOperator(
    task_id='notify_success',
    bash_command='echo "ETL completed successfully" | mail -s "Daily ETL Success" team@example.com',
    dag=dag
)

# ============================================
# Define task dependencies
# ============================================

# Parallel extraction
[extract_task, extract_customers_task] >> transform_task >> load_task >> validate_task >> notify_task

# Visualization in Airflow UI:
# extract_orders ─┐
#                 ├─→ transform_data → load_to_warehouse → validate_data → notify_success
# extract_customers ─┘
```

---

## 6. Data Quality and Validation

### Data Quality Framework

```python
# ============================================
# Data Quality Checks
# ============================================

class DataQualityValidator:
    def __init__(self, df):
        self.df = df
        self.issues = []
    
    def check_completeness(self, required_columns):
        """Check for missing values in required columns"""
        for col in required_columns:
            null_count = self.df[col].isnull().sum()
            null_percent = (null_count / len(self.df)) * 100
            
            if null_percent > 0:
                self.issues.append({
                    'check': 'completeness',
                    'column': col,
                    'issue': f'{null_percent:.2f}% null values',
                    'severity': 'high' if null_percent > 5 else 'medium'
                })
    
    def check_uniqueness(self, unique_columns):
        """Check for duplicates in columns that should be unique"""
        for col in unique_columns:
            duplicate_count = self.df[col].duplicated().sum()
            
            if duplicate_count > 0:
                self.issues.append({
                    'check': 'uniqueness',
                    'column': col,
                    'issue': f'{duplicate_count} duplicate values',
                    'severity': 'high'
                })
    
    def check_validity(self, validations):
        """Check business rules"""
        for validation in validations:
            col = validation['column']
            condition = validation['condition']
            
            invalid_count = (~self.df[col].apply(condition)).sum()
            
            if invalid_count > 0:
                self.issues.append({
                    'check': 'validity',
                    'column': col,
                    'issue': f'{invalid_count} invalid values',
                    'severity': 'high'
                })
    
    def check_consistency(self, checks):
        """Check cross-column consistency"""
        for check in checks:
            violations = self.df.query(check['condition'])
            
            if len(violations) > 0:
                self.issues.append({
                    'check': 'consistency',
                    'issue': check['description'],
                    'count': len(violations),
                    'severity': 'high'
                })
    
    def validate(self):
        """Run all validation checks"""
        # Completeness
        self.check_completeness(['order_id', 'customer_id', 'total'])
        
        # Uniqueness
        self.check_uniqueness(['order_id'])
        
        # Validity
        self.check_validity([
            {
                'column': 'total',
                'condition': lambda x: x > 0
            },
            {
                'column': 'email',
                'condition': lambda x: '@' in str(x)
            }
        ])
        
        # Consistency
        self.check_consistency([
            {
                'condition': 'order_date > shipped_date',
                'description': 'Order date after ship date'
            }
        ])
        
        # Report issues
        if self.issues:
            print(f"Found {len(self.issues)} data quality issues:")
            for issue in self.issues:
                print(f"  [{issue['severity'].upper()}] {issue['issue']}")
            
            # Fail pipeline if critical issues
            critical_issues = [i for i in self.issues if i['severity'] == 'high']
            if critical_issues:
                raise ValueError(f"Pipeline failed: {len(critical_issues)} critical issues")
        else:
            print("✓ All data quality checks passed")

# Usage in pipeline
validator = DataQualityValidator(df_orders)
validator.validate()
```

---

## 7. Data Lineage

**Definition:** Track data from source to destination, showing transformations.

### Lineage Tracking

```python
# ============================================
# Data Lineage Tracker
# ============================================

class DataLineage:
    def __init__(self):
        self.lineage_graph = {}
    
    def track_source(self, dataset_name, source):
        """Track data source"""
        if dataset_name not in self.lineage_graph:
            self.lineage_graph[dataset_name] = {
                'sources': [],
                'transformations': [],
                'consumers': []
            }
        
        self.lineage_graph[dataset_name]['sources'].append({
            'type': source['type'],
            'location': source['location'],
            'timestamp': datetime.now().isoformat()
        })
    
    def track_transformation(self, input_dataset, output_dataset, transformation):
        """Track transformation"""
        self.lineage_graph[output_dataset] = {
            'sources': [input_dataset],
            'transformations': [transformation],
            'consumers': []
        }
        
        self.lineage_graph[input_dataset]['consumers'].append(output_dataset)
    
    def get_lineage(self, dataset_name):
        """Get full lineage for dataset"""
        def traverse(dataset, visited=None):
            if visited is None:
                visited = set()
            
            if dataset in visited:
                return {}
            
            visited.add(dataset)
            
            info = self.lineage_graph.get(dataset, {})
            
            lineage = {
                'dataset': dataset,
                'sources': info.get('sources', []),
                'transformations': info.get('transformations', []),
                'upstream': []
            }
            
            # Recursively get upstream lineage
            for source in info.get('sources', []):
                if isinstance(source, str):
                    lineage['upstream'].append(traverse(source, visited))
            
            return lineage
        
        return traverse(dataset_name)

# Usage
lineage = DataLineage()

# Track pipeline
lineage.track_source('raw_orders', {
    'type': 'postgresql',
    'location': 'postgresql://prod-db:5432/orders'
})

lineage.track_transformation(
    'raw_orders',
    'cleaned_orders',
    {
        'type': 'pandas',
        'description': 'Remove nulls, fix dtypes',
        'code': 'df.dropna().astype({...})'
    }
)

lineage.track_transformation(
    'cleaned_orders',
    'enriched_orders',
    {
        'type': 'join',
        'description': 'Join with customer data',
        'code': 'df.merge(customers, on="customer_id")'
    }
)

# Get lineage
order_lineage = lineage.get_lineage('enriched_orders')
print(json.dumps(order_lineage, indent=2))

# Output:
# {
#   "dataset": "enriched_orders",
#   "transformations": [{
#     "type": "join",
#     "description": "Join with customer data"
#   }],
#   "upstream": [{
#     "dataset": "cleaned_orders",
#     "transformations": [{
#       "type": "pandas",
#       "description": "Remove nulls, fix dtypes"
#     }],
#     "upstream": [{
#       "dataset": "raw_orders",
#       "sources": [{
#         "type": "postgresql",
#         "location": "postgresql://prod-db:5432/orders"
#       }]
#     }]
#   }]
# }
```

---

## 8. Error Handling and Retries

### Pipeline Error Handling

```python
# ============================================
# Robust Pipeline with Error Handling
# ============================================

class ResilientPipeline:
    def __init__(self):
        self.max_retries = 3
        self.dead_letter_queue = []
    
    def process_batch(self, records):
        """Process batch with error handling"""
        successful = []
        failed = []
        
        for record in records:
            try:
                processed = self.process_record(record)
                successful.append(processed)
                
            except Exception as e:
                print(f"Error processing record {record['id']}: {e}")
                failed.append({
                    'record': record,
                    'error': str(e),
                    'timestamp': datetime.now()
                })
        
        # Retry failed records
        if failed:
            retried = self.retry_failed(failed)
            successful.extend(retried['successful'])
            
            # Move permanently failed to dead letter queue
            self.dead_letter_queue.extend(retried['failed'])
        
        return {
            'successful': len(successful),
            'failed': len(self.dead_letter_queue),
            'total': len(records)
        }
    
    def retry_failed(self, failed_records):
        """Retry failed records with exponential backoff"""
        successful = []
        still_failed = []
        
        for failed in failed_records:
            record = failed['record']
            retries = 0
            
            while retries < self.max_retries:
                try:
                    processed = self.process_record(record)
                    successful.append(processed)
                    break
                    
                except Exception as e:
                    retries += 1
                    
                    if retries < self.max_retries:
                        delay = 2 ** retries  # 2s, 4s, 8s
                        time.sleep(delay)
                    else:
                        # Max retries exceeded
                        still_failed.append(failed)
        
        return {
            'successful': successful,
            'failed': still_failed
        }
    
    def process_record(self, record):
        """Process single record"""
        # Transformation logic
        # Raises exception on failure
        return transformed_record

# Usage
pipeline = ResilientPipeline()

result = pipeline.process_batch(records)
print(f"Processed {result['successful']}/{result['total']} records")

if result['failed'] > 0:
    print(f"⚠️  {result['failed']} records in dead letter queue")
    # Alert team for manual review
```

---

## 9. Monitoring Data Pipelines

### Pipeline Monitoring

```python
# ============================================
# Pipeline Metrics and Alerting
# ============================================

from prometheus_client import Counter, Histogram, Gauge
import time

class PipelineMonitor:
    def __init__(self):
        # Metrics
        self.records_processed = Counter(
            'pipeline_records_processed_total',
            'Total records processed',
            ['pipeline', 'status']
        )
        
        self.processing_duration = Histogram(
            'pipeline_processing_duration_seconds',
            'Pipeline processing duration',
            ['pipeline']
        )
        
        self.records_in_queue = Gauge(
            'pipeline_records_in_queue',
            'Records waiting to be processed',
            ['pipeline']
        )
        
        self.data_freshness = Gauge(
            'pipeline_data_freshness_seconds',
            'Age of most recent data',
            ['pipeline']
        )
    
    def track_pipeline_run(self, pipeline_name, func):
        """Decorator to track pipeline execution"""
        def wrapper(*args, **kwargs):
            start_time = time.time()
            
            try:
                result = func(*args, **kwargs)
                
                # Track success
                self.records_processed.labels(
                    pipeline=pipeline_name,
                    status='success'
                ).inc(result.get('count', 0))
                
                duration = time.time() - start_time
                self.processing_duration.labels(
                    pipeline=pipeline_name
                ).observe(duration)
                
                return result
                
            except Exception as e:
                # Track failure
                self.records_processed.labels(
                    pipeline=pipeline_name,
                    status='failure'
                ).inc()
                
                raise
        
        return wrapper

# Alerting rules (Prometheus)
"""
# Alert if pipeline hasn't run in 25 hours (should run daily)
- alert: PipelineStale
  expr: |
    time() - pipeline_last_run_timestamp > 25 * 3600
  for: 5m
  labels:
    severity: critical
  annotations:
    summary: "Pipeline {{ $labels.pipeline }} hasn't run in 25 hours"

# Alert if error rate > 1%
- alert: HighPipelineErrorRate
  expr: |
    rate(pipeline_records_processed_total{status="failure"}[5m]) /
    rate(pipeline_records_processed_total[5m]) > 0.01
  for: 10m
  labels:
    severity: warning
  annotations:
    summary: "High error rate in pipeline {{ $labels.pipeline }}"

# Alert if data is stale (> 2 hours old)
- alert: StaleData
  expr: |
    pipeline_data_freshness_seconds > 7200
  for: 10m
  labels:
    severity: warning
  annotations:
    summary: "Data in {{ $labels.pipeline }} is stale"
"""
```

---

## 10. Real-World Pipeline Examples

### Example 1: E-Commerce Analytics Pipeline

```python
# ============================================
# Complete E-Commerce Data Pipeline
# ============================================

from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta

# DAG for e-commerce analytics
with DAG(
    'ecommerce_analytics',
    schedule_interval='0 * * * *',  # Hourly
    start_date=datetime(2024, 1, 1),
    catchup=False
) as dag:
    
    def extract_orders(**context):
        """Extract orders from production DB"""
        # Read orders from last hour
        df = pd.read_sql("""
            SELECT * FROM orders
            WHERE created_at >= NOW() - INTERVAL '1 hour'
        """, source_db)
        
        context['ti'].xcom_push(key='orders', value=df.to_json())
    
    def extract_events(**context):
        """Extract clickstream from S3"""
        # Read Parquet files from S3
        df = pd.read_parquet(
            f's3://clickstream/year={datetime.now().year}/'
            f'month={datetime.now().month}/'
            f'day={datetime.now().day}/'
            f'hour={datetime.now().hour}/'
        )
        
        context['ti'].xcom_push(key='events', value=df.to_json())
    
    def join_and_analyze(**context):
        """Join orders with clickstream"""
        orders_json = context['ti'].xcom_pull(key='orders', task_ids='extract_orders')
        events_json = context['ti'].xcom_pull(key='events', task_ids='extract_events')
        
        df_orders = pd.read_json(orders_json)
        df_events = pd.read_json(events_json)
        
        # Join on session_id
        df_joined = df_events.merge(
            df_orders,
            on='session_id',
            how='left'
        )
        
        # Calculate conversion metrics
        total_sessions = df_events['session_id'].nunique()
        converted_sessions = df_joined['order_id'].notna().sum()
        conversion_rate = (converted_sessions / total_sessions) * 100
        
        # Calculate time to purchase
        df_joined['time_to_purchase'] = (
            df_joined['order_time'] - df_joined['first_visit_time']
        ).dt.total_seconds() / 60  # minutes
        
        avg_time = df_joined['time_to_purchase'].mean()
        
        # Store metrics
        context['ti'].xcom_push(key='metrics', value={
            'total_sessions': total_sessions,
            'conversions': converted_sessions,
            'conversion_rate': conversion_rate,
            'avg_time_to_purchase': avg_time
        })
    
    def load_to_warehouse(**context):
        """Load results to warehouse"""
        metrics = context['ti'].xcom_pull(key='metrics', task_ids='join_and_analyze')
        
        # Insert into metrics table
        warehouse_db.execute("""
            INSERT INTO hourly_metrics 
            (hour, total_sessions, conversions, conversion_rate, avg_time_to_purchase)
            VALUES (%(hour)s, %(total_sessions)s, %(conversions)s, %(conversion_rate)s, %(avg_time)s)
        """, {
            'hour': datetime.now().replace(minute=0, second=0),
            'total_sessions': metrics['total_sessions'],
            'conversions': metrics['conversions'],
            'conversion_rate': metrics['conversion_rate'],
            'avg_time': metrics['avg_time_to_purchase']
        })
    
    def send_alert(**context):
        """Alert if conversion rate drops"""
        metrics = context['ti'].xcom_pull(key='metrics', task_ids='join_and_analyze')
        
        if metrics['conversion_rate'] < 2.0:  # Threshold
            send_slack_alert(
                f"⚠️  Conversion rate dropped to {metrics['conversion_rate']:.2f}%"
            )
    
    # Define tasks
    extract_orders_task = PythonOperator(
        task_id='extract_orders',
        python_callable=extract_orders
    )
    
    extract_events_task = PythonOperator(
        task_id='extract_events',
        python_callable=extract_events
    )
    
    analyze_task = PythonOperator(
        task_id='join_and_analyze',
        python_callable=join_and_analyze
    )
    
    load_task = PythonOperator(
        task_id='load_to_warehouse',
        python_callable=load_to_warehouse
    )
    
    alert_task = PythonOperator(
        task_id='send_alert',
        python_callable=send_alert,
        trigger_rule='all_done'
    )
    
    # Dependencies
    [extract_orders_task, extract_events_task] >> analyze_task >> load_task >> alert_task
```

---

## Chapter 30 Summary

### Key Concepts

1. **ETL** - Extract, Transform, Load (traditional)
2. **ELT** - Extract, Load, Transform (modern)
3. **Batch Processing** - Scheduled, large volumes
4. **Stream Processing** - Real-time, continuous
5. **Orchestration** - Apache Airflow, schedulers
6. **Data Quality** - Validation and checks
7. **Data Lineage** - Track data flow
8. **Error Handling** - Retries, dead letter queue
9. **Monitoring** - Metrics, alerting

### ETL vs ELT

| Aspect | ETL | ELT |
|--------|-----|-----|
| **Transform Location** | External tool | In warehouse |
| **Warehouse Load** | Clean data only | Raw + transformed |
| **Flexibility** | Less (pre-defined) | More (SQL transforms) |
| **Storage Cost** | Lower | Higher |
| **Processing Power** | Need ETL servers | Use warehouse compute |
| **Best For** | Legacy systems | Cloud data warehouses |

### Batch vs Stream

| Aspect | Batch | Stream |
|--------|-------|--------|
| **Latency** | Minutes to hours | Milliseconds to seconds |
| **Data Volume** | Large (GB to TB) | Small (KB to MB) |
| **Use Case** | Reports, analysis | Real-time alerts |
| **Complexity** | Lower | Higher |
| **Cost** | Lower (scheduled) | Higher (always on) |
| **Tools** | Spark, dbt | Kafka Streams, Flink |

### Pipeline Best Practices

1. **Idempotent operations** - Safe to rerun
2. **Incremental processing** - Process only new data
3. **Checkpointing** - Resume from failure
4. **Data quality checks** - Validate at each stage
5. **Monitoring** - Track metrics and failures
6. **Retry logic** - Handle transient failures
7. **Dead letter queue** - Store failed records
8. **Documentation** - Track lineage and transformations

### Interview Tips

**Common Questions:**
1. "Design a data pipeline for [use case]"
2. "ETL vs ELT - when to use each?"
3. "Batch vs stream processing?"
4. "How do you ensure data quality?"

**How to Answer:**
- Draw pipeline flow diagram
- Explain each stage (extract, transform, load)
- Discuss error handling
- Mention tools (Airflow, Spark, Kafka)
- Talk about monitoring and alerts

### Next Steps

Chapter 31 will cover **Big Data Architecture** - Lambda and Kappa architectures, handling massive datasets, and building systems that process petabytes of data.