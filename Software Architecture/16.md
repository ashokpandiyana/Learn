# Chapter 16: Database Scaling

## Table of Contents
1. Introduction to Database Scaling
2. Read Replicas
3. Connection Pooling
4. Database Sharding Strategies
5. Consistent Hashing
6. Partitioning vs Sharding
7. Handling Hot Partitions
8. Cross-Shard Queries
9. Real-World Sharding Examples
10. Database Scaling Patterns

---

## 1. Introduction to Database Scaling

**The Problem:** Database is often the bottleneck in scaling applications.

### Common Bottlenecks

```
1. Too many connections
   - 1000 app servers × 10 connections each = 10,000 connections
   - Database max: 100 connections
   - Result: Connection exhaustion

2. Too many reads
   - 10,000 SELECT queries/second
   - Single database can't handle load

3. Too many writes
   - 5,000 INSERT/UPDATE queries/second
   - Write throughput maxed out

4. Data volume
   - 10 TB data
   - Queries become slow
   - Indexes don't fit in memory
```

### Scaling Strategies

```
Problem → Solution

Too many connections → Connection pooling
Too many reads → Read replicas
Too many writes → Sharding
Data too large → Partitioning / Sharding
```

---

## 2. Read Replicas

**Definition:** Create read-only copies of database to distribute read load.

### Architecture

```
Application Servers (100)
        ↓
    ┌───┴───────────┬────────────┐
    ↓ Writes        ↓ Reads      ↓ Reads
┌────────┐      ┌────────┐   ┌────────┐
│ Master │─────→│Replica1│   │Replica2│
└────────┘      └────────┘   └────────┘
    ↑               ↑            ↑
    │               │            │
  1 write      50 reads      50 reads
```

### Implementation

```python
# ============================================
# PostgreSQL with Read Replicas
# ============================================
import psycopg2
from psycopg2 import pool
import random

class DatabaseManager:
    def __init__(self):
        # Master database (writes)
        self.master = pool.ThreadedConnectionPool(
            minconn=5,
            maxconn=20,
            host='master.db.example.com',
            port=5432,
            database='myapp',
            user='app',
            password='secret'
        )
        
        # Read replicas
        self.replicas = [
            pool.ThreadedConnectionPool(
                minconn=10,
                maxconn=50,
                host='replica1.db.example.com',
                port=5432,
                database='myapp',
                user='app_read',
                password='secret'
            ),
            pool.ThreadedConnectionPool(
                minconn=10,
                maxconn=50,
                host='replica2.db.example.com',
                port=5432,
                database='myapp',
                user='app_read',
                password='secret'
            ),
            pool.ThreadedConnectionPool(
                minconn=10,
                maxconn=50,
                host='replica3.db.example.com',
                port=5432,
                database='myapp',
                user='app_read',
                password='secret'
            )
        ]
    
    def get_master_connection(self):
        """Get connection to master"""
        return self.master.getconn()
    
    def get_replica_connection(self):
        """Get connection to random replica (load balancing)"""
        replica = random.choice(self.replicas)
        return replica.getconn()
    
    def execute_write(self, query, params=()):
        """Execute write query on master"""
        conn = self.get_master_connection()
        try:
            cursor = conn.cursor()
            cursor.execute(query, params)
            conn.commit()
            result = cursor.fetchall() if cursor.description else None
            return result
        finally:
            self.master.putconn(conn)
    
    def execute_read(self, query, params=(), use_master=False):
        """Execute read query on replica (or master if specified)"""
        if use_master:
            conn = self.get_master_connection()
            pool_to_return = self.master
        else:
            conn = self.get_replica_connection()
            pool_to_return = random.choice(self.replicas)
        
        try:
            cursor = conn.cursor()
            cursor.execute(query, params)
            result = cursor.fetchall()
            return result
        finally:
            pool_to_return.putconn(conn)

# Usage
db = DatabaseManager()

# Writes go to master
db.execute_write(
    'INSERT INTO users (name, email) VALUES (%s, %s)',
    ('John Doe', 'john@example.com')
)

# Reads from replicas (load distributed)
users = db.execute_read('SELECT * FROM users WHERE active = TRUE')

# Critical read from master (avoid replication lag)
user = db.execute_read(
    'SELECT * FROM users WHERE id = %s',
    (123,),
    use_master=True  # Read after write consistency
)
```

### Handling Replication Lag

```python
import time

class ReplicationLagHandler:
    def __init__(self, db_manager):
        self.db = db_manager
        self.recent_writes = {}  # Track recent writes
    
    def write_user(self, user_id, name, email):
        """Write to master and track"""
        self.db.execute_write(
            'UPDATE users SET name = %s, email = %s WHERE id = %s',
            (name, email, user_id)
        )
        
        # Track write timestamp
        self.recent_writes[user_id] = time.time()
        
        # Clear after 5 seconds
        def clear_tracking():
            time.sleep(5)
            if user_id in self.recent_writes:
                del self.recent_writes[user_id]
        
        import threading
        threading.Thread(target=clear_tracking, daemon=True).start()
    
    def read_user(self, user_id):
        """Read user, considering replication lag"""
        # If recently written, read from master
        if user_id in self.recent_writes:
            age = time.time() - self.recent_writes[user_id]
            if age < 5:  # Within 5 seconds of write
                print(f"Reading from master (recent write, age={age:.2f}s)")
                return self.db.execute_read(
                    'SELECT * FROM users WHERE id = %s',
                    (user_id,),
                    use_master=True
                )
        
        # Otherwise read from replica
        print("Reading from replica")
        return self.db.execute_read(
            'SELECT * FROM users WHERE id = %s',
            (user_id,)
        )
```

---

## 3. Connection Pooling

**Problem:** Creating database connections is expensive.

```
Without pooling:
Request 1 → Create connection (50ms) → Query (10ms) → Close connection
Request 2 → Create connection (50ms) → Query (10ms) → Close connection
= Slow + wasteful
```

**With pooling:**
```
Pool: [Conn1, Conn2, Conn3, ...] (pre-created)

Request 1 → Borrow Conn1 → Query (10ms) → Return Conn1
Request 2 → Borrow Conn2 → Query (10ms) → Return Conn2
= Fast + efficient
```

### Implementation

```javascript
// ============================================
// Node.js with pg-pool
// ============================================
const { Pool } = require('pg');

const pool = new Pool({
  host: 'localhost',
  port: 5432,
  database: 'myapp',
  user: 'app',
  password: 'secret',
  
  // Pool configuration
  min: 5,           // Minimum connections
  max: 20,          // Maximum connections
  idleTimeoutMillis: 30000,  // Close idle connections after 30s
  connectionTimeoutMillis: 2000,  // Fail if can't get connection in 2s
});

// Query using pool
async function getUser(userId) {
  // Pool automatically manages connections
  const result = await pool.query(
    'SELECT * FROM users WHERE id = $1',
    [userId]
  );
  
  return result.rows[0];
}

// For transactions, explicitly get client
async function transferMoney(fromAccount, toAccount, amount) {
  const client = await pool.connect();
  
  try {
    await client.query('BEGIN');
    
    await client.query(
      'UPDATE accounts SET balance = balance - $1 WHERE id = $2',
      [amount, fromAccount]
    );
    
    await client.query(
      'UPDATE accounts SET balance = balance + $1 WHERE id = $2',
      [amount, toAccount]
    );
    
    await client.query('COMMIT');
    
  } catch (error) {
    await client.query('ROLLBACK');
    throw error;
  } finally {
    client.release();  // Return connection to pool
  }
}

// Monitor pool
pool.on('connect', () => {
  console.log('New connection created');
});

pool.on('acquire', () => {
  console.log('Connection acquired from pool');
});

pool.on('remove', () => {
  console.log('Connection removed from pool');
});
```

### Python Connection Pooling

```python
# ============================================
# Python with psycopg2 pooling
# ============================================
from psycopg2 import pool
import threading

class DatabasePool:
    _instance = None
    _lock = threading.Lock()
    
    def __new__(cls):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
                    cls._instance._initialize_pool()
        return cls._instance
    
    def _initialize_pool(self):
        """Initialize connection pool (singleton)"""
        self.connection_pool = pool.ThreadedConnectionPool(
            minconn=5,
            maxconn=20,
            host='localhost',
            database='myapp',
            user='app',
            password='secret'
        )
    
    def execute_query(self, query, params=()):
        """Execute query using pool"""
        conn = self.connection_pool.getconn()
        
        try:
            cursor = conn.cursor()
            cursor.execute(query, params)
            
            # Commit if write query
            if query.strip().upper().startswith(('INSERT', 'UPDATE', 'DELETE')):
                conn.commit()
            
            # Fetch results if available
            result = cursor.fetchall() if cursor.description else None
            
            return result
        finally:
            # Always return connection to pool
            self.connection_pool.putconn(conn)
    
    def close_all(self):
        """Close all connections"""
        self.connection_pool.closeall()

# Usage (thread-safe singleton)
db = DatabasePool()

# Multiple threads can use same pool
def worker():
    users = db.execute_query('SELECT * FROM users LIMIT 10')
    print(f"Thread {threading.current_thread().name}: {len(users)} users")

threads = [threading.Thread(target=worker) for _ in range(10)]
for t in threads:
    t.start()
for t in threads:
    t.join()
```

### Pool Sizing Formula

```
Optimal Pool Size = (Core Count × 2) + Effective Spindle Count

For cloud databases (SSD):
Pool Size = Core Count × 2

Example:
- Database: 4 CPU cores
- Optimal pool size: 4 × 2 = 8 connections

Important: Don't set pool > database max connections!
If database max = 100:
- App servers: 10
- Pool per server: 10
- Total: 100 connections ✓

If pool per server = 20:
- Total: 200 connections ❌ (exceeds database limit)
```

---

## 4. Database Sharding Strategies

**Definition:** Sharding is horizontal partitioning - splitting data across multiple databases.

### Strategy 1: Hash-Based Sharding

**How it works:** Hash the shard key, use modulo to determine shard.

```python
def get_shard_by_hash(user_id, num_shards=4):
    """Simple hash-based sharding"""
    shard_index = hash(user_id) % num_shards
    return shard_index

# Examples:
# user_id = 123 → hash(123) % 4 = 3 → Shard 3
# user_id = 456 → hash(456) % 4 = 0 → Shard 0
# user_id = 789 → hash(789) % 4 = 1 → Shard 1
```

**Pros:**
- Even distribution
- Simple to implement

**Cons:**
- Adding/removing shards requires rehashing all data
- Can't do range queries across shards

### Strategy 2: Range-Based Sharding

**How it works:** Assign ranges to shards.

```python
def get_shard_by_range(user_id):
    """Range-based sharding"""
    if user_id < 1_000_000:
        return 0  # Shard 0: 0 - 999,999
    elif user_id < 2_000_000:
        return 1  # Shard 1: 1,000,000 - 1,999,999
    elif user_id < 3_000_000:
        return 2  # Shard 2: 2,000,000 - 2,999,999
    else:
        return 3  # Shard 3: 3,000,000+

# Or for timestamps:
def get_shard_by_date(created_at):
    year = created_at.year
    if year == 2020:
        return 0
    elif year == 2021:
        return 1
    elif year == 2022:
        return 2
    else:
        return 3
```

**Pros:**
- Easy to add new shards (just extend range)
- Range queries work within shard

**Cons:**
- Uneven distribution (newer data gets more traffic)
- Hot shards (recent data accessed more)

### Strategy 3: Geographic Sharding

```python
def get_shard_by_geography(country_code):
    """Geographic sharding"""
    if country_code in ['US', 'CA', 'MX']:
        return 'americas_shard'
    elif country_code in ['GB', 'FR', 'DE', 'IT', 'ES']:
        return 'europe_shard'
    elif country_code in ['CN', 'JP', 'IN', 'KR']:
        return 'asia_shard'
    elif country_code in ['AU', 'NZ']:
        return 'oceania_shard'
    else:
        return 'global_shard'
```

**Pros:**
- Low latency (data close to users)
- Regulatory compliance (data residency)

**Cons:**
- Uneven distribution
- Complex cross-region queries

### Complete Sharding Implementation

```typescript
// ============================================
// Sharded Database Manager
// ============================================
import { Pool } from 'pg';
import crypto from 'crypto';

interface ShardConfig {
  id: number;
  host: string;
  port: number;
  database: string;
}

class ShardedDatabase {
  private shards: Map<number, Pool>;
  private numShards: number;
  
  constructor(shardConfigs: ShardConfig[]) {
    this.numShards = shardConfigs.length;
    this.shards = new Map();
    
    // Initialize connection pool for each shard
    shardConfigs.forEach(config => {
      this.shards.set(config.id, new Pool({
        host: config.host,
        port: config.port,
        database: config.database,
        user: 'app',
        password: process.env.DB_PASSWORD,
        max: 20
      }));
    });
  }
  
  private getShardId(key: string): number {
    // Hash-based sharding
    const hash = crypto.createHash('md5').update(key).digest('hex');
    const hashNumber = parseInt(hash.substring(0, 8), 16);
    return hashNumber % this.numShards;
  }
  
  private getShard(key: string): Pool {
    const shardId = this.getShardId(key);
    return this.shards.get(shardId)!;
  }
  
  async createUser(userId: string, name: string, email: string): Promise<void> {
    const shard = this.getShard(userId);
    
    await shard.query(
      'INSERT INTO users (id, name, email, created_at) VALUES ($1, $2, $3, NOW())',
      [userId, name, email]
    );
    
    console.log(`User ${userId} created in shard ${this.getShardId(userId)}`);
  }
  
  async getUser(userId: string): Promise<any> {
    const shard = this.getShard(userId);
    
    const result = await shard.query(
      'SELECT * FROM users WHERE id = $1',
      [userId]
    );
    
    return result.rows[0];
  }
  
  async updateUser(userId: string, updates: any): Promise<void> {
    const shard = this.getShard(userId);
    
    const setClauses = Object.keys(updates)
      .map((key, i) => `${key} = $${i + 2}`)
      .join(', ');
    
    const values = [userId, ...Object.values(updates)];
    
    await shard.query(
      `UPDATE users SET ${setClauses}, updated_at = NOW() WHERE id = $1`,
      values
    );
  }
  
  async deleteUser(userId: string): Promise<void> {
    const shard = this.getShard(userId);
    
    await shard.query(
      'DELETE FROM users WHERE id = $1',
      [userId]
    );
  }
  
  async getAllUsers(limit: number = 100): Promise<any[]> {
    /**
     * Query across all shards (expensive!)
     * Avoid when possible
     */
    const promises = Array.from(this.shards.values()).map(shard =>
      shard.query('SELECT * FROM users LIMIT $1', [limit])
    );
    
    const results = await Promise.all(promises);
    
    // Merge results from all shards
    return results.flatMap(r => r.rows);
  }
  
  async getUsersByEmail(email: string): Promise<any[]> {
    /**
     * Email is not shard key - must query all shards
     * This is why shard key choice is critical!
     */
    const promises = Array.from(this.shards.values()).map(shard =>
      shard.query('SELECT * FROM users WHERE email = $1', [email])
    );
    
    const results = await Promise.all(promises);
    
    // Find user in results
    for (const result of results) {
      if (result.rows.length > 0) {
        return result.rows;
      }
    }
    
    return [];
  }
}

// Usage
const shardedDb = new ShardedDatabase([
  { id: 0, host: 'shard0.example.com', port: 5432, database: 'myapp_shard0' },
  { id: 1, host: 'shard1.example.com', port: 5432, database: 'myapp_shard1' },
  { id: 2, host: 'shard2.example.com', port: 5432, database: 'myapp_shard2' },
  { id: 3, host: 'shard3.example.com', port: 5432, database: 'myapp_shard3' }
]);

// Create user (goes to specific shard based on ID)
await shardedDb.createUser('user_12345', 'John Doe', 'john@example.com');

// Get user (queries only one shard)
const user = await shardedDb.getUser('user_12345');

// Update user (queries only one shard)
await shardedDb.updateUser('user_12345', { name: 'John Smith' });

// Get all users (queries ALL shards - expensive!)
const allUsers = await shardedDb.getAllUsers(1000);
```

---

## 5. Consistent Hashing

**Problem:** Simple hash-based sharding breaks when adding/removing shards.

```
With 4 shards:
user_123 → hash(123) % 4 = 3 → Shard 3

Add 5th shard:
user_123 → hash(123) % 5 = 3 → Still Shard 3? Maybe not!

Result: Most keys redistributed = massive data migration
```

**Solution: Consistent Hashing**

```python
import hashlib
import bisect

class ConsistentHashRing:
    def __init__(self, nodes=None, virtual_nodes=150):
        """
        nodes: List of node identifiers
        virtual_nodes: Number of virtual nodes per physical node
        """
        self.virtual_nodes = virtual_nodes
        self.ring = {}
        self.sorted_keys = []
        
        if nodes:
            for node in nodes:
                self.add_node(node)
    
    def _hash(self, key):
        """Generate hash for key"""
        return int(hashlib.md5(str(key).encode()).hexdigest(), 16)
    
    def add_node(self, node):
        """Add a node to the ring"""
        for i in range(self.virtual_nodes):
            virtual_key = f"{node}:{i}"
            hash_value = self._hash(virtual_key)
            
            self.ring[hash_value] = node
            bisect.insort(self.sorted_keys, hash_value)
        
        print(f"Added node {node} with {self.virtual_nodes} virtual nodes")
    
    def remove_node(self, node):
        """Remove a node from the ring"""
        for i in range(self.virtual_nodes):
            virtual_key = f"{node}:{i}"
            hash_value = self._hash(virtual_key)
            
            if hash_value in self.ring:
                del self.ring[hash_value]
                self.sorted_keys.remove(hash_value)
        
        print(f"Removed node {node}")
    
    def get_node(self, key):
        """Get the node responsible for a key"""
        if not self.ring:
            return None
        
        hash_value = self._hash(key)
        
        # Find first node clockwise from hash
        index = bisect.bisect_right(self.sorted_keys, hash_value)
        
        if index == len(self.sorted_keys):
            index = 0
        
        node = self.ring[self.sorted_keys[index]]
        return node
    
    def get_nodes_for_replication(self, key, num_replicas=3):
        """Get multiple nodes for replication"""
        if not self.ring:
            return []
        
        hash_value = self._hash(key)
        index = bisect.bisect_right(self.sorted_keys, hash_value)
        
        nodes = set()
        
        # Get unique nodes (skip virtual duplicates)
        while len(nodes) < num_replicas and len(nodes) < len(set(self.ring.values())):
            if index >= len(self.sorted_keys):
                index = 0
            
            node = self.ring[self.sorted_keys[index]]
            nodes.add(node)
            index += 1
        
        return list(nodes)

# Usage
ring = ConsistentHashRing()

# Add 4 nodes
ring.add_node('shard0.db.example.com')
ring.add_node('shard1.db.example.com')
ring.add_node('shard2.db.example.com')
ring.add_node('shard3.db.example.com')

# Keys distributed across nodes
print(ring.get_node('user_123'))  # shard2.db.example.com
print(ring.get_node('user_456'))  # shard0.db.example.com
print(ring.get_node('user_789'))  # shard1.db.example.com

# Add 5th node - only ~25% of keys move (not 100%!)
ring.add_node('shard4.db.example.com')

print(ring.get_node('user_123'))  # Might still be shard2
print(ring.get_node('user_456'))  # Might move to shard4

# Replication support
replicas = ring.get_nodes_for_replication('user_123', num_replicas=3)
print(f"Replicate to: {replicas}")
```

**Benefits:**
- Adding node redistributes ~1/N keys only
- Removing node redistributes keys to adjacent nodes only
- Minimal disruption

---

## 6. Partitioning vs Sharding

### Partitioning (Vertical Split)

**Definition:** Split table into smaller tables within same database.

```sql
-- Original large table
CREATE TABLE orders (
    id BIGSERIAL PRIMARY KEY,
    user_id BIGINT,
    total DECIMAL,
    created_at TIMESTAMP,
    status VARCHAR(20)
);

-- Partitioned by year (PostgreSQL)
CREATE TABLE orders (
    id BIGSERIAL,
    user_id BIGINT,
    total DECIMAL,
    created_at TIMESTAMP,
    status VARCHAR(20)
) PARTITION BY RANGE (created_at);

-- Create partitions
CREATE TABLE orders_2020 PARTITION OF orders
    FOR VALUES FROM ('2020-01-01') TO ('2021-01-01');

CREATE TABLE orders_2021 PARTITION OF orders
    FOR VALUES FROM ('2021-01-01') TO ('2022-01-01');

CREATE TABLE orders_2022 PARTITION OF orders
    FOR VALUES FROM ('2022-01-01') TO ('2023-01-01');

CREATE TABLE orders_2023 PARTITION OF orders
    FOR VALUES FROM ('2023-01-01') TO ('2024-01-01');

-- Queries automatically route to correct partition
SELECT * FROM orders WHERE created_at >= '2022-01-01';
-- Only scans orders_2022 and orders_2023 partitions
```

**Benefits:**
- Better query performance (smaller tables)
- Easy to archive old data (drop partition)
- Maintenance on partition, not whole table

### Sharding (Horizontal Split Across Servers)

**Definition:** Split data across multiple database servers.

```
Same data, different servers:

Shard 0 (server1): Users 0-2,499,999
Shard 1 (server2): Users 2,500,000-4,999,999
Shard 2 (server3): Users 5,000,000-7,499,999
Shard 3 (server4): Users 7,500,000+
```

---

## 7. Handling Hot Partitions

**Problem:** Some shards get more traffic than others.

```
Shard 0: Celebrities (1M followers each) → 90% of traffic
Shard 1: Regular users → 5% of traffic
Shard 2: Regular users → 3% of traffic
Shard 3: Regular users → 2% of traffic

Shard 0 is a HOT PARTITION (overloaded)
```

### Solution 1: Split Hot Shard

```python
def get_shard_for_celebrity(user_id, is_celebrity):
    """Special handling for celebrities"""
    if is_celebrity:
        # Celebrities distributed across dedicated shards
        celebrity_shard = hash(user_id) % 10  # 10 celebrity shards
        return f'celebrity_shard_{celebrity_shard}'
    else:
        # Regular users
        shard = hash(user_id) % 4
        return f'user_shard_{shard}'
```

### Solution 2: Caching Hot Data

```python
import redis

class HotPartitionHandler:
    def __init__(self, db, cache):
        self.db = db
        self.cache = cache
    
    async def get_user(self, user_id):
        # Try cache first (for hot data)
        cached = await self.cache.get(f'user:{user_id}')
        if cached:
            return json.loads(cached)
        
        # Query database
        user = await self.db.query(
            'SELECT * FROM users WHERE id = ?',
            [user_id]
        )
        
        # Cache hot data with longer TTL
        if user['follower_count'] > 100000:  # Celebrity
            ttl = 3600  # 1 hour
        else:
            ttl = 300  # 5 minutes
        
        await self.cache.setex(
            f'user:{user_id}',
            ttl,
            json.dumps(user)
        )
        
        return user
```

### Solution 3: Read Replicas for Hot Shard

```
Hot Shard Architecture:

Shard 0 (Hot):
  Master (writes)
  + 5 Read Replicas (reads)

Shard 1 (Normal):
  Master (writes)
  + 1 Read Replica (reads)
```

---

## 8. Cross-Shard Queries

**Problem:** Data spread across shards makes queries complex.

### Scatter-Gather Pattern

```python
class ShardedQuery:
    def __init__(self, shards):
        self.shards = shards
    
    async def query_all_shards(self, query, params=()):
        """Execute query on all shards and merge results"""
        tasks = []
        
        for shard in self.shards.values():
            task = shard.query(query, params)
            tasks.append(task)
        
        # Execute in parallel
        results = await asyncio.gather(*tasks)
        
        # Merge results
        merged = []
        for result in results:
            merged.extend(result.rows)
        
        return merged
    
    async def aggregate_across_shards(self):
        """Aggregate data from all shards"""
        # Get count from each shard
        tasks = []
        for shard in self.shards.values():
            task = shard.query('SELECT COUNT(*) as count FROM users')
            tasks.append(task)
        
        results = await asyncio.gather(*tasks)
        
        # Sum counts
        total = sum(result.rows[0]['count'] for result in results)
        
        return total
    
    async def search_across_shards(self, email):
        """Search for user by non-shard-key field"""
        # Must query all shards
        tasks = []
        
        for shard_id, shard in self.shards.items():
            task = shard.query(
                'SELECT * FROM users WHERE email = $1',
                [email]
            )
            tasks.append((shard_id, task))
        
        results = await asyncio.gather(*[task for _, task in tasks])
        
        # Find which shard has the user
        for i, result in enumerate(results):
            if result.rows:
                shard_id = list(self.shards.keys())[i]
                print(f"User found in shard {shard_id}")
                return result.rows[0]
        
        return None
```

### Avoiding Cross-Shard Queries

```python
# ❌ Bad: Requires cross-shard query
def get_user_orders(user_id):
    # User on Shard A
    user = get_user_from_shard(user_id)
    
    # Orders on Shard B (sharded by order_id)
    orders = get_orders_from_all_shards(user_id)  # Expensive!
    
    return user, orders

# ✅ Good: Co-locate related data
def get_user_orders(user_id):
    # Both user and their orders on same shard
    shard = get_shard(user_id)
    
    user = shard.query('SELECT * FROM users WHERE id = ?', [user_id])
    orders = shard.query('SELECT * FROM orders WHERE user_id = ?', [user_id])
    
    return user, orders

# Shard by user_id, not order_id
# Store user's orders on same shard as user
```

---

## 9. Real-World Sharding Examples

### Instagram's Sharding Strategy

```python
def instagram_id_generation(logical_shard_id):
    """
    Instagram's ID generation for sharding
    
    64-bit ID structure:
    - 41 bits: timestamp (milliseconds since epoch)
    - 13 bits: logical shard ID
    - 10 bits: sequence number
    
    Example: 1234567890123456789
    """
    import time
    
    timestamp = int(time.time() * 1000)  # Current time in ms
    sequence = 0  # Increment per ID
    
    # Combine into 64-bit ID
    id = (timestamp << 23) | (logical_shard_id << 10) | sequence
    
    return id

# Benefits:
# - ID contains shard info (can route without lookup)
# - Time-sortable
# - Unique across all shards
```

### Discord's Sharding (Snowflake IDs)

```javascript
class SnowflakeIDGenerator {
  constructor(workerId, datacenterId) {
    this.workerId = workerId;          // 5 bits (0-31)
    this.datacenterId = datacenterId;  // 5 bits (0-31)
    this.sequence = 0;                 // 12 bits (0-4095)
    this.lastTimestamp = -1;
    
    // Epoch (Discord: 2015-01-01)
    this.epoch = 1420070400000;
  }
  
  generate() {
    let timestamp = Date.now();
    
    if (timestamp < this.lastTimestamp) {
      throw new Error('Clock moved backwards');
    }
    
    if (timestamp === this.lastTimestamp) {
      // Same millisecond - increment sequence
      this.sequence = (this.sequence + 1) & 4095;
      
      if (this.sequence === 0) {
        // Sequence overflow - wait for next millisecond
        while (timestamp <= this.lastTimestamp) {
          timestamp = Date.now();
        }
      }
    } else {
      this.sequence = 0;
    }
    
    this.lastTimestamp = timestamp;
    
    // Build 64-bit ID
    // [41 bits timestamp][5 bits datacenter][5 bits worker][12 bits sequence]
    const id = 
      ((timestamp - this.epoch) << 22) |
      (this.datacenterId << 17) |
      (this.workerId << 12) |
      this.sequence;
    
    return id.toString();
  }
  
  // Extract components from ID
  static parse(snowflake) {
    const id = BigInt(snowflake);
    const epoch = 1420070400000n;
    
    return {
      timestamp: Number((id >> 22n) + epoch),
      datacenterId: Number((id & 0x3E0000n) >> 17n),
      workerId: Number((id & 0x1F000n) >> 12n),
      sequence: Number(id & 0xFFFn)
    };
  }
}

// Usage
const generator = new SnowflakeIDGenerator(1, 1);

const id1 = generator.generate();
const id2 = generator.generate();

console.log('ID:', id1);
console.log('Parsed:', SnowflakeIDGenerator.parse(id1));

// Output:
// ID: 175928847299117063
// Parsed: {
//   timestamp: 1642089600000,
//   datacenterId: 1,
//   workerId: 1,
//   sequence: 7
// }
```

---

## 10. Database Scaling Patterns

### Pattern 1: CQRS with Separate Databases

```typescript
// ============================================
// Write Database (Normalized, ACID)
// ============================================
class OrderWriteRepository {
  private masterDb: Pool;
  
  async createOrder(order: Order): Promise<void> {
    const client = await this.masterDb.connect();
    
    try {
      await client.query('BEGIN');
      
      // Insert into normalized tables
      await client.query(
        'INSERT INTO orders (id, user_id, total) VALUES ($1, $2, $3)',
        [order.id, order.userId, order.total]
      );
      
      for (const item of order.items) {
        await client.query(
          'INSERT INTO order_items (order_id, product_id, quantity, price) VALUES ($1, $2, $3, $4)',
          [order.id, item.productId, item.quantity, item.price]
        );
      }
      
      await client.query('COMMIT');
      
      // Publish event to update read database
      await eventBus.publish('OrderCreated', order);
      
    } catch (error) {
      await client.query('ROLLBACK');
      throw error;
    } finally {
      client.release();
    }
  }
}

// ============================================
// Read Database (Denormalized, Fast)
// ============================================
class OrderReadRepository {
  private mongodb: MongoClient;
  
  async updateFromEvent(event: OrderCreatedEvent): Promise<void> {
    // Build denormalized document
    const orderDocument = {
      _id: event.orderId,
      userId: event.userId,
      userName: await this.getUserName(event.userId),
      items: await this.enrichItems(event.items),
      total: event.total,
      status: 'pending',
      createdAt: event.timestamp
    };
    
    // Store in MongoDB (optimized for reads)
    await this.mongodb.db('orders').collection('orders').insertOne(orderDocument);
  }
  
  async getOrder(orderId: string): Promise<any> {
    // Single query, everything in one document
    return await this.mongodb.db('orders')
      .collection('orders')
      .findOne({ _id: orderId });
  }
  
  async getUserOrders(userId: string): Promise<any[]> {
    // Fast query with index on userId
    return await this.mongodb.db('orders')
      .collection('orders')
      .find({ userId })
      .toArray();
  }
}
```

### Pattern 2: Materialized Views

```sql
-- Expensive query (JOINs multiple tables)
SELECT 
    users.id,
    users.name,
    COUNT(orders.id) as order_count,
    SUM(orders.total) as total_spent,
    MAX(orders.created_at) as last_order
FROM users
LEFT JOIN orders ON orders.user_id = users.id
GROUP BY users.id, users.name;

-- Create materialized view (pre-computed)
CREATE MATERIALIZED VIEW user_stats AS
SELECT 
    users.id,
    users.name,
    COUNT(orders.id) as order_count,
    SUM(orders.total) as total_spent,
    MAX(orders.created_at) as last_order
FROM users
LEFT JOIN orders ON orders.user_id = users.id
GROUP BY users.id, users.name;

-- Create index on materialized view
CREATE INDEX idx_user_stats_id ON user_stats(id);

-- Fast query (no JOINs)
SELECT * FROM user_stats WHERE id = 123;

-- Refresh materialized view (scheduled job)
REFRESH MATERIALIZED VIEW CONCURRENTLY user_stats;
```

### Pattern 3: Database per Service (Microservices)

```
User Service → User Database (PostgreSQL)
Order Service → Order Database (PostgreSQL)
Product Service → Product Database (MongoDB)
Analytics Service → Analytics Database (ClickHouse)

Each service owns its database
No shared database
```

```typescript
// User Service
class UserService {
  private userDb: Pool;  // Only accesses user database
  
  async getUser(userId: string): Promise<User> {
    const result = await this.userDb.query(
      'SELECT * FROM users WHERE id = $1',
      [userId]
    );
    return result.rows[0];
  }
}

// Order Service
class OrderService {
  private orderDb: Pool;  // Only accesses order database
  private userServiceClient: UserServiceClient;  // Calls User Service API
  
  async createOrder(userId: string, items: any[]): Promise<Order> {
    // Get user info from User Service (not direct DB access)
    const user = await this.userServiceClient.getUser(userId);
    
    // Create order in own database
    const result = await this.orderDb.query(
      'INSERT INTO orders (user_id, items, total) VALUES ($1, $2, $3) RETURNING *',
      [userId, JSON.stringify(items), calculateTotal(items)]
    );
    
    return result.rows[0];
  }
}
```

---

## Chapter 16 Summary

### Key Concepts

1. **Read Replicas** - Scale reads by copying data
2. **Connection Pooling** - Reuse connections efficiently
3. **Sharding** - Split data across servers
4. **Consistent Hashing** - Minimize reshuffling when scaling
5. **Partitioning** - Split tables within database
6. **Hot Partitions** - Some shards get more traffic

### Sharding Strategies

| Strategy | Distribution | Use Case | Complexity |
|----------|--------------|----------|------------|
| **Hash** | Even | General purpose | Medium |
| **Range** | Uneven | Time-series, sequential IDs | Low |
| **Geographic** | By location | Multi-region | Medium |
| **Consistent Hashing** | Even, scalable | Dynamic cluster | High |

### When to Use Each Scaling Technique

| Technique | When | Complexity |
|-----------|------|------------|
| **Vertical Scaling** | First step, < 100K users | Low |
| **Connection Pooling** | Always | Low |
| **Read Replicas** | Read-heavy workload | Medium |
| **Caching** | Hot data access | Medium |
| **Partitioning** | Large tables (100M+ rows) | Medium |
| **Sharding** | Write-heavy (> 10K writes/sec) | High |

### Database Scaling Checklist

Before sharding:
- [ ] Optimized queries (indexes, explain plans)
- [ ] Connection pooling implemented
- [ ] Caching layer added
- [ ] Vertical scaling exhausted
- [ ] Read replicas added
- [ ] Partitioning explored

Shard only when:
- [ ] Write throughput > single DB capacity
- [ ] Clear shard key identified
- [ ] Application can handle complexity
- [ ] Team has expertise

### Interview Tips

**Common Questions:**
1. "How do you scale a database?"
2. "Explain database sharding"
3. "What is consistent hashing?"
4. "How do you handle cross-shard queries?"
5. "Read replicas vs sharding?"

**How to Answer:**
- Draw architecture diagrams
- Explain trade-offs
- Start simple (replicas) before complex (sharding)
- Give real examples (Instagram, Discord)
- Mention specific numbers (10K writes/sec)

### Best Practices

1. **Choose shard key carefully** - Can't change easily
2. **Co-locate related data** - Avoid cross-shard queries
3. **Use consistent hashing** - For dynamic scaling
4. **Monitor shard balance** - Detect hot partitions
5. **Cache aggressively** - Reduce database load
6. **Plan migration** - Resharding is painful

### Anti-Patterns

❌ **Premature sharding** - Shard before needed
❌ **Wrong shard key** - Causes hot partitions
❌ **No monitoring** - Can't detect issues
❌ **Ignoring replication lag** - Stale reads
❌ **No connection pooling** - Connection exhaustion

Congratulations! You've completed Chapters 13-16 covering Message Queues, API Design, Scaling Strategies, and Database Scaling - critical topics for system design interviews!