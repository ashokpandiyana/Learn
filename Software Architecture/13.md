# Chapter 13: Message Queues & Async Processing

## Table of Contents
1. Introduction to Message Queues
2. Point-to-Point vs Publish-Subscribe
3. Message Ordering and Idempotency
4. Dead Letter Queues
5. Competing Consumers Pattern
6. Implementation with RabbitMQ
7. Implementation with AWS SQS
8. Implementation with Apache Kafka
9. Async Processing Patterns
10. Error Handling and Retries
11. Real-World Use Cases

---

## 1. Introduction to Message Queues

**Definition:** Message queues enable asynchronous communication between services by storing messages in a queue until they can be processed.

### Why Use Message Queues?

**Without Message Queue (Synchronous):**
```
User → API → Process → Email → Database → Response
       └─ Waits for everything ─┘
       User waits 5 seconds for response
```

**With Message Queue (Asynchronous):**
```
User → API → Queue → Response (instant)
              ↓
         Worker processes in background
         └→ Email, Database, etc.
         User gets response in 50ms
```

### Benefits

1. **Decoupling** - Services don't need to know about each other
2. **Resilience** - Messages persist if consumer is down
3. **Scalability** - Add more workers to process faster
4. **Load Leveling** - Handle traffic spikes gracefully
5. **Retry Logic** - Built-in retry mechanisms

### Core Concepts

```
Producer → [Message Queue] → Consumer

Producer: Sends messages to queue
Queue: Stores messages until processed
Consumer: Receives and processes messages
```

---

## 2. Point-to-Point vs Publish-Subscribe

### 2.1 Point-to-Point (Queue)

**Pattern:** One message delivered to exactly one consumer.

```
Producer → Queue → Consumer 1 (gets msg 1)
                → Consumer 2 (gets msg 2)
                → Consumer 3 (gets msg 3)
                
Each message consumed by only one worker
```

**Use Cases:**
- Task processing (image resizing, video encoding)
- Order processing
- Background jobs

### 2.2 Publish-Subscribe (Topic)

**Pattern:** One message delivered to all subscribed consumers.

```
Producer → Topic → Consumer 1 (gets msg)
                → Consumer 2 (gets msg)
                → Consumer 3 (gets msg)
                
All consumers receive the same message
```

**Use Cases:**
- Notifications (email, SMS, push)
- Event broadcasting
- Cache invalidation across servers

### Implementation Comparison

```python
# ============================================
# Point-to-Point (RabbitMQ)
# ============================================
import pika

# Producer
connection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))
channel = connection.channel()

# Declare queue
channel.queue_declare(queue='task_queue', durable=True)

# Send message
channel.basic_publish(
    exchange='',
    routing_key='task_queue',
    body='Process this task',
    properties=pika.BasicProperties(delivery_mode=2)  # Persistent
)

# Consumer 1
def callback(ch, method, properties, body):
    print(f"Consumer 1 received: {body}")
    ch.basic_ack(delivery_tag=method.delivery_tag)

channel.basic_qos(prefetch_count=1)
channel.basic_consume(queue='task_queue', on_message_callback=callback)
channel.start_consuming()

# ============================================
# Publish-Subscribe (RabbitMQ)
# ============================================

# Publisher
channel.exchange_declare(exchange='notifications', exchange_type='fanout')

channel.basic_publish(
    exchange='notifications',
    routing_key='',  # Ignored for fanout
    body='New user registered'
)

# Subscriber 1 (Email Service)
channel.exchange_declare(exchange='notifications', exchange_type='fanout')
result = channel.queue_declare(queue='', exclusive=True)
queue_name = result.method.queue

channel.queue_bind(exchange='notifications', queue=queue_name)

def email_callback(ch, method, properties, body):
    print(f"Email Service: {body}")
    # Send email...

channel.basic_consume(queue=queue_name, on_message_callback=email_callback, auto_ack=True)

# Subscriber 2 (SMS Service)
# Similar setup - both receive the same message
```

---

## 3. Message Ordering and Idempotency

### 3.1 Message Ordering

**Problem:** Messages might be processed out of order.

```
Send: Message 1, Message 2, Message 3
Receive: Message 1, Message 3, Message 2 (out of order!)
```

**Solution 1: Single Consumer (Sequential Processing)**

```python
# Only one consumer processes messages sequentially
# Guaranteed order but slower

channel.basic_qos(prefetch_count=1)  # Process one at a time
```

**Solution 2: Partition Key (Kafka)**

```python
from kafka import KafkaProducer
import json

producer = KafkaProducer(
    bootstrap_servers=['localhost:9092'],
    value_serializer=lambda v: json.dumps(v).encode('utf-8')
)

# Messages with same key go to same partition (ordered)
user_id = 'user_123'

producer.send(
    'orders',
    key=user_id.encode('utf-8'),
    value={'action': 'create', 'order_id': 1}
)

producer.send(
    'orders',
    key=user_id.encode('utf-8'),  # Same key = same partition = ordered
    value={'action': 'update', 'order_id': 1}
)
```

**Solution 3: Sequence Numbers**

```python
class OrderedMessageProcessor:
    def __init__(self):
        self.last_processed = {}  # user_id -> last sequence number
        self.pending = {}  # user_id -> [pending messages]
    
    def process_message(self, user_id, sequence_number, message):
        expected = self.last_processed.get(user_id, 0) + 1
        
        if sequence_number == expected:
            # Process message
            self.handle_message(message)
            self.last_processed[user_id] = sequence_number
            
            # Process any pending messages that are now in order
            self.process_pending(user_id)
            
        elif sequence_number > expected:
            # Future message - store for later
            if user_id not in self.pending:
                self.pending[user_id] = []
            self.pending[user_id].append((sequence_number, message))
            
        # else: duplicate or old message, ignore
    
    def process_pending(self, user_id):
        if user_id not in self.pending:
            return
        
        # Sort pending messages
        self.pending[user_id].sort(key=lambda x: x[0])
        
        # Process consecutive messages
        while self.pending[user_id]:
            seq, msg = self.pending[user_id][0]
            expected = self.last_processed[user_id] + 1
            
            if seq == expected:
                self.pending[user_id].pop(0)
                self.handle_message(msg)
                self.last_processed[user_id] = seq
            else:
                break  # Gap in sequence
```

### 3.2 Idempotency

**Problem:** Message might be processed multiple times.

```
Scenario:
1. Consumer processes message
2. Consumer crashes before acknowledging
3. Message redelivered
4. Processed twice! (duplicate charge, double email, etc.)
```

**Solution: Idempotent Operations**

```typescript
import Redis from 'ioredis';

class IdempotentProcessor {
    private redis: Redis;
    
    constructor() {
        this.redis = new Redis();
    }
    
    async processMessage(messageId: string, message: any): Promise<void> {
        // Check if already processed
        const processed = await this.redis.get(`processed:${messageId}`);
        
        if (processed) {
            console.log(`Message ${messageId} already processed, skipping`);
            return;
        }
        
        // Process message
        await this.handleMessage(message);
        
        // Mark as processed (with expiry)
        await this.redis.setex(
            `processed:${messageId}`,
            86400,  // 24 hours
            'true'
        );
    }
    
    private async handleMessage(message: any): Promise<void> {
        // Your business logic here
        console.log('Processing:', message);
    }
}

// Usage
const processor = new IdempotentProcessor();

// Process same message multiple times - only processes once
await processor.processMessage('msg_123', { action: 'charge', amount: 100 });
await processor.processMessage('msg_123', { action: 'charge', amount: 100 });
// Second call skipped - idempotent
```

**Idempotent Database Operations:**

```sql
-- ❌ Not idempotent - running twice charges twice
UPDATE accounts SET balance = balance - 100 WHERE user_id = 123;

-- ✅ Idempotent - include unique transaction ID
INSERT INTO transactions (id, user_id, amount, type)
VALUES ('txn_123', 123, -100, 'charge')
ON CONFLICT (id) DO NOTHING;  -- Ignore if already exists

UPDATE accounts SET balance = balance - 100
WHERE user_id = 123
AND NOT EXISTS (
    SELECT 1 FROM transactions WHERE id = 'txn_123'
);
```

---

## 4. Dead Letter Queues

**Definition:** Special queue for messages that cannot be processed successfully.

### DLQ Flow

```
Message → Queue → Consumer
                    ↓ Failed
                    ↓ (after retries)
                    ↓
                  DLQ (investigate later)
```

### Implementation

```python
import pika
import json
import traceback

class WorkerWithDLQ:
    def __init__(self):
        self.connection = pika.BlockingConnection(
            pika.ConnectionParameters('localhost')
        )
        self.channel = self.connection.channel()
        
        # Main queue
        self.channel.queue_declare(
            queue='tasks',
            durable=True,
            arguments={
                'x-dead-letter-exchange': 'dlx',
                'x-dead-letter-routing-key': 'failed_tasks'
            }
        )
        
        # Dead letter exchange and queue
        self.channel.exchange_declare(exchange='dlx', exchange_type='direct')
        self.channel.queue_declare(queue='failed_tasks', durable=True)
        self.channel.queue_bind(
            exchange='dlx',
            queue='failed_tasks',
            routing_key='failed_tasks'
        )
        
        self.max_retries = 3
    
    def process_message(self, ch, method, properties, body):
        message = json.loads(body)
        retry_count = properties.headers.get('x-retry-count', 0) if properties.headers else 0
        
        try:
            # Process message
            self.handle_task(message)
            
            # Acknowledge success
            ch.basic_ack(delivery_tag=method.delivery_tag)
            print(f"Successfully processed: {message}")
            
        except Exception as e:
            print(f"Error processing message: {e}")
            
            if retry_count < self.max_retries:
                # Retry: reject and requeue
                retry_count += 1
                
                # Republish with updated retry count
                headers = properties.headers or {}
                headers['x-retry-count'] = retry_count
                
                ch.basic_publish(
                    exchange='',
                    routing_key='tasks',
                    body=body,
                    properties=pika.BasicProperties(
                        headers=headers,
                        delivery_mode=2
                    )
                )
                
                # Acknowledge original message
                ch.basic_ack(delivery_tag=method.delivery_tag)
                
                print(f"Retry {retry_count}/{self.max_retries}")
            else:
                # Max retries exceeded - send to DLQ
                ch.basic_nack(
                    delivery_tag=method.delivery_tag,
                    requeue=False  # Don't requeue, goes to DLQ
                )
                
                print(f"Max retries exceeded, sent to DLQ")
    
    def handle_task(self, message):
        # Simulate task processing
        if message.get('type') == 'fail':
            raise Exception("Intentional failure")
        
        print(f"Processing task: {message}")
    
    def start(self):
        self.channel.basic_qos(prefetch_count=1)
        self.channel.basic_consume(
            queue='tasks',
            on_message_callback=self.process_message
        )
        
        print("Worker started, waiting for messages...")
        self.channel.start_consuming()

# Start worker
worker = WorkerWithDLQ()
worker.start()
```

### Monitoring DLQ

```python
def check_dlq():
    """Monitor dead letter queue and alert"""
    connection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))
    channel = connection.channel()
    
    # Check DLQ size
    queue_info = channel.queue_declare(queue='failed_tasks', passive=True)
    message_count = queue_info.method.message_count
    
    if message_count > 10:
        # Alert: Too many failed messages
        send_alert(f"DLQ has {message_count} messages - investigate!")
    
    connection.close()
```

---

## 5. Competing Consumers Pattern

**Definition:** Multiple consumers process messages from the same queue in parallel.

```
           ┌─→ Consumer 1
           │
Queue ─────┼─→ Consumer 2
           │
           └─→ Consumer 3

All consumers compete for messages
Automatic load balancing
```

### Implementation

```javascript
// worker.js - Multiple instances can run simultaneously
const amqp = require('amqplib');

async function startWorker(workerId) {
    const connection = await amqp.connect('amqp://localhost');
    const channel = await connection.createChannel();
    
    const queue = 'image_processing';
    
    await channel.assertQueue(queue, { durable: true });
    
    // Process one message at a time per worker
    channel.prefetch(1);
    
    console.log(`Worker ${workerId} started, waiting for messages...`);
    
    channel.consume(queue, async (msg) => {
        if (msg) {
            const task = JSON.parse(msg.content.toString());
            
            console.log(`Worker ${workerId} processing: ${task.image_url}`);
            
            try {
                // Simulate image processing
                await processImage(task.image_url);
                
                // Acknowledge success
                channel.ack(msg);
                
                console.log(`Worker ${workerId} completed: ${task.image_url}`);
                
            } catch (error) {
                console.error(`Worker ${workerId} failed:`, error);
                
                // Reject and requeue
                channel.nack(msg, false, true);
            }
        }
    });
}

async function processImage(url) {
    // Simulate processing time
    await new Promise(resolve => setTimeout(resolve, 2000));
    console.log(`Processed image: ${url}`);
}

// Start worker with unique ID
const workerId = process.env.WORKER_ID || Math.random().toString(36).substr(2, 9);
startWorker(workerId);
```

**Run multiple workers:**

```bash
# Terminal 1
WORKER_ID=worker1 node worker.js

# Terminal 2
WORKER_ID=worker2 node worker.js

# Terminal 3
WORKER_ID=worker3 node worker.js

# Producer sends 100 messages
# Workers automatically share the load
```

### Auto-Scaling Based on Queue Depth

```python
import boto3
import time

class AutoScalingWorkers:
    def __init__(self):
        self.sqs = boto3.client('sqs')
        self.ecs = boto3.client('ecs')
        self.queue_url = 'https://sqs.us-east-1.amazonaws.com/123456789/tasks'
        self.cluster_name = 'worker-cluster'
        self.service_name = 'worker-service'
    
    def get_queue_depth(self):
        """Get number of messages in queue"""
        response = self.sqs.get_queue_attributes(
            QueueUrl=self.queue_url,
            AttributeNames=['ApproximateNumberOfMessages']
        )
        return int(response['Attributes']['ApproximateNumberOfMessages'])
    
    def scale_workers(self, desired_count):
        """Scale ECS service"""
        self.ecs.update_service(
            cluster=self.cluster_name,
            service=self.service_name,
            desiredCount=desired_count
        )
        print(f"Scaled workers to {desired_count}")
    
    def auto_scale(self):
        """Auto-scale based on queue depth"""
        queue_depth = self.get_queue_depth()
        
        # Scale formula: 1 worker per 100 messages
        desired_workers = max(1, min(queue_depth // 100, 10))
        
        print(f"Queue depth: {queue_depth}, Desired workers: {desired_workers}")
        
        self.scale_workers(desired_workers)
    
    def run(self):
        """Run auto-scaling loop"""
        while True:
            self.auto_scale()
            time.sleep(60)  # Check every minute

# Start auto-scaler
scaler = AutoScalingWorkers()
scaler.run()
```

---

## 6. Implementation with RabbitMQ

### Complete Example: Order Processing System

```python
# ============================================
# producer.py - Order Service
# ============================================
import pika
import json
import uuid

class OrderService:
    def __init__(self):
        self.connection = pika.BlockingConnection(
            pika.ConnectionParameters('localhost')
        )
        self.channel = self.connection.channel()
        
        # Declare exchanges
        self.channel.exchange_declare(
            exchange='orders',
            exchange_type='topic',
            durable=True
        )
        
        # Declare queues
        self.channel.queue_declare(queue='order_processing', durable=True)
        self.channel.queue_declare(queue='email_notifications', durable=True)
        self.channel.queue_declare(queue='inventory_updates', durable=True)
        
        # Bind queues to exchange with routing keys
        self.channel.queue_bind(
            exchange='orders',
            queue='order_processing',
            routing_key='order.created'
        )
        
        self.channel.queue_bind(
            exchange='orders',
            queue='email_notifications',
            routing_key='order.*'  # All order events
        )
        
        self.channel.queue_bind(
            exchange='orders',
            queue='inventory_updates',
            routing_key='order.created'
        )
    
    def create_order(self, customer_id, items):
        order_id = str(uuid.uuid4())
        
        order_data = {
            'order_id': order_id,
            'customer_id': customer_id,
            'items': items,
            'total': sum(item['price'] * item['quantity'] for item in items),
            'timestamp': str(time.time())
        }
        
        # Publish to exchange
        self.channel.basic_publish(
            exchange='orders',
            routing_key='order.created',
            body=json.dumps(order_data),
            properties=pika.BasicProperties(
                delivery_mode=2,  # Persistent
                content_type='application/json',
                message_id=order_id
            )
        )
        
        print(f"Order {order_id} created and published")
        return order_id
    
    def close(self):
        self.connection.close()

# ============================================
# consumer_payment.py - Payment Worker
# ============================================
import pika
import json
import time

class PaymentWorker:
    def __init__(self):
        self.connection = pika.BlockingConnection(
            pika.ConnectionParameters('localhost')
        )
        self.channel = self.connection.channel()
        
        self.channel.queue_declare(queue='order_processing', durable=True)
        self.channel.basic_qos(prefetch_count=1)
    
    def process_order(self, ch, method, properties, body):
        order = json.loads(body)
        
        print(f"Processing payment for order {order['order_id']}")
        
        try:
            # Simulate payment processing
            time.sleep(2)
            
            success = self.charge_payment(order['customer_id'], order['total'])
            
            if success:
                print(f"Payment successful for order {order['order_id']}")
                
                # Publish order.paid event
                ch.basic_publish(
                    exchange='orders',
                    routing_key='order.paid',
                    body=json.dumps({
                        'order_id': order['order_id'],
                        'status': 'paid'
                    }),
                    properties=pika.BasicProperties(delivery_mode=2)
                )
                
                # Acknowledge message
                ch.basic_ack(delivery_tag=method.delivery_tag)
            else:
                print(f"Payment failed for order {order['order_id']}")
                
                # Publish order.payment_failed event
                ch.basic_publish(
                    exchange='orders',
                    routing_key='order.payment_failed',
                    body=json.dumps({
                        'order_id': order['order_id'],
                        'status': 'payment_failed'
                    }),
                    properties=pika.BasicProperties(delivery_mode=2)
                )
                
                ch.basic_ack(delivery_tag=method.delivery_tag)
                
        except Exception as e:
            print(f"Error processing order: {e}")
            # Reject and requeue
            ch.basic_nack(delivery_tag=method.delivery_tag, requeue=True)
    
    def charge_payment(self, customer_id, amount):
        # Simulate payment gateway call
        return True  # Success
    
    def start(self):
        self.channel.basic_consume(
            queue='order_processing',
            on_message_callback=self.process_order
        )
        
        print("Payment worker started")
        self.channel.start_consuming()

# ============================================
# consumer_email.py - Email Worker
# ============================================
class EmailWorker:
    def __init__(self):
        self.connection = pika.BlockingConnection(
            pika.ConnectionParameters('localhost')
        )
        self.channel = self.connection.channel()
        
        self.channel.queue_declare(queue='email_notifications', durable=True)
        self.channel.basic_qos(prefetch_count=10)  # Can handle more emails
    
    def send_email(self, ch, method, properties, body):
        order = json.loads(body)
        routing_key = method.routing_key
        
        if routing_key == 'order.created':
            print(f"Sending order confirmation for {order['order_id']}")
        elif routing_key == 'order.paid':
            print(f"Sending payment confirmation for {order['order_id']}")
        elif routing_key == 'order.shipped':
            print(f"Sending shipping notification for {order['order_id']}")
        
        # Simulate email sending
        time.sleep(0.5)
        
        ch.basic_ack(delivery_tag=method.delivery_tag)
    
    def start(self):
        self.channel.basic_consume(
            queue='email_notifications',
            on_message_callback=self.send_email
        )
        
        print("Email worker started")
        self.channel.start_consuming()

# Usage
if __name__ == '__main__':
    # Start order service
    order_service = OrderService()
    
    # Create order
    order_service.create_order(
        customer_id='customer_123',
        items=[
            {'product_id': 'prod_1', 'price': 29.99, 'quantity': 2},
            {'product_id': 'prod_2', 'price': 49.99, 'quantity': 1}
        ]
    )
    
    order_service.close()
```

---

## 7. Implementation with AWS SQS

```python
# ============================================
# AWS SQS Producer
# ============================================
import boto3
import json

class SQSProducer:
    def __init__(self, queue_url):
        self.sqs = boto3.client('sqs', region_name='us-east-1')
        self.queue_url = queue_url
    
    def send_message(self, message_body, delay_seconds=0):
        """Send message to SQS queue"""
        response = self.sqs.send_message(
            QueueUrl=self.queue_url,
            MessageBody=json.dumps(message_body),
            DelaySeconds=delay_seconds,
            MessageAttributes={
                'Type': {
                    'StringValue': message_body.get('type', 'default'),
                    'DataType': 'String'
                }
            }
        )
        
        print(f"Message sent: {response['MessageId']}")
        return response['MessageId']
    
    def send_batch(self, messages):
        """Send up to 10 messages in batch"""
        entries = [
            {
                'Id': str(i),
                'MessageBody': json.dumps(msg)
            }
            for i, msg in enumerate(messages[:10])  # Max 10
        ]
        
        response = self.sqs.send_message_batch(
            QueueUrl=self.queue_url,
            Entries=entries
        )
        
        print(f"Batch sent: {len(response['Successful'])} successful")
        return response

# ============================================
# AWS SQS Consumer
# ============================================
class SQSConsumer:
    def __init__(self, queue_url):
        self.sqs = boto3.client('sqs', region_name='us-east-1')
        self.queue_url = queue_url
    
    def process_messages(self, handler_func, max_messages=10):
        """Poll and process messages"""
        while True:
            # Receive messages (long polling)
            response = self.sqs.receive_message(
                QueueUrl=self.queue_url,
                MaxNumberOfMessages=max_messages,
                WaitTimeSeconds=20,  # Long polling
                MessageAttributeNames=['All']
            )
            
            messages = response.get('Messages', [])
            
            if not messages:
                continue
            
            for message in messages:
                try:
                    # Parse message
                    body = json.loads(message['Body'])
                    
                    # Process message
                    handler_func(body)
                    
                    # Delete message after successful processing
                    self.sqs.delete_message(
                        QueueUrl=self.queue_url,
                        ReceiptHandle=message['ReceiptHandle']
                    )
                    
                    print(f"Message processed and deleted")
                    
                except Exception as e:
                    print(f"Error processing message: {e}")
                    # Message will become visible again after visibility timeout
    
    def process_with_visibility_timeout(self, handler_func):
        """Process with extended visibility timeout"""
        while True:
            response = self.sqs.receive_message(
                QueueUrl=self.queue_url,
                MaxNumberOfMessages=1,
                VisibilityTimeout=300,  # 5 minutes
                WaitTimeSeconds=20
            )
            
            messages = response.get('Messages', [])
            
            if not messages:
                continue
            
            message = messages[0]
            receipt_handle = message['ReceiptHandle']
            
            try:
                body = json.loads(message['Body'])
                
                # Long-running task
                handler_func(body)
                
                # Delete after success
                self.sqs.delete_message(
                    QueueUrl=self.queue_url,
                    ReceiptHandle=receipt_handle
                )
                
            except Exception as e:
                print(f"Error: {e}")
                # Message will reappear after visibility timeout

# Usage
queue_url = 'https://sqs.us-east-1.amazonaws.com/123456789/my-queue'

# Producer
producer = SQSProducer(queue_url)
producer.send_message({
    'type': 'order',
    'order_id': 'order_123',
    'customer_id': 'customer_456'
})

# Consumer
consumer = SQSConsumer(queue_url)

def handle_message(message):
    print(f"Processing: {message}")

consumer.process_messages(handle_message)
```

---

## 8. Implementation with Apache Kafka

```python
# ============================================
# Kafka Producer
# ============================================
from kafka import KafkaProducer
import json

class OrderProducer:
    def __init__(self):
        self.producer = KafkaProducer(
            bootstrap_servers=['localhost:9092'],
            value_serializer=lambda v: json.dumps(v).encode('utf-8'),
            key_serializer=lambda k: k.encode('utf-8') if k else None,
            acks='all',  # Wait for all replicas
            retries=3
        )
    
    def send_order(self, order_id, order_data):
        """Send order with key for partitioning"""
        # Messages with same key go to same partition (ordered)
        self.producer.send(
            'orders',
            key=order_data['customer_id'],  # Partition by customer
            value=order_data
        )
        
        self.producer.flush()
        print(f"Order {order_id} sent to Kafka")
    
    def close(self):
        self.producer.close()

# ============================================
# Kafka Consumer
# ============================================
from kafka import KafkaConsumer
import json

class OrderConsumer:
    def __init__(self, group_id):
        self.consumer = KafkaConsumer(
            'orders',
            bootstrap_servers=['localhost:9092'],
            group_id=group_id,
            auto_offset_reset='earliest',  # Start from beginning
            enable_auto_commit=False,  # Manual commit
            value_deserializer=lambda m: json.loads(m.decode('utf-8'))
        )
    
    def process_orders(self):
        """Process orders from Kafka"""
        for message in self.consumer:
            order = message.value
            
            print(f"Processing order: {order['order_id']}")
            print(f"Partition: {message.partition}, Offset: {message.offset}")
            
            try:
                # Process order
                self.handle_order(order)
                
                # Commit offset after successful processing
                self.consumer.commit()
                
            except Exception as e:
                print(f"Error processing order: {e}")
                # Don't commit - will reprocess on restart
    
    def handle_order(self, order):
        # Business logic
        print(f"Order {order['order_id']} processed")
    
    def close(self):
        self.consumer.close()

# ============================================
# Multiple Consumers (Consumer Group)
# ============================================

# Consumer 1
consumer1 = OrderConsumer(group_id='order-processors')
# Will get partitions 0, 1

# Consumer 2
consumer2 = OrderConsumer(group_id='order-processors')
# Will get partitions 2, 3

# Both in same group = automatic load balancing
# Messages distributed across consumers
```

---

## 9. Async Processing Patterns

### Pattern 1: Task Queue (Celery)

```python
# ============================================
# tasks.py - Define async tasks
# ============================================
from celery import Celery
import time

app = Celery('tasks', broker='redis://localhost:6379/0')

@app.task
def send_email(email, subject, body):
    """Async email sending"""
    print(f"Sending email to {email}")
    time.sleep(2)  # Simulate email sending
    print(f"Email sent to {email}")
    return f"Email sent to {email}"

@app.task
def process_image(image_url):
    """Async image processing"""
    print(f"Processing image: {image_url}")
    time.sleep(5)  # Simulate processing
    print(f"Image processed: {image_url}")
    return f"Processed: {image_url}"

@app.task(bind=True, max_retries=3)
def charge_payment(self, user_id, amount):
    """Async payment with retries"""
    try:
        print(f"Charging ${amount} to user {user_id}")
        # Payment gateway call
        return {'status': 'success', 'amount': amount}
    except Exception as e:
        # Retry with exponential backoff
        raise self.retry(exc=e, countdown=2 ** self.request.retries)

# ============================================
# app.py - Use async tasks
# ============================================
from flask import Flask, request, jsonify
from tasks import send_email, process_image, charge_payment

app = Flask(__name__)

@app.route('/register', methods=['POST'])
def register():
    data = request.json
    email = data['email']
    
    # Create user in database (synchronous)
    user = create_user(email)
    
    # Send welcome email asynchronously
    send_email.delay(email, 'Welcome!', 'Thanks for registering')
    
    # Return immediately
    return jsonify({'user_id': user.id}), 201

@app.route('/upload-image', methods=['POST'])
def upload_image():
    data = request.json
    image_url = data['image_url']
    
    # Queue image processing
    task = process_image.delay(image_url)
    
    # Return task ID immediately
    return jsonify({'task_id': task.id}), 202

@app.route('/task-status/<task_id>')
def task_status(task_id):
    """Check task status"""
    task = process_image.AsyncResult(task_id)
    
    if task.state == 'PENDING':
        response = {'state': 'PENDING', 'status': 'Task is queued'}
    elif task.state == 'SUCCESS':
        response = {'state': 'SUCCESS', 'result': task.result}
    elif task.state == 'FAILURE':
        response = {'state': 'FAILURE', 'error': str(task.info)}
    else:
        response = {'state': task.state}
    
    return jsonify(response)
```

**Start Celery Worker:**
```bash
celery -A tasks worker --loglevel=info --concurrency=4
```

### Pattern 2: Scheduled Tasks

```python
from celery import Celery
from celery.schedules import crontab

app = Celery('tasks', broker='redis://localhost:6379/0')

# Configure periodic tasks
app.conf.beat_schedule = {
    'send-daily-reports': {
        'task': 'tasks.send_daily_report',
        'schedule': crontab(hour=8, minute=0),  # Every day at 8 AM
    },
    'cleanup-old-data': {
        'task': 'tasks.cleanup_old_data',
        'schedule': crontab(hour=2, minute=0),  # Every day at 2 AM
    },
    'check-system-health': {
        'task': 'tasks.check_health',
        'schedule': 60.0,  # Every 60 seconds
    },
}

@app.task
def send_daily_report():
    print("Generating daily report...")
    # Generate report logic
    return "Report sent"

@app.task
def cleanup_old_data():
    print("Cleaning up old data...")
    # Cleanup logic
    return "Cleanup complete"

@app.task
def check_health():
    print("Checking system health...")
    # Health check logic
    return "System healthy"
```

---

## 10. Error Handling and Retries

### Exponential Backoff

```python
import time
import random

def exponential_backoff_retry(func, max_retries=5, base_delay=1):
    """Retry with exponential backoff"""
    for attempt in range(max_retries):
        try:
            return func()
        except Exception as e:
            if attempt == max_retries - 1:
                raise  # Last attempt, give up
            
            # Calculate delay: base * 2^attempt + jitter
            delay = base_delay * (2 ** attempt) + random.uniform(0, 1)
            
            print(f"Attempt {attempt + 1} failed: {e}")
            print(f"Retrying in {delay:.2f} seconds...")
            
            time.sleep(delay)

# Usage
def unreliable_api_call():
    # Simulate flaky API
    if random.random() < 0.7:
        raise Exception("API timeout")
    return "Success"

result = exponential_backoff_retry(unreliable_api_call)
print(result)
```

### Circuit Breaker for Queue Processing

```typescript
class CircuitBreaker {
    private failures: number = 0;
    private lastFailureTime: number = 0;
    private state: 'CLOSED' | 'OPEN' | 'HALF_OPEN' = 'CLOSED';
    
    constructor(
        private threshold: number = 5,
        private timeout: number = 60000  // 1 minute
    ) {}
    
    async execute<T>(fn: () => Promise<T>): Promise<T> {
        if (this.state === 'OPEN') {
            if (Date.now() - this.lastFailureTime > this.timeout) {
                this.state = 'HALF_OPEN';
            } else {
                throw new Error('Circuit breaker is OPEN');
            }
        }
        
        try {
            const result = await fn();
            this.onSuccess();
            return result;
        } catch (error) {
            this.onFailure();
            throw error;
        }
    }
    
    private onSuccess(): void {
        this.failures = 0;
        this.state = 'CLOSED';
    }
    
    private onFailure(): void {
        this.failures++;
        this.lastFailureTime = Date.now();
        
        if (this.failures >= this.threshold) {
            this.state = 'OPEN';
            console.log('Circuit breaker opened');
        }
    }
}

// Usage with message processing
const breaker = new CircuitBreaker();

async function processMessage(message: any): Promise<void> {
    await breaker.execute(async () => {
        // Process message
        await externalApiCall(message);
    });
}
```

---

## 11. Real-World Use Cases

### Use Case 1: E-Commerce Order Processing

```
User places order
     ↓
API saves order → Queue
     ↓                ↓
Return to user   Workers process:
(instant)        - Charge payment
                 - Update inventory
                 - Send confirmation email
                 - Notify warehouse
                 - Update analytics
```

### Use Case 2: Video Processing Pipeline

```
User uploads video
     ↓
API saves video → Queue: "transcode"
     ↓                ↓
Return upload ID  Worker transcodes video
                     ↓
                  Queue: "thumbnail"
                     ↓
                  Worker generates thumbnail
                     ↓
                  Queue: "notification"
                     ↓
                  Worker notifies user
```

### Use Case 3: Webhook Delivery

```
Event occurs → Queue: "webhooks"
                   ↓
               Worker delivers webhooks
               to customer endpoints
               (with retries)
```

---

## Chapter 13 Summary

### Key Concepts

1. **Message Queues** - Asynchronous communication between services
2. **Point-to-Point** - One message, one consumer
3. **Pub/Sub** - One message, multiple consumers
4. **Idempotency** - Process same message multiple times safely
5. **DLQ** - Handle permanently failed messages
6. **Competing Consumers** - Parallel processing
7. **Ordering** - Maintain message order when needed

### Technology Comparison

| Feature | RabbitMQ | AWS SQS | Kafka |
|---------|----------|---------|-------|
| **Delivery** | At-most-once, At-least-once | At-least-once | At-least-once, Exactly-once |
| **Ordering** | Queue-level | FIFO queues | Partition-level |
| **Throughput** | Medium | Medium | Very High |
| **Persistence** | Optional | Yes | Yes |
| **Retention** | Until consumed | Up to 14 days | Configurable (days to years) |
| **Use Case** | Task queues | AWS-integrated | Event streaming |

### Best Practices

1. **Make operations idempotent** - Handle duplicate messages
2. **Use DLQ** - Don't lose failed messages
3. **Monitor queue depth** - Alert on backlog
4. **Set appropriate timeouts** - Visibility/processing time
5. **Handle errors gracefully** - Retry with backoff
6. **Log message IDs** - Track message flow
7. **Use batch operations** - When possible for efficiency

### Interview Tips

**Common Questions:**
1. "Explain message queues and benefits"
2. "How do you handle duplicate messages?"
3. "What is a dead letter queue?"
4. "Pub/Sub vs Point-to-Point?"
5. "How do you ensure message ordering?"

**How to Answer:**
- Draw flow diagrams
- Explain async benefits
- Discuss idempotency
- Compare technologies
- Give real examples

### Next Steps

Chapter 14 will cover **API Design** - RESTful principles, GraphQL, gRPC, versioning, and best practices for building robust APIs.