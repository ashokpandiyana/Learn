# Chapter 35: Observability

## Table of Contents
1. Introduction to Observability
2. Logging (ELK Stack)
3. Metrics (Prometheus & Grafana)
4. Distributed Tracing (Jaeger)
5. Application Performance Monitoring (APM)
6. Log Aggregation
7. Alerting
8. SLIs, SLOs, and SLAs
9. OpenTelemetry
10. Observability in Microservices
11. Best Practices

---

## 1. Introduction to Observability

**Definition:** Observability is the ability to understand the internal state of a system by examining its outputs.

### Monitoring vs Observability

```
Monitoring (Traditional):
- Known unknowns
- Predefined dashboards
- "Is the CPU high?"
- "Is the service down?"

Observability (Modern):
- Unknown unknowns
- Ad-hoc exploration
- "Why is this request slow?"
- "What happened at 3:47 PM?"

Monitoring: Tells you WHAT is broken
Observability: Tells you WHY it's broken
```

### Three Pillars

**1. Logs** - What happened (events)
**2. Metrics** - Numerical measurements (counters, gauges)
**3. Traces** - Request journey across services

```
Complete Picture:

Metric: "API latency increased to 500ms" (WHAT)
   ↓
Log: "Database query timeout at 14:32:15" (WHEN/WHERE)
   ↓
Trace: "Request spent 450ms waiting for database" (WHY)
```

---

## 2. Logging (ELK Stack)

### Structured Logging

```javascript
// ============================================
// Structured Logging with Winston
// ============================================

const winston = require('winston');

const logger = winston.createLogger({
  level: 'info',
  format: winston.format.combine(
    winston.format.timestamp(),
    winston.format.errors({ stack: true }),
    winston.format.json()  // Structured JSON
  ),
  defaultMeta: {
    service: 'api-service',
    environment: process.env.NODE_ENV
  },
  transports: [
    // Console output
    new winston.transports.Console({
      format: winston.format.combine(
        winston.format.colorize(),
        winston.format.simple()
      )
    }),
    
    // File output
    new winston.transports.File({
      filename: '/var/log/app/error.log',
      level: 'error'
    }),
    
    new winston.transports.File({
      filename: '/var/log/app/combined.log'
    })
  ]
});

// ============================================
// Usage in Application
// ============================================

app.post('/api/orders', async (req, res) => {
  const startTime = Date.now();
  
  logger.info('Order creation started', {
    userId: req.user.id,
    correlationId: req.correlationId
  });
  
  try {
    const order = await createOrder(req.body);
    
    logger.info('Order created successfully', {
      orderId: order.id,
      userId: req.user.id,
      amount: order.total,
      duration: Date.now() - startTime,
      correlationId: req.correlationId
    });
    
    res.json(order);
    
  } catch (error) {
    logger.error('Order creation failed', {
      userId: req.user.id,
      error: error.message,
      stack: error.stack,
      duration: Date.now() - startTime,
      correlationId: req.correlationId
    });
    
    res.status(500).json({ error: 'Failed to create order' });
  }
});

// Structured log output (JSON):
// {
//   "timestamp": "2024-01-15T14:32:15.123Z",
//   "level": "info",
//   "message": "Order created successfully",
//   "service": "api-service",
//   "orderId": "ORD-123",
//   "userId": "user-456",
//   "amount": 99.99,
//   "duration": 245,
//   "correlationId": "req-abc-123"
// }
```

### Elasticsearch, Logstash, Kibana (ELK)

```yaml
# ============================================
# docker-compose.yml - ELK Stack
# ============================================

version: '3.8'

services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.6.0
    environment:
      - discovery.type=single-node
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
      - xpack.security.enabled=false
    ports:
      - "9200:9200"
    volumes:
      - elasticsearch-data:/usr/share/elasticsearch/data
  
  logstash:
    image: docker.elastic.co/logstash/logstash:8.6.0
    ports:
      - "5000:5000"
    volumes:
      - ./logstash.conf:/usr/share/logstash/pipeline/logstash.conf
    depends_on:
      - elasticsearch
  
  kibana:
    image: docker.elastic.co/kibana/kibana:8.6.0
    ports:
      - "5601:5601"
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    depends_on:
      - elasticsearch

volumes:
  elasticsearch-data:
```

**Logstash Configuration:**

```ruby
# ============================================
# logstash.conf
# ============================================

input {
  # Receive logs via TCP
  tcp {
    port => 5000
    codec => json
  }
  
  # Or read from file
  file {
    path => "/var/log/app/*.log"
    start_position => "beginning"
    codec => json
  }
}

filter {
  # Parse JSON logs
  json {
    source => "message"
  }
  
  # Add fields
  mutate {
    add_field => {
      "[@metadata][index_prefix]" => "app-logs"
    }
  }
  
  # Parse timestamp
  date {
    match => ["timestamp", "ISO8601"]
    target => "@timestamp"
  }
  
  # Grok for unstructured logs
  if [message] =~ /ERROR/ {
    grok {
      match => {
        "message" => "%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:level} %{GREEDYDATA:error_message}"
      }
    }
  }
}

output {
  # Send to Elasticsearch
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "%{[@metadata][index_prefix]}-%{+YYYY.MM.dd}"
  }
  
  # Also print to stdout (debugging)
  stdout {
    codec => rubydebug
  }
}
```

---

## 3. Metrics (Prometheus & Grafana)

### Prometheus Metrics

```javascript
// ============================================
// Prometheus Metrics in Node.js
// ============================================

const express = require('express');
const promClient = require('prom-client');

const app = express();

// Create metrics registry
const register = new promClient.Registry();

// Add default metrics (CPU, memory, etc.)
promClient.collectDefaultMetrics({ register });

// ============================================
// Custom Metrics
// ============================================

// Counter (only increases)
const httpRequestsTotal = new promClient.Counter({
  name: 'http_requests_total',
  help: 'Total HTTP requests',
  labelNames: ['method', 'path', 'status'],
  registers: [register]
});

// Histogram (distribution)
const httpRequestDuration = new promClient.Histogram({
  name: 'http_request_duration_seconds',
  help: 'HTTP request duration in seconds',
  labelNames: ['method', 'path'],
  buckets: [0.1, 0.3, 0.5, 0.7, 1, 3, 5, 7, 10],  // Buckets for latency
  registers: [register]
});

// Gauge (can go up or down)
const activeConnections = new promClient.Gauge({
  name: 'active_connections',
  help: 'Number of active connections',
  registers: [register]
});

// Summary (like histogram but with quantiles)
const paymentAmount = new promClient.Summary({
  name: 'payment_amount_dollars',
  help: 'Payment amount in dollars',
  percentiles: [0.5, 0.9, 0.95, 0.99],
  registers: [register]
});

// ============================================
// Instrument Application
// ============================================

// Middleware to track metrics
app.use((req, res, next) => {
  const start = Date.now();
  
  // Track active connections
  activeConnections.inc();
  
  // On response finished
  res.on('finish', () => {
    const duration = (Date.now() - start) / 1000;
    
    // Increment request counter
    httpRequestsTotal.labels(req.method, req.path, res.statusCode).inc();
    
    // Record request duration
    httpRequestDuration.labels(req.method, req.path).observe(duration);
    
    // Decrement active connections
    activeConnections.dec();
  });
  
  next();
});

// Business metrics
app.post('/api/payments', async (req, res) => {
  const { amount } = req.body;
  
  // Track payment amount
  paymentAmount.observe(amount);
  
  // ... process payment
});

// Metrics endpoint (Prometheus scrapes this)
app.get('/metrics', async (req, res) => {
  res.set('Content-Type', register.contentType);
  res.end(await register.metrics());
});

app.listen(3000);
```

### Prometheus Configuration

```yaml
# ============================================
# prometheus.yml
# ============================================

global:
  scrape_interval: 15s  # Scrape targets every 15 seconds
  evaluation_interval: 15s  # Evaluate rules every 15 seconds

# Alerting configuration
alerting:
  alertmanagers:
    - static_configs:
        - targets:
            - alertmanager:9093

# Scrape configurations
scrape_configs:
  # Prometheus itself
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']
  
  # API service
  - job_name: 'api-service'
    static_configs:
      - targets:
          - 'api-1:3000'
          - 'api-2:3000'
          - 'api-3:3000'
    metrics_path: '/metrics'
    scrape_interval: 10s
  
  # Kubernetes service discovery
  - job_name: 'kubernetes-pods'
    kubernetes_sd_configs:
      - role: pod
    
    relabel_configs:
      # Only scrape pods with annotation
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
        action: keep
        regex: true
      
      # Get port from annotation
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_port]
        action: replace
        target_label: __address__
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2

# Alert rules
rule_files:
  - 'alerts.yml'
```

### Prometheus Queries (PromQL)

```promql
# ============================================
# Common Prometheus Queries
# ============================================

# Request rate (requests per second)
rate(http_requests_total[5m])

# Error rate
rate(http_requests_total{status=~"5.."}[5m])

# Success rate percentage
100 - (
  rate(http_requests_total{status=~"5.."}[5m]) / 
  rate(http_requests_total[5m])
) * 100

# P95 latency
histogram_quantile(0.95, 
  rate(http_request_duration_seconds_bucket[5m])
)

# P99 latency
histogram_quantile(0.99, 
  rate(http_request_duration_seconds_bucket[5m])
)

# Average response time
avg(rate(http_request_duration_seconds_sum[5m]) / 
    rate(http_request_duration_seconds_count[5m]))

# Memory usage
container_memory_usage_bytes{pod="api-pod"}

# CPU usage
rate(container_cpu_usage_seconds_total{pod="api-pod"}[5m])

# Request rate by endpoint
sum by (path) (rate(http_requests_total[5m]))

# Top 5 slowest endpoints
topk(5, 
  histogram_quantile(0.95, 
    rate(http_request_duration_seconds_bucket[5m])
  )
)
```

### Grafana Dashboard

```json
{
  "dashboard": {
    "title": "API Service Dashboard",
    "panels": [
      {
        "title": "Request Rate",
        "targets": [{
          "expr": "sum(rate(http_requests_total[5m]))",
          "legendFormat": "Requests/sec"
        }],
        "type": "graph"
      },
      {
        "title": "Error Rate",
        "targets": [{
          "expr": "sum(rate(http_requests_total{status=~\"5..\"}[5m]))",
          "legendFormat": "Errors/sec"
        }],
        "type": "graph"
      },
      {
        "title": "P95 Latency",
        "targets": [{
          "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))",
          "legendFormat": "P95"
        }],
        "type": "graph"
      },
      {
        "title": "Active Connections",
        "targets": [{
          "expr": "active_connections",
          "legendFormat": "Connections"
        }],
        "type": "stat"
      }
    ]
  }
}
```

---

## 4. Distributed Tracing (Jaeger)

### Tracing Concepts

```
Trace: Complete request journey
  ├─ Span 1: API Gateway (50ms)
  │   ├─ Span 2: User Service (20ms)
  │   └─ Span 3: Auth Service (15ms)
  └─ Span 4: Order Service (100ms)
      ├─ Span 5: Database Query (80ms)
      └─ Span 6: Payment Service (50ms)

Total time: 150ms
Bottleneck: Database query (80ms)
```

### OpenTracing Implementation

```javascript
// ============================================
// Distributed Tracing with Jaeger
// ============================================

const { initTracer } = require('jaeger-client');
const opentracing = require('opentracing');

// Initialize tracer
const config = {
  serviceName: 'api-service',
  sampler: {
    type: 'const',
    param: 1  // Sample 100% of traces
  },
  reporter: {
    logSpans: true,
    agentHost: 'jaeger-agent',
    agentPort: 6832
  }
};

const tracer = initTracer(config);

// ============================================
// Instrument HTTP endpoint
// ============================================

app.get('/api/users/:id', async (req, res) => {
  // Start span
  const span = tracer.startSpan('get_user');
  span.setTag(opentracing.Tags.HTTP_METHOD, 'GET');
  span.setTag(opentracing.Tags.HTTP_URL, req.url);
  span.setTag('user.id', req.params.id);
  
  try {
    // Child span for database query
    const dbSpan = tracer.startSpan('database_query', {
      childOf: span
    });
    
    const user = await db.query(
      'SELECT * FROM users WHERE id = $1',
      [req.params.id]
    );
    
    dbSpan.setTag('db.statement', 'SELECT * FROM users WHERE id = $1');
    dbSpan.setTag('db.rows', user ? 1 : 0);
    dbSpan.finish();
    
    // Child span for calling order service
    const orderSpan = tracer.startSpan('get_orders', {
      childOf: span
    });
    
    const orders = await fetch(`http://order-service/orders?userId=${req.params.id}`, {
      headers: {
        // Propagate trace context
        'uber-trace-id': orderSpan.context().toString()
      }
    }).then(r => r.json());
    
    orderSpan.setTag('order.count', orders.length);
    orderSpan.finish();
    
    // Success
    span.setTag(opentracing.Tags.HTTP_STATUS_CODE, 200);
    res.json({ user, orders });
    
  } catch (error) {
    // Log error in span
    span.setTag(opentracing.Tags.ERROR, true);
    span.log({
      'event': 'error',
      'error.object': error,
      'message': error.message,
      'stack': error.stack
    });
    
    span.setTag(opentracing.Tags.HTTP_STATUS_CODE, 500);
    res.status(500).json({ error: 'Internal error' });
    
  } finally {
    span.finish();
  }
});

// Jaeger UI shows:
// - Complete request timeline
// - Time spent in each service
// - Database query duration
// - Service dependencies
// - Errors and their location
```

---

## 5. Application Performance Monitoring (APM)

### New Relic APM

```javascript
// ============================================
// New Relic APM Integration
// ============================================

// newrelic.js
exports.config = {
  app_name: ['API Service'],
  license_key: process.env.NEW_RELIC_LICENSE_KEY,
  logging: {
    level: 'info'
  },
  
  // Distributed tracing
  distributed_tracing: {
    enabled: true
  },
  
  // Custom attributes
  attributes: {
    enabled: true,
    include: ['request.*', 'response.*']
  }
};

// app.js
require('newrelic');  // Must be first line!

const express = require('express');
const app = express();

// New Relic automatically instruments:
// - HTTP requests
// - Database queries
// - External API calls
// - Errors and exceptions

app.get('/api/users/:id', async (req, res) => {
  // Custom transaction attributes
  newrelic.addCustomAttribute('userId', req.params.id);
  newrelic.addCustomAttribute('userTier', req.user?.tier);
  
  try {
    const user = await db.query('SELECT * FROM users WHERE id = $1', [req.params.id]);
    
    // Custom metric
    newrelic.recordMetric('Custom/UserLookup', 1);
    
    res.json(user);
    
  } catch (error) {
    // Error automatically captured
    newrelic.noticeError(error);
    res.status(500).json({ error: 'Internal error' });
  }
});

// New Relic dashboard shows:
// - Transaction traces
// - Slow queries
// - Error rate
// - Throughput
// - External services
// - Database performance
// - Infrastructure metrics
```

---

## 6. Log Aggregation

### Centralized Logging Architecture

```python
# ============================================
# Python Logging to Centralized System
# ============================================

import logging
import json
import socket
from pythonjsonlogger import jsonlogger

class ContextFilter(logging.Filter):
    """Add context to all log records"""
    
    def __init__(self, service_name, environment):
        super().__init__()
        self.service_name = service_name
        self.environment = environment
        self.hostname = socket.gethostname()
    
    def filter(self, record):
        record.service = self.service_name
        record.environment = self.environment
        record.hostname = self.hostname
        return True

# Configure logger
logger = logging.getLogger()
logger.setLevel(logging.INFO)

# JSON formatter
formatter = jsonlogger.JsonFormatter(
    '%(timestamp)s %(level)s %(name)s %(message)s'
)

# Console handler
console_handler = logging.StreamHandler()
console_handler.setFormatter(formatter)
logger.addHandler(console_handler)

# Logstash handler (TCP)
logstash_handler = logging.handlers.SocketHandler(
    'logstash.example.com',
    5000
)
logstash_handler.setFormatter(formatter)
logger.addHandler(logstash_handler)

# Add context filter
context_filter = ContextFilter('user-service', 'production')
logger.addFilter(context_filter)

# ============================================
# Usage
# ============================================

def process_order(order_id, user_id):
    logger.info('Processing order', extra={
        'order_id': order_id,
        'user_id': user_id
    })
    
    try:
        # Process order
        result = do_processing(order_id)
        
        logger.info('Order processed successfully', extra={
            'order_id': order_id,
            'result': result
        })
        
    except Exception as e:
        logger.error('Order processing failed', extra={
            'order_id': order_id,
            'error': str(e),
            'error_type': type(e).__name__
        }, exc_info=True)

# All logs sent to:
# - Console (for local development)
# - Logstash (for centralized aggregation)
# - Stored in Elasticsearch
# - Viewable in Kibana
```

---

## 7. Alerting

### Prometheus Alerts

```yaml
# ============================================
# alerts.yml - Prometheus Alert Rules
# ============================================

groups:
  - name: api_alerts
    interval: 30s
    rules:
      # High error rate
      - alert: HighErrorRate
        expr: |
          (
            rate(http_requests_total{status=~"5.."}[5m]) / 
            rate(http_requests_total[5m])
          ) > 0.05
        for: 5m
        labels:
          severity: critical
          service: api
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value | humanizePercentage }} (threshold: 5%)"
      
      # High latency
      - alert: HighLatency
        expr: |
          histogram_quantile(0.95, 
            rate(http_request_duration_seconds_bucket[5m])
          ) > 1
        for: 5m
        labels:
          severity: warning
          service: api
        annotations:
          summary: "High P95 latency"
          description: "P95 latency is {{ $value }}s (threshold: 1s)"
      
      # Service down
      - alert: ServiceDown
        expr: up{job="api-service"} == 0
        for: 2m
        labels:
          severity: critical
          service: api
        annotations:
          summary: "Service is down"
          description: "{{ $labels.instance }} has been down for more than 2 minutes"
      
      # High memory usage
      - alert: HighMemoryUsage
        expr: |
          (
            container_memory_usage_bytes{pod=~"api-.*"} / 
            container_spec_memory_limit_bytes{pod=~"api-.*"}
          ) > 0.9
        for: 5m
        labels:
          severity: warning
          service: api
        annotations:
          summary: "High memory usage"
          description: "Pod {{ $labels.pod }} is using {{ $value | humanizePercentage }} of memory limit"
      
      # Database connection pool exhaustion
      - alert: DatabaseConnectionPoolExhausted
        expr: db_connection_pool_active >= db_connection_pool_max
        for: 2m
        labels:
          severity: critical
          service: database
        annotations:
          summary: "Database connection pool exhausted"
          description: "All {{ $value }} connections in use"
```

### Alertmanager Configuration

```yaml
# ============================================
# alertmanager.yml
# ============================================

global:
  resolve_timeout: 5m
  
  # Slack configuration
  slack_api_url: 'https://hooks.slack.com/services/T00000000/B00000000/XXXXXXXXXXXXXXXXXXXX'

# Routing tree
route:
  receiver: 'default'
  group_by: ['alertname', 'service']
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 4h
  
  routes:
    # Critical alerts → PagerDuty
    - match:
        severity: critical
      receiver: 'pagerduty'
      group_wait: 10s
      repeat_interval: 1h
    
    # Warning alerts → Slack
    - match:
        severity: warning
      receiver: 'slack'

# Receivers
receivers:
  - name: 'default'
    slack_configs:
      - channel: '#alerts'
        title: 'Alert: {{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
  
  - name: 'slack'
    slack_configs:
      - channel: '#alerts-warning'
        title: '⚠️ {{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
        send_resolved: true
  
  - name: 'pagerduty'
    pagerduty_configs:
      - service_key: 'your-pagerduty-key'
        description: '{{ .GroupLabels.alertname }}'

# Inhibition rules (suppress alerts)
inhibit_rules:
  # If service is down, don't alert on high latency
  - source_match:
      alertname: 'ServiceDown'
    target_match:
      alertname: 'HighLatency'
    equal: ['service']
```

---

## 8. SLIs, SLOs, and SLAs

### Service Level Indicators (SLIs)

```python
# ============================================
# SLI Tracking
# ============================================

from prometheus_client import Counter, Histogram

class SLITracker:
    def __init__(self):
        # Total requests
        self.requests_total = Counter(
            'requests_total',
            'Total requests',
            ['endpoint']
        )
        
        # Successful requests (2xx, 3xx)
        self.requests_successful = Counter(
            'requests_successful',
            'Successful requests',
            ['endpoint']
        )
        
        # Request duration
        self.request_duration = Histogram(
            'request_duration_seconds',
            'Request duration',
            ['endpoint'],
            buckets=[0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0]
        )
    
    def track_request(self, endpoint, duration, status_code):
        # Increment total
        self.requests_total.labels(endpoint).inc()
        
        # Track success
        if 200 <= status_code < 400:
            self.requests_successful.labels(endpoint).inc()
        
        # Track duration
        self.request_duration.labels(endpoint).observe(duration)

# ============================================
# Calculate SLIs
# ============================================

# SLI 1: Availability
# Target: 99.9% of requests succeed

availability_sli = """
sum(rate(requests_successful[30d])) / 
sum(rate(requests_total[30d]))
"""

# SLI 2: Latency
# Target: 95% of requests complete in < 500ms

latency_sli = """
histogram_quantile(0.95, 
  rate(request_duration_seconds_bucket[30d])
)
"""

# SLI 3: Error Budget
# If SLO is 99.9% (0.1% error budget):
# - 30 days = 43,200 minutes
# - Error budget = 43.2 minutes
# - Used = time with availability < 99.9%
```

### SLO Monitoring

```yaml
# ============================================
# SLO Alert Rules
# ============================================

groups:
  - name: slo_alerts
    interval: 1m
    rules:
      # Alert if burning error budget too fast
      - alert: ErrorBudgetBurn
        expr: |
          (
            1 - (
              sum(rate(requests_successful[1h])) / 
              sum(rate(requests_total[1h]))
            )
          ) > (1 - 0.999) * 10
        labels:
          severity: critical
        annotations:
          summary: "Burning error budget 10x too fast"
          description: "At current rate, will exhaust monthly error budget in 3 days"
      
      # Alert if SLO violated
      - alert: SLOViolation
        expr: |
          (
            sum(rate(requests_successful[30d])) / 
            sum(rate(requests_total[30d]))
          ) < 0.999
        labels:
          severity: critical
        annotations:
          summary: "SLO violated (99.9% availability)"
          description: "30-day availability is {{ $value | humanizePercentage }}"
```

---

## 9. OpenTelemetry

**Definition:** Vendor-neutral standard for observability (logs, metrics, traces).

```javascript
// ============================================
// OpenTelemetry (replaces vendor-specific SDKs)
// ============================================

const { NodeTracerProvider } = require('@opentelemetry/sdk-trace-node');
const { Resource } = require('@opentelemetry/resources');
const { SemanticResourceAttributes } = require('@opentelemetry/semantic-conventions');
const { JaegerExporter } = require('@opentelemetry/exporter-jaeger');
const { PrometheusExporter } = require('@opentelemetry/exporter-prometheus');
const { registerInstrumentations } = require('@opentelemetry/instrumentation');
const { HttpInstrumentation } = require('@opentelemetry/instrumentation-http');
const { ExpressInstrumentation } = require('@opentelemetry/instrumentation-express');

// ============================================
// Setup Tracing
// ============================================

const provider = new NodeTracerProvider({
  resource: new Resource({
    [SemanticResourceAttributes.SERVICE_NAME]: 'api-service',
    [SemanticResourceAttributes.SERVICE_VERSION]: '1.0.0',
    [SemanticResourceAttributes.DEPLOYMENT_ENVIRONMENT]: 'production'
  })
});

// Export traces to Jaeger
const jaegerExporter = new JaegerExporter({
  endpoint: 'http://jaeger:14268/api/traces'
});

provider.addSpanProcessor(
  new BatchSpanProcessor(jaegerExporter)
);

provider.register();

// ============================================
// Setup Metrics
// ============================================

const { MeterProvider } = require('@opentelemetry/sdk-metrics');

const meterProvider = new MeterProvider();

const prometheusExporter = new PrometheusExporter(
  { port: 9464 },
  () => {
    console.log('Prometheus metrics available at http://localhost:9464/metrics');
  }
);

meterProvider.addMetricReader(prometheusExporter);

// ============================================
// Auto-instrumentation
// ============================================

registerInstrumentations({
  instrumentations: [
    new HttpInstrumentation(),
    new ExpressInstrumentation(),
    // Automatically instruments HTTP requests, Express routes
  ]
});

// ============================================
// Application Code (No changes needed!)
// ============================================

const express = require('express');
const app = express();

app.get('/api/users/:id', async (req, res) => {
  // Automatically traced!
  const user = await db.query('SELECT * FROM users WHERE id = $1', [req.params.id]);
  res.json(user);
});

// OpenTelemetry automatically:
// - Creates traces
// - Collects metrics
// - Propagates context
// - Exports to Jaeger, Prometheus
```

---

## 10. Observability in Microservices

### Correlation IDs

```javascript
// ============================================
// Correlation ID Middleware
// ============================================

const { v4: uuidv4 } = require('uuid');

function correlationIdMiddleware(req, res, next) {
  // Get correlation ID from header or generate new one
  const correlationId = req.headers['x-correlation-id'] || uuidv4();
  
  // Attach to request
  req.correlationId = correlationId;
  
  // Add to response headers
  res.setHeader('x-correlation-id', correlationId);
  
  // Add to logger context
  req.log = logger.child({ correlationId });
  
  next();
}

app.use(correlationIdMiddleware);

// ============================================
// Propagate Correlation ID
// ============================================

app.get('/api/users/:id', async (req, res) => {
  req.log.info('Fetching user');
  
  // Call another service (propagate correlation ID)
  const orders = await fetch('http://order-service/orders', {
    headers: {
      'x-correlation-id': req.correlationId  // Propagate!
    }
  });
  
  req.log.info('User fetched successfully');
  
  res.json({ user, orders });
});

// Now can trace request across all services:
// API Service (correlation-id: abc-123)
//   → Order Service (correlation-id: abc-123)
//     → Payment Service (correlation-id: abc-123)
//       → Database (correlation-id: abc-123)
//
// Search logs for "abc-123" shows complete request flow
```

---

## Chapter 35 Summary

### Key Concepts

1. **Observability** - Understand system from outputs
2. **Three Pillars** - Logs, Metrics, Traces
3. **Logging** - ELK stack (Elasticsearch, Logstash, Kibana)
4. **Metrics** - Prometheus + Grafana
5. **Tracing** - Jaeger, Zipkin
6. **APM** - New Relic, DataDog
7. **SLIs/SLOs** - Service level tracking
8. **OpenTelemetry** - Vendor-neutral standard
9. **Correlation IDs** - Track requests across services
10. **Alerting** - Proactive issue detection

### Observability Stack

| Component | Tool | Purpose |
|-----------|------|---------|
| **Logs** | ELK, Loki | Event history |
| **Metrics** | Prometheus | Health indicators |
| **Traces** | Jaeger, Zipkin | Request flow |
| **Dashboards** | Grafana | Visualization |
| **Alerts** | Alertmanager, PagerDuty | Notifications |
| **APM** | New Relic, DataDog | All-in-one |

### Metrics Types

| Type | Behavior | Use Case | Example |
|------|----------|----------|---------|
| **Counter** | Only increases | Total events | http_requests_total |
| **Gauge** | Up/down | Current state | memory_usage_bytes |
| **Histogram** | Distribution | Latency | request_duration_seconds |
| **Summary** | Quantiles | Percentiles | payment_amount |

### Interview Tips

**Common Questions:**
1. "Explain the three pillars of observability"
2. "Logs vs Metrics vs Traces?"
3. "How do you monitor microservices?"
4. "What is distributed tracing?"

**How to Answer:**
- Define observability (understanding from outputs)
- Explain each pillar with examples
- Draw distributed tracing flow
- Mention specific tools (Prometheus, Jaeger, ELK)
- Discuss correlation IDs for request tracking

### Best Practices

1. **Structured logging** - JSON format
2. **Correlation IDs** - Track across services
3. **Sample traces** - Not all (performance)
4. **Set alerts** - Based on SLOs
5. **Dashboard hierarchy** - Overview → Service → Details
6. **Retention policies** - Balance cost vs need
7. **Tag metrics** - Enable filtering
8. **Document SLOs** - Clear targets

### Next Steps

Chapter 36 will cover **Domain-Driven Design (DDD) Strategic Design** - bounded contexts, ubiquitous language, context mapping, and modeling complex business domains.