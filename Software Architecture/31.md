# Chapter 31: Big Data Architecture

## Table of Contents
1. Introduction to Big Data
2. Lambda Architecture
3. Kappa Architecture
4. Hadoop Ecosystem
5. Apache Spark
6. Data Lake Architecture
7. Stream Processing with Flink
8. Data Partitioning Strategies
9. Scalable Storage Solutions
10. Real-World Big Data Systems
11. Choosing the Right Architecture

---

## 1. Introduction to Big Data

**Definition:** Big Data refers to datasets that are so large or complex that traditional data processing applications are inadequate.

### The 5 Vs of Big Data

**1. Volume**
```
Small Data: GB to TB
Big Data: TB to PB to EB

Example:
- Facebook: 4 PB of data generated daily
- Netflix: 1 PB of video data watched daily
- Twitter: 500 million tweets daily
```

**2. Velocity**
```
How fast data arrives:
- Batch: Hours to days
- Real-time: Milliseconds to seconds

Example:
- Stock market: Millions of trades per second
- IoT sensors: Billions of readings per hour
```

**3. Variety**
```
Different data types:
- Structured: SQL databases
- Semi-structured: JSON, XML
- Unstructured: Text, images, videos
```

**4. Veracity**
```
Data quality and trustworthiness:
- Incomplete data
- Inaccurate data
- Inconsistent data
```

**5. Value**
```
Extract insights from data:
- Predictive analytics
- Pattern discovery
- Decision making
```

---

## 2. Lambda Architecture

**Definition:** Hybrid architecture combining batch and stream processing.

### Components

```
1. Batch Layer (Hadoop/Spark):
   - Process ALL data
   - Complete and accurate
   - High latency (hours)
   - Creates batch views

2. Speed Layer (Storm/Flink):
   - Process recent data
   - Fast but approximate
   - Low latency (seconds)
   - Creates real-time views

3. Serving Layer:
   - Merges batch + speed views
   - Answers queries
```

### Flow

```
Data Source → Immutable append-only log
              ↓
      ┌───────┴───────┐
      ↓               ↓
Batch Layer      Speed Layer
(complete)       (recent)
      ↓               ↓
Batch Views    Real-time Views
      └───────┬───────┘
              ↓
        Serving Layer
              ↓
           Query
```

### Implementation

```python
# ============================================
# Lambda Architecture Implementation
# ============================================

class LambdaArchitecture:
    def __init__(self):
        self.batch_processor = BatchProcessor()
        self.speed_processor = SpeedProcessor()
        self.serving_layer = ServingLayer()
    
    # ==================
    # BATCH LAYER
    # ==================
    
    class BatchProcessor:
        def process_batch(self, start_date, end_date):
            """Process historical data (Spark)"""
            from pyspark.sql import SparkSession
            
            spark = SparkSession.builder.appName("BatchLayer").getOrCreate()
            
            # Read all data in date range
            df = spark.read.parquet(f"s3://data-lake/events/")
            
            # Filter date range
            df = df.filter(
                (col("timestamp") >= start_date) &
                (col("timestamp") <= end_date)
            )
            
            # Aggregate
            df_summary = df.groupBy("user_id", "date") \
                .agg(
                    count("event_id").alias("event_count"),
                    sum("revenue").alias("total_revenue")
                )
            
            # Write to batch view
            df_summary.write \
                .mode("overwrite") \
                .partitionBy("date") \
                .parquet("s3://batch-views/user_stats/")
            
            print(f"Batch processing complete: {start_date} to {end_date}")
    
    # ==================
    # SPEED LAYER
    # ==================
    
    class SpeedProcessor:
        def process_stream(self):
            """Process real-time data (Kafka Streams)"""
            from kafka import KafkaConsumer
            
            consumer = KafkaConsumer(
                'events',
                bootstrap_servers=['kafka:9092'],
                group_id='speed-layer'
            )
            
            # In-memory aggregation (last hour)
            stats = {}
            
            for message in consumer:
                event = json.loads(message.value)
                
                user_id = event['user_id']
                
                if user_id not in stats:
                    stats[user_id] = {
                        'event_count': 0,
                        'total_revenue': 0
                    }
                
                stats[user_id]['event_count'] += 1
                stats[user_id]['total_revenue'] += event.get('revenue', 0)
                
                # Write to real-time view (Redis)
                redis.setex(
                    f"realtime:user:{user_id}",
                    3600,  # 1 hour TTL
                    json.dumps(stats[user_id])
                )
    
    # ==================
    # SERVING LAYER
    # ==================
    
    class ServingLayer:
        def query_user_stats(self, user_id, date=None):
            """Query merging batch + speed views"""
            
            # Get batch view (complete history)
            if date:
                batch_stats = self.get_batch_view(user_id, date)
            else:
                batch_stats = {'event_count': 0, 'total_revenue': 0}
            
            # Get speed view (last hour)
            realtime_stats = self.get_realtime_view(user_id)
            
            # Merge views
            total_stats = {
                'event_count': batch_stats['event_count'] + realtime_stats['event_count'],
                'total_revenue': batch_stats['total_revenue'] + realtime_stats['total_revenue']
            }
            
            return total_stats
        
        def get_batch_view(self, user_id, date):
            """Read from batch view (S3/Parquet)"""
            df = pd.read_parquet(
                f"s3://batch-views/user_stats/date={date}/"
            )
            
            user_data = df[df['user_id'] == user_id]
            
            if user_data.empty:
                return {'event_count': 0, 'total_revenue': 0}
            
            return user_data.iloc[0].to_dict()
        
        def get_realtime_view(self, user_id):
            """Read from real-time view (Redis)"""
            data = redis.get(f"realtime:user:{user_id}")
            
            if data:
                return json.loads(data)
            
            return {'event_count': 0, 'total_revenue': 0}

# Usage
lambda_arch = LambdaArchitecture()

# Batch layer runs nightly
lambda_arch.batch_processor.process_batch(
    start_date='2024-01-01',
    end_date='2024-01-15'
)

# Speed layer runs continuously
# (in separate process)

# Query combines both
stats = lambda_arch.serving_layer.query_user_stats('user_123')
print(f"Total events: {stats['event_count']}")
print(f"Total revenue: ${stats['total_revenue']}")
```

### Lambda Architecture Pros/Cons

**Pros:**
- ✅ Accurate results (batch layer)
- ✅ Real-time results (speed layer)
- ✅ Fault-tolerant (immutable data)

**Cons:**
- ❌ Complex (maintain two pipelines)
- ❌ Duplicate logic (batch + speed)
- ❌ Eventual consistency between layers

---

## 3. Kappa Architecture

**Definition:** Simplified architecture - everything is a stream.

### Key Idea

```
Why process data twice (batch + stream)?
Just process everything as a stream!

Kappa:
Data Source → Stream Processor → Serving Layer

Reprocessing:
Just replay the stream from beginning
```

### Implementation

```python
# ============================================
# Kappa Architecture with Kafka
# ============================================

from kafka import KafkaConsumer, KafkaProducer
import json

class KappaArchitecture:
    def __init__(self):
        self.consumer = KafkaConsumer(
            'events',
            bootstrap_servers=['kafka:9092'],
            group_id='kappa-processor',
            auto_offset_reset='earliest',  # Can replay from beginning
            enable_auto_commit=False
        )
        
        self.producer = KafkaProducer(
            bootstrap_servers=['kafka:9092'],
            value_serializer=lambda v: json.dumps(v).encode('utf-8')
        )
    
    def process_stream(self):
        """Single stream processing pipeline"""
        
        # State store (can be rebuilt by replaying stream)
        user_stats = {}
        
        for message in self.consumer:
            event = json.loads(message.value)
            
            # Process event
            user_id = event['user_id']
            
            if user_id not in user_stats:
                user_stats[user_id] = {
                    'event_count': 0,
                    'total_revenue': 0,
                    'last_updated': None
                }
            
            user_stats[user_id]['event_count'] += 1
            user_stats[user_id]['total_revenue'] += event.get('revenue', 0)
            user_stats[user_id]['last_updated'] = event['timestamp']
            
            # Emit to output stream
            self.producer.send('user-stats', {
                'user_id': user_id,
                'stats': user_stats[user_id]
            })
            
            # Commit offset
            self.consumer.commit()
        
        # To reprocess historical data:
        # 1. Create new consumer group
        # 2. Reset offset to beginning
        # 3. Replay entire stream
        # 4. State rebuilt from scratch

# Advantages over Lambda:
# - Single codebase
# - Simpler architecture
# - Consistent results
# - Easy reprocessing (replay stream)
```

---

## 4. Hadoop Ecosystem

### HDFS (Hadoop Distributed File System)

**Architecture:**
```
NameNode (Master):
- Manages file system metadata
- Knows which DataNodes have which blocks

DataNodes (Workers):
- Store actual data blocks
- Default: 3 replicas per block
- Block size: 128 MB (default)

File stored as blocks:
File (1 GB) → 8 blocks × 128 MB
Each block replicated 3x
Total storage: 3 GB (with replication)
```

**Usage:**

```bash
# ============================================
# HDFS Commands
# ============================================

# Upload file to HDFS
hdfs dfs -put local-file.txt /user/data/

# List files
hdfs dfs -ls /user/data/

# Download file
hdfs dfs -get /user/data/file.txt ./

# Create directory
hdfs dfs -mkdir -p /user/data/year=2024/month=01/

# Delete file
hdfs dfs -rm /user/data/old-file.txt

# Check file replication
hdfs fsck /user/data/file.txt -files -blocks -locations
```

### MapReduce

**Paradigm:** Map (transform) then Reduce (aggregate)

```python
# ============================================
# MapReduce Word Count Example
# ============================================

# Input: Large text files

# MAP Phase:
def mapper(line):
    """Emit (word, 1) for each word"""
    words = line.split()
    for word in words:
        emit(word, 1)

# Input:  "hello world hello"
# Output: ("hello", 1), ("world", 1), ("hello", 1)

# SHUFFLE Phase (automatic):
# Group by key:
# "hello" → [1, 1]
# "world" → [1]

# REDUCE Phase:
def reducer(word, counts):
    """Sum counts for each word"""
    total = sum(counts)
    emit(word, total)

# Input:  ("hello", [1, 1])
# Output: ("hello", 2)

# Input:  ("world", [1])
# Output: ("world", 1)

# Final result:
# hello: 2
# world: 1
```

**Real MapReduce Implementation:**

```python
# ============================================
# MapReduce with mrjob (Python)
# ============================================

from mrjob.job import MRJob
from mrjob.step import MRStep

class PageViewAnalysis(MRJob):
    
    def mapper(self, _, line):
        """Parse log line and emit (page, 1)"""
        # Log format: timestamp | user_id | page | duration
        parts = line.split('|')
        
        if len(parts) >= 3:
            page = parts[2].strip()
            yield (page, 1)
    
    def combiner(self, page, counts):
        """Combine partial results (optimization)"""
        yield (page, sum(counts))
    
    def reducer(self, page, counts):
        """Final aggregation"""
        total_views = sum(counts)
        yield (page, total_views)

if __name__ == '__main__':
    PageViewAnalysis.run()

# Run on Hadoop cluster:
# python pageview_analysis.py input.txt -r hadoop --output-dir hdfs:///output/
```

---

## 5. Apache Spark

**Definition:** Fast and general-purpose cluster computing system.

### Spark Architecture

```
Driver Program:
- Main program
- Creates SparkContext
- Defines transformations

Cluster Manager:
- YARN, Mesos, or Standalone
- Manages resources

Executors:
- Run on worker nodes
- Execute tasks
- Cache data
```

### Spark Example

```python
# ============================================
# Apache Spark Data Processing
# ============================================

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.window import Window

# Create Spark session
spark = SparkSession.builder \
    .appName("Sales Analysis") \
    .config("spark.executor.memory", "4g") \
    .config("spark.executor.cores", "2") \
    .getOrCreate()

# ============================================
# Read Data
# ============================================

# Read from multiple sources
df_orders = spark.read \
    .format("parquet") \
    .load("s3://data-lake/orders/")

df_customers = spark.read \
    .format("json") \
    .load("s3://data-lake/customers/")

df_products = spark.read \
    .format("jdbc") \
    .option("url", "jdbc:postgresql://db:5432/production") \
    .option("dbtable", "products") \
    .option("user", "spark") \
    .option("password", "secret") \
    .load()

# ============================================
# Transform Data
# ============================================

# Join datasets
df_enriched = df_orders \
    .join(df_customers, "customer_id", "left") \
    .join(df_products, "product_id", "left")

# Add calculated columns
df_enriched = df_enriched \
    .withColumn("revenue", col("quantity") * col("price")) \
    .withColumn("year", year(col("order_date"))) \
    .withColumn("month", month(col("order_date"))) \
    .withColumn("day_of_week", dayofweek(col("order_date")))

# Filter
df_enriched = df_enriched.filter(col("revenue") > 0)

# ============================================
# Aggregate
# ============================================

# Daily summary
df_daily = df_enriched \
    .groupBy("year", "month", "day_of_week") \
    .agg(
        count("order_id").alias("order_count"),
        sum("revenue").alias("total_revenue"),
        avg("revenue").alias("avg_order_value"),
        countDistinct("customer_id").alias("unique_customers")
    ) \
    .orderBy("year", "month", "day_of_week")

# Product performance
df_products = df_enriched \
    .groupBy("product_id", "product_name", "category") \
    .agg(
        sum("quantity").alias("units_sold"),
        sum("revenue").alias("total_revenue")
    ) \
    .orderBy(desc("total_revenue"))

# Customer segmentation (RFM)
df_rfm = df_enriched \
    .groupBy("customer_id") \
    .agg(
        max("order_date").alias("last_order_date"),
        count("order_id").alias("frequency"),
        sum("revenue").alias("monetary")
    )

# Add recency (days since last order)
df_rfm = df_rfm \
    .withColumn("recency", datediff(current_date(), col("last_order_date")))

# ============================================
# Window Functions
# ============================================

# Running total
window_spec = Window.partitionBy("customer_id").orderBy("order_date")

df_with_running_total = df_enriched \
    .withColumn("running_total", sum("revenue").over(window_spec))

# Rank products by revenue per category
window_spec = Window.partitionBy("category").orderBy(desc("revenue"))

df_ranked = df_enriched \
    .withColumn("rank", row_number().over(window_spec)) \
    .filter(col("rank") <= 10)  # Top 10 per category

# ============================================
# Write Results
# ============================================

# Write to S3 (partitioned)
df_daily.write \
    .mode("overwrite") \
    .partitionBy("year", "month") \
    .parquet("s3://data-warehouse/daily_summary/")

# Write to database
df_rfm.write \
    .format("jdbc") \
    .option("url", "jdbc:postgresql://warehouse:5432/analytics") \
    .option("dbtable", "customer_rfm") \
    .option("user", "spark") \
    .option("password", "secret") \
    .mode("overwrite") \
    .save()

# Stop Spark
spark.stop()

# Spark Benefits:
# - 100x faster than Hadoop MapReduce (in-memory)
# - Easy to use (DataFrame API like Pandas)
# - Supports SQL, streaming, ML, graph processing
# - Scales to petabytes
```

---

## 6. Data Lake Architecture

### Modern Data Lake (AWS)

```python
# ============================================
# Data Lake Architecture on AWS
# ============================================

class DataLakeArchitecture:
    def __init__(self):
        self.s3 = boto3.client('s3')
        self.glue = boto3.client('glue')
        self.athena = boto3.client('athena')
        self.bucket = 'my-data-lake'
    
    def ingest_data(self, source, data, partition_keys):
        """Ingest data into data lake (partitioned)"""
        
        # Organize by source and partitions
        # Example: s3://my-data-lake/raw/orders/year=2024/month=01/day=15/
        
        partition_path = '/'.join([
            f"{k}={v}" for k, v in partition_keys.items()
        ])
        
        key = f"raw/{source}/{partition_path}/data_{int(time.time())}.parquet"
        
        # Convert to Parquet (columnar format, compressed)
        import pandas as pd
        df = pd.DataFrame(data)
        
        parquet_buffer = df.to_parquet()
        
        # Upload to S3
        self.s3.put_object(
            Bucket=self.bucket,
            Key=key,
            Body=parquet_buffer
        )
        
        print(f"Ingested to: s3://{self.bucket}/{key}")
        
        # Update Glue Data Catalog
        self.update_catalog(source, partition_keys)
    
    def update_catalog(self, table_name, partition_keys):
        """Update Glue Data Catalog (metadata)"""
        
        # Add partition to table
        self.glue.create_partition(
            DatabaseName='datalake',
            TableName=table_name,
            PartitionInput={
                'Values': list(partition_keys.values()),
                'StorageDescriptor': {
                    'Columns': self.get_schema(table_name),
                    'Location': f"s3://{self.bucket}/raw/{table_name}/{'/'.join([f'{k}={v}' for k, v in partition_keys.items()])}/",
                    'InputFormat': 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat',
                    'OutputFormat': 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat',
                    'SerdeInfo': {
                        'SerializationLibrary': 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'
                    }
                }
            }
        )
    
    def query_with_athena(self, sql):
        """Query data lake using Athena (serverless SQL)"""
        
        response = self.athena.start_query_execution(
            QueryString=sql,
            QueryExecutionContext={'Database': 'datalake'},
            ResultConfiguration={
                'OutputLocation': f's3://{self.bucket}/athena-results/'
            }
        )
        
        query_id = response['QueryExecutionId']
        
        # Wait for completion
        while True:
            status = self.athena.get_query_execution(
                QueryExecutionId=query_id
            )
            
            state = status['QueryExecution']['Status']['State']
            
            if state == 'SUCCEEDED':
                break
            elif state in ['FAILED', 'CANCELLED']:
                raise Exception(f"Query failed: {state}")
            
            time.sleep(1)
        
        # Get results
        results = self.athena.get_query_results(
            QueryExecutionId=query_id
        )
        
        return results

# Usage
lake = DataLakeArchitecture()

# Ingest data (partitioned by date)
lake.ingest_data(
    source='orders',
    data=[
        {'order_id': 1, 'total': 100, 'customer_id': 'C1'},
        {'order_id': 2, 'total': 200, 'customer_id': 'C2'}
    ],
    partition_keys={'year': 2024, 'month': 1, 'day': 15}
)

# Query with SQL (Athena)
results = lake.query_with_athena("""
    SELECT 
        year,
        month,
        SUM(total) as revenue,
        COUNT(*) as order_count
    FROM orders
    WHERE year = 2024
    GROUP BY year, month
""")

# Athena queries data directly in S3
# No need to load into database
# Pay per query (serverless)
```

---

## 7. Stream Processing with Flink

**Definition:** Apache Flink is a framework for stateful computations over streams.

```python
# ============================================
# Apache Flink Stream Processing (PyFlink)
# ============================================

from pyflink.datastream import StreamExecutionEnvironment
from pyflink.table import StreamTableEnvironment, EnvironmentSettings

# Create execution environment
env = StreamExecutionEnvironment.get_execution_environment()
env.set_parallelism(4)  # 4 parallel tasks

# Table environment for SQL
settings = EnvironmentSettings.new_instance() \
    .in_streaming_mode() \
    .build()

table_env = StreamTableEnvironment.create(env, settings)

# ============================================
# Define source (Kafka)
# ============================================

table_env.execute_sql("""
    CREATE TABLE orders (
        order_id BIGINT,
        customer_id STRING,
        product_id STRING,
        quantity INT,
        price DOUBLE,
        order_time TIMESTAMP(3),
        WATERMARK FOR order_time AS order_time - INTERVAL '5' SECOND
    ) WITH (
        'connector' = 'kafka',
        'topic' = 'orders',
        'properties.bootstrap.servers' = 'localhost:9092',
        'format' = 'json'
    )
""")

# ============================================
# Stream Processing with SQL
# ============================================

# Real-time aggregation (1-minute tumbling window)
table_env.execute_sql("""
    CREATE TABLE order_stats (
        window_start TIMESTAMP(3),
        window_end TIMESTAMP(3),
        order_count BIGINT,
        total_revenue DOUBLE
    ) WITH (
        'connector' = 'kafka',
        'topic' = 'order-stats',
        'properties.bootstrap.servers' = 'localhost:9092',
        'format' = 'json'
    )
""")

table_env.execute_sql("""
    INSERT INTO order_stats
    SELECT
        TUMBLE_START(order_time, INTERVAL '1' MINUTE) as window_start,
        TUMBLE_END(order_time, INTERVAL '1' MINUTE) as window_end,
        COUNT(*) as order_count,
        SUM(quantity * price) as total_revenue
    FROM orders
    GROUP BY TUMBLE(order_time, INTERVAL '1' MINUTE)
""")

# ============================================
# Complex Event Processing
# ============================================

# Detect pattern: 3 orders from same user within 5 minutes
table_env.execute_sql("""
    SELECT *
    FROM orders
    MATCH_RECOGNIZE (
        PARTITION BY customer_id
        ORDER BY order_time
        MEASURES
            FIRST(order_id) as first_order,
            LAST(order_id) as last_order,
            COUNT(*) as order_count
        PATTERN (A B C)
        DEFINE
            A AS TRUE,
            B AS B.order_time <= A.order_time + INTERVAL '5' MINUTE,
            C AS C.order_time <= A.order_time + INTERVAL '5' MINUTE
    )
""")

# Execute
env.execute("Flink Order Processing")

# Flink Features:
# - Exactly-once semantics
# - Event time processing
# - Windowing (tumbling, sliding, session)
# - Stateful operations
# - Low latency (milliseconds)
```

---

## 8. Data Partitioning Strategies

### Partitioning for Performance

```sql
-- ============================================
-- Partition by Date (Most Common)
-- ============================================

-- Hive table partitioned by date
CREATE EXTERNAL TABLE orders (
    order_id BIGINT,
    customer_id STRING,
    total DOUBLE
)
PARTITIONED BY (
    year INT,
    month INT,
    day INT
)
STORED AS PARQUET
LOCATION 's3://data-lake/orders/';

-- Data organized on disk:
-- s3://data-lake/orders/year=2024/month=01/day=01/
-- s3://data-lake/orders/year=2024/month=01/day=02/
-- ...

-- Query only scans relevant partitions
SELECT SUM(total) FROM orders
WHERE year = 2024 AND month = 1 AND day = 15;

-- Only reads: s3://data-lake/orders/year=2024/month=01/day=15/
-- Doesn't scan entire dataset!

-- Add partition
ALTER TABLE orders ADD PARTITION (year=2024, month=1, day=16)
LOCATION 's3://data-lake/orders/year=2024/month=01/day=16/';
```

### Bucketing (Hash Partitioning)

```sql
-- ============================================
-- Bucketing for Join Optimization
-- ============================================

CREATE TABLE orders (
    order_id BIGINT,
    customer_id STRING,
    total DOUBLE
)
CLUSTERED BY (customer_id) INTO 256 BUCKETS
STORED AS PARQUET;

CREATE TABLE customers (
    customer_id STRING,
    name STRING,
    email STRING
)
CLUSTERED BY (customer_id) INTO 256 BUCKETS
STORED AS PARQUET;

-- Join on bucketed column (very fast)
SELECT 
    o.order_id,
    c.name,
    o.total
FROM orders o
JOIN customers c ON o.customer_id = c.customer_id;

-- Spark knows both tables bucketed on same column
-- No shuffle needed for join!
-- Co-located data in same buckets
```

---

## 9. Scalable Storage Solutions

### Columnar Storage (Parquet)

```python
# ============================================
# Parquet vs CSV
# ============================================

import pandas as pd

# Sample data
df = pd.DataFrame({
    'id': range(1000000),
    'name': ['User' + str(i) for i in range(1000000)],
    'email': ['user' + str(i) + '@example.com' for i in range(1000000)],
    'age': [random.randint(18, 80) for _ in range(1000000)],
    'balance': [random.uniform(0, 10000) for _ in range(1000000)]
})

# Save as CSV
df.to_csv('data.csv', index=False)
# File size: ~80 MB

# Save as Parquet
df.to_parquet('data.parquet', compression='snappy')
# File size: ~12 MB (85% smaller!)

# Query performance
import time

# CSV: Read entire file
start = time.time()
df_csv = pd.read_csv('data.csv')
result = df_csv[df_csv['age'] > 50]['balance'].sum()
csv_time = time.time() - start
# Time: 2.5 seconds

# Parquet: Read only needed columns
start = time.time()
df_parquet = pd.read_parquet('data.parquet', columns=['age', 'balance'])
result = df_parquet[df_parquet['age'] > 50]['balance'].sum()
parquet_time = time.time() - start
# Time: 0.3 seconds (8x faster!)

print(f"CSV: {csv_time:.2f}s, Parquet: {parquet_time:.2f}s")
```

**Why Parquet is Better:**
- Columnar format (read only needed columns)
- Built-in compression
- Efficient encoding
- Metadata for filtering
- Native support in Spark, Hive, Presto

---

## Chapter 31 Summary

### Key Concepts

1. **Lambda Architecture** - Batch + Speed layers
2. **Kappa Architecture** - Stream-only processing
3. **Hadoop** - Distributed storage (HDFS) + processing (MapReduce)
4. **Spark** - Fast in-memory processing
5. **Data Lake** - Store raw data, query with Athena
6. **Stream Processing** - Flink, Kafka Streams for real-time
7. **Partitioning** - Organize data for fast queries
8. **Columnar Storage** - Parquet for analytics

### Architecture Comparison

| Architecture | Complexity | Latency | Use Case |
|--------------|------------|---------|----------|
| **Lambda** | High | Batch: Hours, Speed: Seconds | Need both accurate and fast |
| **Kappa** | Medium | Seconds | Everything real-time |
| **Batch Only** | Low | Hours | Accuracy over speed |

### Big Data Technologies

| Component | Options | Best For |
|-----------|---------|----------|
| **Storage** | HDFS, S3, GCS | Raw data |
| **Batch** | Spark, Hadoop | Historical analysis |
| **Stream** | Flink, Kafka Streams | Real-time processing |
| **Query** | Athena, Presto, Hive | Ad-hoc SQL |
| **Warehouse** | Redshift, Snowflake, BigQuery | Structured analytics |

### When to Use Big Data Architecture

✅ **Use when:**
- Data > 1 TB
- Need to process petabytes
- Multiple data sources
- Both batch and real-time needed
- Data science/ML at scale

❌ **Don't use when:**
- Data < 100 GB
- Simple analytics
- Traditional database sufficient
- Small team (complexity overhead)

### Interview Tips

**Common Questions:**
1. "Explain Lambda architecture"
2. "Hadoop vs Spark - differences?"
3. "When to use stream vs batch processing?"
4. "How do you design a data lake?"

**How to Answer:**
- Draw architecture diagrams (Lambda, Kappa)
- Explain batch vs speed layers
- Discuss trade-offs (complexity vs capability)
- Mention specific tools (Spark, Flink, Kafka)
- Give scale numbers (petabytes, billions of records)

### Next Steps

Chapter 32 will cover **CI/CD Pipelines** - automating build, test, and deployment processes for continuous integration and continuous delivery.