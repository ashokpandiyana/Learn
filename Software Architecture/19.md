# Chapter 19: High Availability Design

## Table of Contents
1. Introduction to High Availability
2. Redundancy and Replication
3. Failover Strategies
4. Health Checks and Monitoring
5. SLA, SLO, and SLI
6. Multi-Region Deployment
7. Active-Active vs Active-Passive
8. Split-Brain Problem
9. Disaster Recovery
10. Implementation Examples
11. Testing for High Availability

---

## 1. Introduction to High Availability

**Definition:** High Availability (HA) is the ability of a system to remain operational and accessible even when components fail.

### The Nine Nines

```
Availability %   Downtime/Year   Downtime/Month   Downtime/Week
90%             36.5 days        72 hours         16.8 hours
95%             18.25 days       36 hours         8.4 hours
99%             3.65 days        7.2 hours        1.68 hours
99.9%           8.76 hours       43.2 minutes     10.1 minutes
99.99%          52.6 minutes     4.32 minutes     1.01 minutes
99.999%         5.26 minutes     25.9 seconds     6.05 seconds
99.9999%        31.5 seconds     2.59 seconds     0.605 seconds
```

**Cost of Downtime:**

```
E-commerce site earning $100,000/day:

99% uptime:
- Downtime: 3.65 days/year
- Lost revenue: $365,000/year

99.9% uptime:
- Downtime: 8.76 hours/year
- Lost revenue: $36,500/year

99.99% uptime:
- Downtime: 52.6 minutes/year
- Lost revenue: $3,650/year

Each additional 9 costs more to achieve!
```

### Calculating Availability

```
Component Availability = (Total Time - Downtime) / Total Time

System Availability (Serial):
A = A‚ÇÅ √ó A‚ÇÇ √ó A‚ÇÉ √ó ... √ó A‚Çô

Example:
Load Balancer: 99.99%
App Servers: 99.9%
Database: 99.99%

System = 0.9999 √ó 0.999 √ó 0.9999 = 0.9988 = 99.88%

System Availability (Parallel/Redundant):
A = 1 - (1-A‚ÇÅ) √ó (1-A‚ÇÇ) √ó ... √ó (1-A‚Çô)

Example (2 app servers):
Each server: 99.9%
System = 1 - (1-0.999)¬≤ = 1 - 0.000001 = 99.9999%
```

---

## 2. Redundancy and Replication

**Definition:** Eliminate single points of failure by duplicating critical components.

### Component Redundancy

```
Single Point of Failure:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Client   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇLoad       ‚îÇ ‚Üê If this fails, system down!
‚îÇBalancer   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇApp Server ‚îÇ ‚Üê If this fails, system down!
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Database  ‚îÇ ‚Üê If this fails, system down!
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

High Availability:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Client   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇLoad       ‚îÇ  ‚îÇLoad      ‚îÇ ‚Üê Redundant
‚îÇBalancer 1 ‚îÇ  ‚îÇBalancer 2‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îê
‚îÇApp 1 ‚îÇ ‚îÇApp 2 ‚îÇ ‚îÇApp 3 ‚îÇ ‚îÇApp 4 ‚îÇ ‚Üê Redundant
‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇPrimary DB ‚îÇ  ‚îÇReplica DB‚îÇ ‚Üê Redundant
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Any single component can fail - system stays up!
```

### Implementation

```python
# ============================================
# Redundant Service Implementation
# ============================================
import random
import time

class HighAvailabilityService:
    def __init__(self):
        # Multiple backend instances
        self.backends = [
            'http://backend1.example.com:3000',
            'http://backend2.example.com:3000',
            'http://backend3.example.com:3000'
        ]
        
        # Track health of each backend
        self.health_status = {
            backend: True for backend in self.backends
        }
    
    async def call_with_retry(self, endpoint, max_retries=3):
        """Call backend with automatic failover"""
        healthy_backends = [
            b for b in self.backends 
            if self.health_status[b]
        ]
        
        if not healthy_backends:
            raise Exception("All backends unhealthy!")
        
        for attempt in range(max_retries):
            # Try random healthy backend
            backend = random.choice(healthy_backends)
            
            try:
                response = await self.make_request(f"{backend}{endpoint}")
                return response
                
            except Exception as e:
                print(f"Attempt {attempt + 1} failed on {backend}: {e}")
                
                # Mark backend as unhealthy
                self.health_status[backend] = False
                
                # Remove from healthy list
                healthy_backends.remove(backend)
                
                if not healthy_backends or attempt == max_retries - 1:
                    raise Exception(f"All retries failed for {endpoint}")
                
                # Exponential backoff
                await asyncio.sleep(2 ** attempt)
    
    async def health_check_loop(self):
        """Continuously check backend health"""
        while True:
            for backend in self.backends:
                try:
                    response = await self.make_request(f"{backend}/health")
                    self.health_status[backend] = (response.status == 200)
                except:
                    self.health_status[backend] = False
                
                status = "‚úì" if self.health_status[backend] else "‚úó"
                print(f"{backend}: {status}")
            
            await asyncio.sleep(10)  # Check every 10 seconds
```

---

## 3. Failover Strategies

### Automatic Failover

**Definition:** Automatically switch to backup when primary fails.

```
Normal Operation:
Client ‚Üí Primary Server ‚úì

Primary Fails:
Client ‚Üí Primary Server ‚úó
         ‚Üì (detected by health check)
         Switch to Backup
         ‚Üì
Client ‚Üí Backup Server ‚úì
```

### DNS Failover (Route 53)

```python
# ============================================
# AWS Route 53 Health Checks & Failover
# ============================================
import boto3

route53 = boto3.client('route53')

# Create health check for primary
health_check_response = route53.create_health_check(
    HealthCheckConfig={
        'Type': 'HTTPS',
        'ResourcePath': '/health',
        'FullyQualifiedDomainName': 'primary.example.com',
        'Port': 443,
        'RequestInterval': 30,  # Check every 30 seconds
        'FailureThreshold': 3    # 3 failures = unhealthy
    }
)

health_check_id = health_check_response['HealthCheck']['Id']

# Create DNS records with failover
route53.change_resource_record_sets(
    HostedZoneId='Z1234567890ABC',
    ChangeBatch={
        'Changes': [
            {
                'Action': 'CREATE',
                'ResourceRecordSet': {
                    'Name': 'api.example.com',
                    'Type': 'A',
                    'SetIdentifier': 'Primary',
                    'Failover': 'PRIMARY',
                    'TTL': 60,
                    'ResourceRecords': [
                        {'Value': '54.123.45.67'}  # Primary IP
                    ],
                    'HealthCheckId': health_check_id
                }
            },
            {
                'Action': 'CREATE',
                'ResourceRecordSet': {
                    'Name': 'api.example.com',
                    'Type': 'A',
                    'SetIdentifier': 'Secondary',
                    'Failover': 'SECONDARY',
                    'TTL': 60,
                    'ResourceRecords': [
                        {'Value': '52.234.56.78'}  # Backup IP
                    ]
                }
            }
        ]
    }
)

# Failover flow:
# 1. Health check fails 3 times on primary
# 2. Route 53 marks primary unhealthy
# 3. DNS returns secondary IP
# 4. Traffic automatically goes to backup
```

### Database Failover

```python
# ============================================
# PostgreSQL Automatic Failover
# ============================================
import psycopg2
import time

class DatabaseFailover:
    def __init__(self):
        self.primary = {
            'host': 'primary.db.example.com',
            'port': 5432,
            'database': 'myapp',
            'user': 'app',
            'password': 'secret'
        }
        
        self.replicas = [
            {
                'host': 'replica1.db.example.com',
                'port': 5432,
                'database': 'myapp',
                'user': 'app',
                'password': 'secret'
            },
            {
                'host': 'replica2.db.example.com',
                'port': 5432,
                'database': 'myapp',
                'user': 'app',
                'password': 'secret'
            }
        ]
        
        self.current_connection = None
        self.is_using_primary = True
    
    def connect(self):
        """Connect with automatic failover"""
        # Try primary first
        try:
            conn = psycopg2.connect(**self.primary)
            self.is_using_primary = True
            print("Connected to primary database")
            return conn
        except Exception as e:
            print(f"Primary connection failed: {e}")
            
            # Failover to replica
            for i, replica in enumerate(self.replicas):
                try:
                    conn = psycopg2.connect(**replica)
                    self.is_using_primary = False
                    print(f"Failed over to replica {i+1}")
                    return conn
                except Exception as e:
                    print(f"Replica {i+1} connection failed: {e}")
            
            raise Exception("All database connections failed!")
    
    def execute_query(self, query, params=()):
        """Execute query with automatic retry on failure"""
        max_retries = 3
        
        for attempt in range(max_retries):
            try:
                if not self.current_connection or self.current_connection.closed:
                    self.current_connection = self.connect()
                
                cursor = self.current_connection.cursor()
                cursor.execute(query, params)
                
                # Commit for write queries
                if query.strip().upper().startswith(('INSERT', 'UPDATE', 'DELETE')):
                    if not self.is_using_primary:
                        raise Exception("Cannot write to replica!")
                    self.current_connection.commit()
                
                result = cursor.fetchall()
                return result
                
            except Exception as e:
                print(f"Query failed (attempt {attempt + 1}): {e}")
                
                # Close bad connection
                if self.current_connection:
                    self.current_connection.close()
                    self.current_connection = None
                
                if attempt == max_retries - 1:
                    raise
                
                time.sleep(2 ** attempt)  # Exponential backoff

# Usage
db = DatabaseFailover()

# Automatically connects to primary or fails over
users = db.execute_query('SELECT * FROM users WHERE active = TRUE')
```

---

## 4. Health Checks and Monitoring

### Comprehensive Health Check

```javascript
// ============================================
// Health Check Endpoint
// ============================================
const express = require('express');
const app = express();

app.get('/health', async (req, res) => {
  const health = {
    status: 'healthy',
    timestamp: new Date().toISOString(),
    uptime: process.uptime(),
    checks: {}
  };
  
  // Check 1: Database connectivity
  try {
    await db.query('SELECT 1');
    health.checks.database = { status: 'healthy' };
  } catch (error) {
    health.checks.database = { 
      status: 'unhealthy', 
      error: error.message 
    };
    health.status = 'unhealthy';
  }
  
  // Check 2: Redis connectivity
  try {
    await redis.ping();
    health.checks.redis = { status: 'healthy' };
  } catch (error) {
    health.checks.redis = { 
      status: 'unhealthy', 
      error: error.message 
    };
    health.status = 'unhealthy';
  }
  
  // Check 3: Disk space
  const diskUsage = await checkDiskSpace();
  if (diskUsage < 90) {
    health.checks.disk = { 
      status: 'healthy', 
      usage: `${diskUsage}%` 
    };
  } else {
    health.checks.disk = { 
      status: 'warning', 
      usage: `${diskUsage}%` 
    };
    if (diskUsage > 95) {
      health.status = 'unhealthy';
    }
  }
  
  // Check 4: Memory usage
  const memoryUsage = process.memoryUsage();
  const memoryPercent = (memoryUsage.heapUsed / memoryUsage.heapTotal) * 100;
  
  health.checks.memory = {
    status: memoryPercent < 80 ? 'healthy' : 'warning',
    heapUsed: `${(memoryUsage.heapUsed / 1024 / 1024).toFixed(2)} MB`,
    heapTotal: `${(memoryUsage.heapTotal / 1024 / 1024).toFixed(2)} MB`,
    percentage: `${memoryPercent.toFixed(2)}%`
  };
  
  // Check 5: External dependencies
  try {
    await fetch('https://api.external-service.com/health', {
      timeout: 2000
    });
    health.checks.externalService = { status: 'healthy' };
  } catch (error) {
    health.checks.externalService = { 
      status: 'degraded',
      error: error.message 
    };
    // Don't mark overall status as unhealthy for external service
  }
  
  // Return appropriate status code
  const statusCode = health.status === 'healthy' ? 200 : 503;
  res.status(statusCode).json(health);
});

// Liveness probe (simple check - is process alive?)
app.get('/health/live', (req, res) => {
  res.status(200).json({ status: 'alive' });
});

// Readiness probe (is service ready to receive traffic?)
app.get('/health/ready', async (req, res) => {
  try {
    await db.query('SELECT 1');
    res.status(200).json({ status: 'ready' });
  } catch (error) {
    res.status(503).json({ 
      status: 'not ready',
      reason: 'database unavailable'
    });
  }
});
```

### Kubernetes Health Checks

```yaml
# ============================================
# Kubernetes Deployment with Health Checks
# ============================================
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api-server
spec:
  replicas: 3
  selector:
    matchLabels:
      app: api-server
  template:
    metadata:
      labels:
        app: api-server
    spec:
      containers:
      - name: api
        image: api-server:1.0
        ports:
        - containerPort: 3000
        
        # Liveness probe - restart if fails
        livenessProbe:
          httpGet:
            path: /health/live
            port: 3000
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        
        # Readiness probe - remove from load balancer if fails
        readinessProbe:
          httpGet:
            path: /health/ready
            port: 3000
          initialDelaySeconds: 10
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 2
          successThreshold: 1
        
        # Resource limits
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 500m
            memory: 512Mi
```

---

## 5. SLA, SLO, and SLI

### Definitions

**SLI (Service Level Indicator):** Metric that measures performance
```
Examples:
- Request latency (95th percentile < 200ms)
- Error rate (< 0.1%)
- Availability (> 99.9%)
```

**SLO (Service Level Objective):** Target value for SLI
```
Examples:
- 95% of requests complete in < 200ms
- 99.9% of requests succeed
- System available 99.95% of time
```

**SLA (Service Level Agreement):** Contract with consequences
```
Example:
"We guarantee 99.9% uptime per month.
If uptime < 99.9%, customer receives 10% credit.
If uptime < 99%, customer receives 25% credit."
```

### Implementing SLI Tracking

```python
# ============================================
# SLI Tracker
# ============================================
from prometheus_client import Counter, Histogram, Gauge
import time

class SLITracker:
    def __init__(self):
        # Metrics
        self.request_count = Counter(
            'http_requests_total',
            'Total HTTP requests',
            ['method', 'endpoint', 'status']
        )
        
        self.request_duration = Histogram(
            'http_request_duration_seconds',
            'HTTP request latency',
            ['method', 'endpoint']
        )
        
        self.error_count = Counter(
            'http_errors_total',
            'Total HTTP errors',
            ['method', 'endpoint', 'error_type']
        )
    
    def track_request(self, method, endpoint, status, duration):
        """Track request metrics"""
        self.request_count.labels(
            method=method,
            endpoint=endpoint,
            status=status
        ).inc()
        
        self.request_duration.labels(
            method=method,
            endpoint=endpoint
        ).observe(duration)
        
        if status >= 500:
            self.error_count.labels(
                method=method,
                endpoint=endpoint,
                error_type='server_error'
            ).inc()
        elif status >= 400:
            self.error_count.labels(
                method=method,
                endpoint=endpoint,
                error_type='client_error'
            ).inc()

# Middleware
tracker = SLITracker()

def track_middleware(req, res, next):
    start = time.time()
    
    # Track response
    original_send = res.send
    
    def tracked_send(*args, **kwargs):
        duration = time.time() - start
        tracker.track_request(
            req.method,
            req.path,
            res.status_code,
            duration
        )
        return original_send(*args, **kwargs)
    
    res.send = tracked_send
    next()

# SLO Alerts (Prometheus)
"""
# Alert if error rate > 0.1%
- alert: HighErrorRate
  expr: |
    rate(http_errors_total[5m]) / rate(http_requests_total[5m]) > 0.001
  for: 5m
  labels:
    severity: critical
  annotations:
    summary: "High error rate detected"

# Alert if p95 latency > 200ms
- alert: HighLatency
  expr: |
    histogram_quantile(0.95, http_request_duration_seconds) > 0.2
  for: 5m
  labels:
    severity: warning
  annotations:
    summary: "P95 latency above SLO"
"""
```

---

## 6. Multi-Region Deployment

### Active-Passive (Primary-Backup)

```
Normal Operation:
Region 1 (Active) ‚Üê All traffic
Region 2 (Passive) ‚Üê Standby, replicating data

Primary Fails:
Region 1 (Failed) ‚Üê No traffic
Region 2 (Active) ‚Üê All traffic switched here
```

**Implementation:**

```python
# ============================================
# Multi-Region Setup
# ============================================
import boto3

class MultiRegionSetup:
    def __init__(self):
        self.route53 = boto3.client('route53')
        self.rds = boto3.client('rds')
    
    def setup_active_passive(self):
        """Setup active-passive configuration"""
        
        # Primary region (us-east-1)
        primary_db = self.create_database(
            region='us-east-1',
            identifier='myapp-primary',
            multi_az=True  # Multi-AZ for HA within region
        )
        
        # Standby region (us-west-2) - read replica
        standby_db = self.create_read_replica(
            source_db='myapp-primary',
            region='us-west-2',
            identifier='myapp-standby'
        )
        
        # Setup DNS failover
        self.setup_dns_failover(
            primary_endpoint=primary_db['endpoint'],
            standby_endpoint=standby_db['endpoint']
        )
        
        print("Active-passive setup complete")
        print(f"Primary: {primary_db['endpoint']}")
        print(f"Standby: {standby_db['endpoint']}")
    
    def failover_to_standby(self):
        """Manually trigger failover"""
        # Promote read replica to standalone database
        self.rds.promote_read_replica(
            DBInstanceIdentifier='myapp-standby'
        )
        
        # Update DNS to point to new primary
        # (or use Route 53 health checks for automatic)
        
        print("Failover complete - standby is now primary")
```

### Active-Active (Multi-Master)

```
Region 1 (Active) ‚Üê 50% traffic
                 ‚Üï (bidirectional replication)
Region 2 (Active) ‚Üê 50% traffic

Both regions serve traffic simultaneously
```

**Challenges:**
- Conflict resolution (both regions write same data)
- Data synchronization
- Network partitions

```python
# ============================================
# Active-Active with Conflict Resolution
# ============================================

class ActiveActiveDatabase:
    def __init__(self):
        self.region1_db = connect_to_database('us-east-1')
        self.region2_db = connect_to_database('us-west-2')
    
    def write_with_timestamp(self, key, value, region):
        """Write with last-write-wins conflict resolution"""
        timestamp = time.time()
        
        data = {
            'key': key,
            'value': value,
            'timestamp': timestamp,
            'region': region
        }
        
        # Write to local region
        if region == 'us-east-1':
            self.region1_db.insert(data)
            # Replicate to other region asynchronously
            self.replicate_async(data, 'us-west-2')
        else:
            self.region2_db.insert(data)
            self.replicate_async(data, 'us-east-1')
    
    def resolve_conflict(self, data1, data2):
        """Last-write-wins conflict resolution"""
        if data1['timestamp'] > data2['timestamp']:
            return data1
        else:
            return data2
    
    def replicate_async(self, data, target_region):
        """Asynchronously replicate to other region"""
        # Queue for replication
        replication_queue.enqueue({
            'data': data,
            'target': target_region
        })
```

---

## 7. Active-Active vs Active-Passive

### Comparison

| Aspect | Active-Passive | Active-Active |
|--------|----------------|---------------|
| **Complexity** | Lower | Higher |
| **Cost** | Lower (standby idle) | Higher (both active) |
| **Failover Time** | 30-60 seconds | 0 seconds |
| **Resource Usage** | 50% (standby idle) | 100% (both used) |
| **Data Consistency** | Easier | Complex (conflicts) |
| **Use Case** | Traditional apps | Global apps |

### When to Use Each

**Active-Passive:**
```
‚úÖ Cost-sensitive
‚úÖ Can tolerate brief failover time
‚úÖ Simpler architecture preferred
‚úÖ Traditional monolithic apps

Examples: Internal tools, corporate apps
```

**Active-Active:**
```
‚úÖ Global user base
‚úÖ Zero downtime required
‚úÖ Can handle complexity
‚úÖ High traffic justifies cost

Examples: Netflix, Amazon, Facebook
```

---

## 8. Split-Brain Problem

**Problem:** In active-active setup, network partition causes both sides to think they're primary.

```
Normal:
Region A ‚Üê‚Üí Region B (connected)

Network Partition:
Region A   Region B (disconnected)
    ‚Üì         ‚Üì
Both think they're primary
Both accept writes
Conflict when reconnected!
```

### Solution: Quorum-Based Consensus

```python
# ============================================
# Quorum-Based Write
# ============================================

class QuorumDatabase:
    def __init__(self, replicas):
        self.replicas = replicas  # List of database connections
        self.quorum_size = len(replicas) // 2 + 1  # Majority
    
    async def write_with_quorum(self, key, value):
        """Write succeeds only if majority of replicas confirm"""
        tasks = []
        
        for replica in self.replicas:
            task = replica.write(key, value)
            tasks.append(task)
        
        # Wait for quorum
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Count successes
        successes = sum(1 for r in results if not isinstance(r, Exception))
        
        if successes >= self.quorum_size:
            print(f"Write successful ({successes}/{len(self.replicas)} replicas)")
            return True
        else:
            print(f"Write failed (only {successes}/{self.quorum_size} required)")
            # Rollback on replicas that succeeded
            return False
    
    async def read_with_quorum(self, key):
        """Read from quorum to get latest value"""
        tasks = [replica.read(key) for replica in self.replicas]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Get values from successful reads
        values = {}
        for result in results:
            if not isinstance(result, Exception):
                timestamp = result['timestamp']
                if timestamp not in values:
                    values[timestamp] = []
                values[timestamp].append(result['value'])
        
        # Return most recent value with quorum
        for timestamp in sorted(values.keys(), reverse=True):
            if len(values[timestamp]) >= self.quorum_size:
                return values[timestamp][0]
        
        raise Exception("Could not achieve read quorum")

# With 5 replicas:
# Quorum = 3
# Can tolerate 2 failures
```

---

## 9. Disaster Recovery

### RTO and RPO

**RTO (Recovery Time Objective):** How long to recover
```
RTO = 1 hour
Meaning: Must restore service within 1 hour of failure
```

**RPO (Recovery Point Objective):** How much data loss acceptable
```
RPO = 5 minutes
Meaning: Can lose up to 5 minutes of data
```

### Backup Strategies

```python
# ============================================
# Database Backup Strategy
# ============================================
import subprocess
from datetime import datetime

class BackupManager:
    def __init__(self, db_config):
        self.db_config = db_config
        self.s3_bucket = 'myapp-backups'
    
    def create_full_backup(self):
        """Full database backup"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_file = f'backup_full_{timestamp}.sql'
        
        # pg_dump for full backup
        subprocess.run([
            'pg_dump',
            '-h', self.db_config['host'],
            '-U', self.db_config['user'],
            '-d', self.db_config['database'],
            '-f', backup_file
        ])
        
        # Upload to S3
        self.upload_to_s3(backup_file)
        
        print(f"Full backup created: {backup_file}")
        return backup_file
    
    def create_incremental_backup(self):
        """Incremental backup (only changes since last backup)"""
        # Using WAL archiving for PostgreSQL
        subprocess.run([
            'pg_basebackup',
            '-h', self.db_config['host'],
            '-U', self.db_config['user'],
            '-D', '/backups/incremental',
            '-F', 't',  # Tar format
            '-X', 'stream'  # Include WAL
        ])
        
        print("Incremental backup created")
    
    def schedule_backups(self):
        """Backup schedule"""
        # Full backup: Daily at 2 AM
        schedule.every().day.at("02:00").do(self.create_full_backup)
        
        # Incremental backup: Every 4 hours
        schedule.every(4).hours.do(self.create_incremental_backup)
        
        # Keep backups for 30 days
        schedule.every().day.do(self.cleanup_old_backups, days=30)
    
    def restore_from_backup(self, backup_file):
        """Restore database from backup"""
        subprocess.run([
            'psql',
            '-h', self.db_config['host'],
            '-U', self.db_config['user'],
            '-d', self.db_config['database'],
            '-f', backup_file
        ])
        
        print(f"Database restored from {backup_file}")
```

### Disaster Recovery Plan

```markdown
# Disaster Recovery Runbook

## Scenario: Primary Region Failure

### Detection (< 5 minutes)
1. Health checks fail
2. Alerts triggered (PagerDuty)
3. On-call engineer paged

### Assessment (5-10 minutes)
1. Verify region is down
2. Check standby region health
3. Confirm data replication up-to-date

### Failover (10-20 minutes)
1. Promote standby database to primary
2. Update DNS to point to standby region
3. Restart application services in standby
4. Verify system operational

### Communication (Immediate)
1. Update status page
2. Notify customers via email
3. Post on social media

### Recovery (Hours to Days)
1. Investigate root cause
2. Fix primary region
3. Restore replication
4. Plan failback
```

---

## 10. Implementation Examples

### Example 1: Complete HA Web Application

```typescript
// ============================================
// High Availability Express Application
// ============================================
import express from 'express';
import Redis from 'ioredis';
import { Pool } from 'pg';

const app = express();

// ============================================
// 1. Redundant Database Connections
// ============================================
const primaryDb = new Pool({
  host: 'primary.db.example.com',
  port: 5432,
  database: 'myapp',
  max: 20,
  connectionTimeoutMillis: 5000
});

const replicaDbs = [
  new Pool({ host: 'replica1.db.example.com', port: 5432, database: 'myapp', max: 50 }),
  new Pool({ host: 'replica2.db.example.com', port: 5432, database: 'myapp', max: 50 })
];

let currentReplicaIndex = 0;

function getReadConnection() {
  const pool = replicaDbs[currentReplicaIndex];
  currentReplicaIndex = (currentReplicaIndex + 1) % replicaDbs.length;
  return pool;
}

// ============================================
// 2. Redundant Cache (Redis Sentinel)
// ============================================
const redis = new Redis({
  sentinels: [
    { host: 'sentinel1.example.com', port: 26379 },
    { host: 'sentinel2.example.com', port: 26379 },
    { host: 'sentinel3.example.com', port: 26379 }
  ],
  name: 'mymaster',  // Master name
  sentinelRetryStrategy: (times) => {
    if (times > 10) return null;
    return Math.min(times * 100, 3000);
  }
});

// Sentinel automatically fails over to new master

// ============================================
// 3. Graceful Shutdown
// ============================================
const server = app.listen(3000);

function gracefulShutdown(signal) {
  console.log(`${signal} received, starting graceful shutdown`);
  
  // Stop accepting new connections
  server.close(() => {
    console.log('HTTP server closed');
    
    // Close database connections
    primaryDb.end(() => {
      console.log('Database connections closed');
      
      // Close Redis connection
      redis.quit(() => {
        console.log('Redis connection closed');
        process.exit(0);
      });
    });
  });
  
  // Force shutdown after 30 seconds
  setTimeout(() => {
    console.error('Forced shutdown after timeout');
    process.exit(1);
  }, 30000);
}

process.on('SIGTERM', () => gracefulShutdown('SIGTERM'));
process.on('SIGINT', () => gracefulShutdown('SIGINT'));

// ============================================
// 4. Health Check Endpoint
// ============================================
app.get('/health', async (req, res) => {
  const health = { status: 'healthy', checks: {} };
  
  try {
    await primaryDb.query('SELECT 1');
    health.checks.database = 'healthy';
  } catch (error) {
    health.checks.database = 'unhealthy';
    health.status = 'unhealthy';
  }
  
  try {
    await redis.ping();
    health.checks.redis = 'healthy';
  } catch (error) {
    health.checks.redis = 'unhealthy';
    health.status = 'unhealthy';
  }
  
  const statusCode = health.status === 'healthy' ? 200 : 503;
  res.status(statusCode).json(health);
});

// ============================================
// 5. Request Handler with Retries
// ============================================
async function executeWithRetry(operation, maxRetries = 3) {
  for (let attempt = 1; attempt <= maxRetries; attempt++) {
    try {
      return await operation();
    } catch (error) {
      console.error(`Attempt ${attempt} failed:`, error.message);
      
      if (attempt === maxRetries) {
        throw error;
      }
      
      // Exponential backoff
      await new Promise(resolve => setTimeout(resolve, 2 ** attempt * 100));
    }
  }
}

app.get('/api/users/:id', async (req, res) => {
  try {
    const user = await executeWithRetry(async () => {
      return await getReadConnection().query(
        'SELECT * FROM users WHERE id = $1',
        [req.params.id]
      );
    });
    
    res.json(user.rows[0]);
  } catch (error) {
    res.status(500).json({ error: 'Service temporarily unavailable' });
  }
});
```

---

## 11. Testing for High Availability

### Chaos Engineering

```python
# ============================================
# Chaos Testing
# ============================================
import random
import time

class ChaosMonkey:
    """Randomly kill services to test resilience"""
    
    def __init__(self, services):
        self.services = services
    
    def random_failure(self):
        """Randomly kill a service"""
        service = random.choice(self.services)
        print(f"üêµ Chaos Monkey killing {service}")
        
        # Kill service
        subprocess.run(['kubectl', 'delete', 'pod', '-l', f'app={service}'])
        
        # Wait and check if system recovered
        time.sleep(30)
        
        # Verify system still operational
        response = requests.get('https://api.example.com/health')
        if response.status_code == 200:
            print("‚úÖ System recovered from failure")
        else:
            print("‚ùå System did not recover - ALERT!")
    
    def run_chaos_tests(self, duration_hours=24):
        """Run chaos tests for specified duration"""
        end_time = time.time() + (duration_hours * 3600)
        
        while time.time() < end_time:
            # Random failure every 1-6 hours
            wait_time = random.randint(3600, 21600)
            time.sleep(wait_time)
            
            self.random_failure()

# Usage
chaos = ChaosMonkey([
    'api-server',
    'worker-service',
    'notification-service'
])

# Run chaos tests
chaos.run_chaos_tests(duration_hours=24)
```

---

## Chapter 19 Summary

### Key Concepts

1. **High Availability** - System stays operational despite failures
2. **Nine Nines** - Availability percentages (99.9%, 99.99%, etc.)
3. **Redundancy** - Eliminate single points of failure
4. **Failover** - Automatic switch to backup
5. **Health Checks** - Continuous monitoring
6. **SLA/SLO/SLI** - Service level metrics
7. **Multi-Region** - Geographic redundancy
8. **Active-Active** - Both regions serve traffic
9. **Active-Passive** - One region on standby

### Availability Formula

```
Serial Components (all must work):
A = A‚ÇÅ √ó A‚ÇÇ √ó A‚ÇÉ

Parallel Components (any can work):
A = 1 - (1-A‚ÇÅ) √ó (1-A‚ÇÇ) √ó (1-A‚ÇÉ)

Example:
2 load balancers (99.9% each)
System availability = 1 - (1-0.999)¬≤ = 99.9999%
```

### Building HA Systems

1. **Eliminate SPOFs** - Redundancy everywhere
2. **Auto-failover** - Don't rely on manual intervention
3. **Health checks** - Detect failures quickly
4. **Graceful degradation** - Degrade, don't fail completely
5. **Regular testing** - Chaos engineering
6. **Monitoring** - Know when things fail
7. **Runbooks** - Document recovery procedures

### Interview Tips

**Common Questions:**
1. "How do you achieve 99.99% availability?"
2. "Explain active-active vs active-passive"
3. "What is the split-brain problem?"
4. "How do you calculate system availability?"

**How to Answer:**
- Draw multi-region architecture
- Explain redundancy at each layer
- Calculate availability with formula
- Discuss trade-offs (cost vs availability)
- Mention real examples (AWS Multi-AZ)

### Next Steps

Chapter 20 will cover **Fault Tolerance Patterns** - specific patterns for handling failures gracefully, including Circuit Breaker, Bulkhead, Retry, and more.