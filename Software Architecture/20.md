# Chapter 20: Fault Tolerance Patterns

## Table of Contents
1. Introduction to Fault Tolerance
2. Circuit Breaker Pattern
3. Retry with Exponential Backoff
4. Bulkhead Pattern
5. Timeout Strategies
6. Graceful Degradation
7. Rate Limiting
8. Fallback Patterns
9. Combining Patterns
10. Real-World Implementation
11. Testing Fault Tolerance

---

## 1. Introduction to Fault Tolerance

**Definition:** Fault tolerance is the ability of a system to continue operating properly in the event of failure of some of its components.

### Design Principles

**1. Fail Fast**
- Detect failures quickly
- Don't wait indefinitely
- Return error immediately if can't proceed

**2. Fail Safe**
- System defaults to safe state
- Prefer degraded service over total failure
- Protect user data

**3. Fail Gracefully**
- Handle errors elegantly
- Provide meaningful error messages
- Maintain user experience

### Types of Failures

```
1. Transient (temporary)
   - Network glitch
   - Temporary overload
   → Solution: Retry

2. Permanent (persistent)
   - Service down
   - Invalid configuration
   → Solution: Failover, fallback

3. Intermittent (sporadic)
   - Flaky network
   - Race conditions
   → Solution: Circuit breaker
```

---

## 2. Circuit Breaker Pattern

**Definition:** Prevent cascading failures by stopping requests to failing service after threshold is reached.

### Circuit Breaker States

```
CLOSED (Normal):
- Requests pass through
- Failures counted
- If failures exceed threshold → OPEN

OPEN (Failing):
- All requests blocked immediately
- No calls to failing service
- After timeout → HALF_OPEN

HALF_OPEN (Testing):
- Limited requests allowed
- If success → CLOSED
- If failure → OPEN
```

### Implementation

```typescript
// ============================================
// Circuit Breaker Implementation
// ============================================

enum CircuitState {
  CLOSED = 'CLOSED',
  OPEN = 'OPEN',
  HALF_OPEN = 'HALF_OPEN'
}

interface CircuitBreakerOptions {
  failureThreshold: number;      // Failures before opening
  successThreshold: number;      // Successes to close from half-open
  timeout: number;                // Time before trying half-open (ms)
  monitoringPeriod: number;      // Time window for counting failures (ms)
}

class CircuitBreaker {
  private state: CircuitState = CircuitState.CLOSED;
  private failures: number = 0;
  private successes: number = 0;
  private nextAttempt: number = 0;
  private lastFailureTime: number = 0;
  
  constructor(
    private serviceName: string,
    private options: CircuitBreakerOptions
  ) {}
  
  async execute<T>(operation: () => Promise<T>): Promise<T> {
    // Check state
    if (this.state === CircuitState.OPEN) {
      if (Date.now() < this.nextAttempt) {
        throw new Error(
          `Circuit breaker OPEN for ${this.serviceName}. ` +
          `Try again at ${new Date(this.nextAttempt).toISOString()}`
        );
      }
      
      // Timeout elapsed, try half-open
      this.state = CircuitState.HALF_OPEN;
      this.successes = 0;
      console.log(`${this.serviceName} circuit breaker: HALF_OPEN`);
    }
    
    try {
      const result = await operation();
      this.onSuccess();
      return result;
      
    } catch (error) {
      this.onFailure();
      throw error;
    }
  }
  
  private onSuccess(): void {
    this.failures = 0;
    
    if (this.state === CircuitState.HALF_OPEN) {
      this.successes++;
      
      if (this.successes >= this.options.successThreshold) {
        this.state = CircuitState.CLOSED;
        console.log(`${this.serviceName} circuit breaker: CLOSED (recovered)`);
      }
    }
  }
  
  private onFailure(): void {
    this.failures++;
    this.lastFailureTime = Date.now();
    
    // Clear old failures outside monitoring period
    if (Date.now() - this.lastFailureTime > this.options.monitoringPeriod) {
      this.failures = 1;
    }
    
    if (this.state === CircuitState.HALF_OPEN) {
      // Failed during half-open, go back to open
      this.state = CircuitState.OPEN;
      this.nextAttempt = Date.now() + this.options.timeout;
      console.log(`${this.serviceName} circuit breaker: OPEN (half-open test failed)`);
      
    } else if (this.failures >= this.options.failureThreshold) {
      // Too many failures, open circuit
      this.state = CircuitState.OPEN;
      this.nextAttempt = Date.now() + this.options.timeout;
      console.log(
        `${this.serviceName} circuit breaker: OPEN ` +
        `(${this.failures} failures in ${this.options.monitoringPeriod}ms)`
      );
    }
  }
  
  getState(): CircuitState {
    return this.state;
  }
  
  getMetrics() {
    return {
      state: this.state,
      failures: this.failures,
      successes: this.successes,
      nextAttempt: this.state === CircuitState.OPEN 
        ? new Date(this.nextAttempt).toISOString() 
        : null
    };
  }
}

// ============================================
// Usage
// ============================================

// Create circuit breaker for payment service
const paymentCircuit = new CircuitBreaker('PaymentService', {
  failureThreshold: 5,      // Open after 5 failures
  successThreshold: 2,      // Close after 2 successes
  timeout: 60000,           // Try recovery after 60s
  monitoringPeriod: 10000   // Count failures in 10s window
});

async function processPayment(orderId: string, amount: number): Promise<any> {
  try {
    return await paymentCircuit.execute(async () => {
      // Call external payment service
      const response = await fetch('https://payment-api.com/charge', {
        method: 'POST',
        body: JSON.stringify({ orderId, amount }),
        headers: { 'Content-Type': 'application/json' }
      });
      
      if (!response.ok) {
        throw new Error(`Payment failed: ${response.status}`);
      }
      
      return await response.json();
    });
    
  } catch (error) {
    if (error.message.includes('Circuit breaker OPEN')) {
      // Circuit is open - use fallback
      return {
        status: 'queued',
        message: 'Payment service temporarily unavailable. Order queued for processing.'
      };
    }
    throw error;
  }
}

// Test circuit breaker
async function testCircuitBreaker() {
  // First 5 calls fail - circuit opens
  for (let i = 0; i < 7; i++) {
    try {
      await processPayment(`order_${i}`, 100);
    } catch (error) {
      console.log(`Call ${i + 1}: ${error.message}`);
    }
  }
  
  // Wait for timeout
  await new Promise(resolve => setTimeout(resolve, 61000));
  
  // Circuit tries half-open
  try {
    await processPayment('order_recovery', 100);
    console.log('Circuit recovered!');
  } catch (error) {
    console.log('Circuit still broken');
  }
}
```

---

## 3. Retry with Exponential Backoff

**Definition:** Retry failed operations with increasing delay between attempts.

### Why Exponential Backoff?

```
Linear Retry (bad):
Attempt 1: Wait 1s
Attempt 2: Wait 1s
Attempt 3: Wait 1s
Problem: Hammers failing service

Exponential Backoff (good):
Attempt 1: Wait 1s
Attempt 2: Wait 2s
Attempt 3: Wait 4s
Attempt 4: Wait 8s
Benefit: Gives service time to recover
```

### Implementation

```python
# ============================================
# Retry with Exponential Backoff
# ============================================
import time
import random
from functools import wraps

def retry_with_backoff(
    max_retries=5,
    base_delay=1,
    max_delay=60,
    exponential_base=2,
    jitter=True
):
    """Decorator for retry with exponential backoff"""
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            last_exception = None
            
            for attempt in range(max_retries):
                try:
                    return await func(*args, **kwargs)
                    
                except Exception as e:
                    last_exception = e
                    
                    # Don't retry on last attempt
                    if attempt == max_retries - 1:
                        raise
                    
                    # Calculate delay
                    delay = min(
                        base_delay * (exponential_base ** attempt),
                        max_delay
                    )
                    
                    # Add jitter to prevent thundering herd
                    if jitter:
                        delay = delay * (0.5 + random.random())
                    
                    print(
                        f"Attempt {attempt + 1} failed: {e}. "
                        f"Retrying in {delay:.2f}s..."
                    )
                    
                    time.sleep(delay)
            
            raise last_exception
        
        return wrapper
    return decorator

# Usage
@retry_with_backoff(max_retries=5, base_delay=1)
async def call_external_api(url):
    """Call API with automatic retry"""
    response = await fetch(url)
    
    if response.status >= 500:
        raise Exception(f"Server error: {response.status}")
    
    return await response.json()

# Test
try:
    data = await call_external_api('https://flaky-api.com/data')
except Exception as e:
    print(f"All retries failed: {e}")

# Output:
# Attempt 1 failed: Server error: 503. Retrying in 0.87s...
# Attempt 2 failed: Server error: 503. Retrying in 2.34s...
# Attempt 3 failed: Server error: 503. Retrying in 5.67s...
# Attempt 4 failed: Server error: 503. Retrying in 10.23s...
# All retries failed: Server error: 503
```

### Smart Retry (Don't Retry Everything)

```typescript
// ============================================
// Selective Retry Based on Error Type
// ============================================

class SmartRetry {
  // Errors that are safe to retry
  private static RETRYABLE_ERRORS = [
    'ECONNRESET',
    'ETIMEDOUT',
    'ECONNREFUSED',
    'NETWORK_ERROR',
    'SERVICE_UNAVAILABLE'
  ];
  
  // HTTP status codes safe to retry
  private static RETRYABLE_STATUS_CODES = [
    408,  // Request Timeout
    429,  // Too Many Requests
    500,  // Internal Server Error
    502,  // Bad Gateway
    503,  // Service Unavailable
    504   // Gateway Timeout
  ];
  
  static isRetryable(error: any): boolean {
    // Check error code
    if (this.RETRYABLE_ERRORS.includes(error.code)) {
      return true;
    }
    
    // Check HTTP status
    if (error.response && this.RETRYABLE_STATUS_CODES.includes(error.response.status)) {
      return true;
    }
    
    return false;
  }
  
  static async execute<T>(
    operation: () => Promise<T>,
    maxRetries: number = 3
  ): Promise<T> {
    let lastError: any;
    
    for (let attempt = 0; attempt < maxRetries; attempt++) {
      try {
        return await operation();
        
      } catch (error: any) {
        lastError = error;
        
        // Don't retry if not retryable error
        if (!this.isRetryable(error)) {
          console.log(`Not retryable error: ${error.message}`);
          throw error;
        }
        
        if (attempt < maxRetries - 1) {
          const delay = Math.min(1000 * (2 ** attempt), 30000);
          console.log(`Retrying in ${delay}ms (attempt ${attempt + 1}/${maxRetries})`);
          await new Promise(resolve => setTimeout(resolve, delay));
        }
      }
    }
    
    throw lastError;
  }
}

// Usage
try {
  const result = await SmartRetry.execute(async () => {
    return await fetch('https://api.example.com/data');
  });
} catch (error) {
  console.error('Request failed after retries:', error);
}
```

---

## 4. Bulkhead Pattern

**Definition:** Isolate resources so failure in one area doesn't affect others. Named after ship compartments that prevent entire ship from sinking.

### The Problem

```
Without Bulkhead:
Shared Thread Pool (50 threads)
    ↓
Payment Service (slow) uses 45 threads
    ↓
Email Service needs threads → only 5 available
    ↓
Notification Service needs threads → none available
    ↓
All services affected by one slow service!
```

### The Solution

```
With Bulkhead:
Payment Service → Thread Pool 1 (20 threads)
Email Service → Thread Pool 2 (15 threads)
Notification Service → Thread Pool 3 (15 threads)

Payment service slow → Only uses its 20 threads
Other services unaffected!
```

### Implementation

```java
// ============================================
// Bulkhead with Thread Pools
// ============================================
import java.util.concurrent.*;

public class BulkheadExecutor {
    // Separate thread pools for different services
    private ExecutorService paymentPool;
    private ExecutorService emailPool;
    private ExecutorService notificationPool;
    
    public BulkheadExecutor() {
        // Payment service pool (20 threads)
        this.paymentPool = new ThreadPoolExecutor(
            5,   // Core pool size
            20,  // Max pool size
            60L, TimeUnit.SECONDS,
            new LinkedBlockingQueue<>(100),  // Max 100 queued tasks
            new ThreadPoolExecutor.CallerRunsPolicy()  // Rejection policy
        );
        
        // Email service pool (15 threads)
        this.emailPool = new ThreadPoolExecutor(
            5, 15, 60L, TimeUnit.SECONDS,
            new LinkedBlockingQueue<>(50),
            new ThreadPoolExecutor.CallerRunsPolicy()
        );
        
        // Notification service pool (15 threads)
        this.notificationPool = new ThreadPoolExecutor(
            5, 15, 60L, TimeUnit.SECONDS,
            new LinkedBlockingQueue<>(50),
            new ThreadPoolExecutor.CallerRunsPolicy()
        );
    }
    
    public Future<PaymentResult> processPayment(Payment payment) {
        return paymentPool.submit(() -> {
            // Payment processing logic
            return paymentService.process(payment);
        });
    }
    
    public Future<Void> sendEmail(Email email) {
        return emailPool.submit(() -> {
            // Email sending logic
            emailService.send(email);
            return null;
        });
    }
    
    public Future<Void> sendNotification(Notification notification) {
        return notificationPool.submit(() -> {
            // Notification logic
            notificationService.send(notification);
            return null;
        });
    }
    
    public void shutdown() {
        paymentPool.shutdown();
        emailPool.shutdown();
        notificationPool.shutdown();
    }
}

// Usage
BulkheadExecutor executor = new BulkheadExecutor();

// Process payment in isolated pool
Future<PaymentResult> payment = executor.processPayment(new Payment(100.0));

// Send email in isolated pool
Future<Void> email = executor.sendEmail(new Email("user@example.com"));

// Even if payment service is slow/hanging,
// email service continues working!
```

### Semaphore-Based Bulkhead

```python
# ============================================
# Bulkhead with Semaphores
# ============================================
import asyncio
from typing import Callable, TypeVar

T = TypeVar('T')

class Bulkhead:
    def __init__(self, max_concurrent: int):
        self.semaphore = asyncio.Semaphore(max_concurrent)
        self.active_calls = 0
        self.total_calls = 0
        self.rejected_calls = 0
    
    async def execute(self, operation: Callable[[], T]) -> T:
        """Execute operation with bulkhead protection"""
        if self.semaphore.locked():
            self.rejected_calls += 1
            raise Exception(
                f"Bulkhead full ({self.active_calls} active calls). "
                f"Request rejected."
            )
        
        async with self.semaphore:
            self.active_calls += 1
            self.total_calls += 1
            
            try:
                return await operation()
            finally:
                self.active_calls -= 1
    
    def get_stats(self):
        return {
            'active_calls': self.active_calls,
            'total_calls': self.total_calls,
            'rejected_calls': self.rejected_calls
        }

# Create bulkheads for different services
payment_bulkhead = Bulkhead(max_concurrent=20)
email_bulkhead = Bulkhead(max_concurrent=50)
database_bulkhead = Bulkhead(max_concurrent=10)

# Usage
async def process_payment(order_id):
    try:
        return await payment_bulkhead.execute(async lambda: (
            await payment_service.charge(order_id)
        ))
    except Exception as e:
        if "Bulkhead full" in str(e):
            # Handle rejection
            return {'status': 'queued', 'message': 'Payment queued for processing'}
        raise

async def send_email(email_address, content):
    try:
        return await email_bulkhead.execute(async lambda: (
            await email_service.send(email_address, content)
        ))
    except Exception as e:
        if "Bulkhead full" in str(e):
            # Queue for later
            await queue.enqueue('emails', {'to': email_address, 'content': content})
            return {'status': 'queued'}
        raise
```

---

## 5. Timeout Strategies

**Definition:** Set maximum time to wait for operation to complete.

### Why Timeouts Matter

```
Without Timeout:
Request → Service (hangs indefinitely)
    ↓
Thread blocked forever
    ↓
All threads blocked
    ↓
System deadlocked!

With Timeout:
Request → Service (max 5s)
    ↓ (after 5s)
Timeout error returned
    ↓
Thread freed
    ↓
System remains responsive
```

### Implementation

```javascript
// ============================================
// Timeout Wrapper
// ============================================

class TimeoutError extends Error {
  constructor(message: string, public timeout: number) {
    super(message);
    this.name = 'TimeoutError';
  }
}

async function withTimeout<T>(
  promise: Promise<T>,
  timeoutMs: number,
  errorMessage?: string
): Promise<T> {
  let timeoutHandle: NodeJS.Timeout;
  
  const timeoutPromise = new Promise<never>((_, reject) => {
    timeoutHandle = setTimeout(() => {
      reject(new TimeoutError(
        errorMessage || `Operation timed out after ${timeoutMs}ms`,
        timeoutMs
      ));
    }, timeoutMs);
  });
  
  try {
    return await Promise.race([promise, timeoutPromise]);
  } finally {
    clearTimeout(timeoutHandle);
  }
}

// Usage
async function getUserData(userId: string): Promise<any> {
  try {
    // Max 5 seconds for database query
    const user = await withTimeout(
      db.query('SELECT * FROM users WHERE id = $1', [userId]),
      5000,
      'Database query timeout'
    );
    
    return user;
    
  } catch (error) {
    if (error instanceof TimeoutError) {
      console.error('Query timed out, using cached data');
      return await cache.get(`user:${userId}`);
    }
    throw error;
  }
}

// ============================================
// Cascading Timeouts
// ============================================

async function handleRequest(req, res) {
  // Overall request timeout: 30s
  try {
    const result = await withTimeout(
      processRequest(req),
      30000,
      'Request processing timeout'
    );
    
    res.json(result);
    
  } catch (error) {
    if (error instanceof TimeoutError) {
      res.status(504).json({
        error: 'Gateway Timeout',
        message: 'Request took too long to process'
      });
    } else {
      res.status(500).json({ error: 'Internal Server Error' });
    }
  }
}

async function processRequest(req) {
  // Sub-operation 1: Max 5s
  const user = await withTimeout(
    getUserData(req.userId),
    5000
  );
  
  // Sub-operation 2: Max 10s
  const orders = await withTimeout(
    getOrders(req.userId),
    10000
  );
  
  // Sub-operation 3: Max 15s
  const recommendations = await withTimeout(
    getRecommendations(req.userId),
    15000
  );
  
  return { user, orders, recommendations };
}
```

### Adaptive Timeouts

```python
# ============================================
# Adaptive Timeout (based on historical performance)
# ============================================
from collections import deque
import statistics

class AdaptiveTimeout:
    def __init__(self, initial_timeout=5.0, window_size=100):
        self.response_times = deque(maxlen=window_size)
        self.initial_timeout = initial_timeout
    
    def record_response_time(self, duration):
        """Record successful response time"""
        self.response_times.append(duration)
    
    def get_timeout(self, percentile=95):
        """Calculate timeout based on historical performance"""
        if len(self.response_times) < 10:
            return self.initial_timeout
        
        # Calculate Nth percentile
        sorted_times = sorted(self.response_times)
        index = int(len(sorted_times) * (percentile / 100))
        p95 = sorted_times[index]
        
        # Timeout = P95 + 50% buffer
        timeout = p95 * 1.5
        
        # Clamp between 1s and 30s
        return max(1.0, min(timeout, 30.0))
    
    async def execute_with_adaptive_timeout(self, operation):
        """Execute with adaptive timeout"""
        timeout = self.get_timeout()
        
        start = time.time()
        
        try:
            result = await asyncio.wait_for(
                operation(),
                timeout=timeout
            )
            
            # Record success
            duration = time.time() - start
            self.record_response_time(duration)
            
            return result
            
        except asyncio.TimeoutError:
            print(f"Operation timed out after {timeout:.2f}s")
            raise

# Usage
adaptive = AdaptiveTimeout()

# First few calls use default 5s timeout
# As we collect data, timeout adapts:
# - If service fast (avg 100ms) → timeout becomes ~150ms
# - If service slow (avg 2s) → timeout becomes ~3s
result = await adaptive.execute_with_adaptive_timeout(
    lambda: fetch_data_from_service()
)
```

---

## 6. Graceful Degradation

**Definition:** When components fail, provide reduced functionality instead of complete failure.

### Example: E-Commerce Site

```javascript
// ============================================
// Graceful Degradation
// ============================================

class ProductService {
  async getProductDetails(productId) {
    const result = {
      product: null,
      reviews: null,
      recommendations: null,
      inventory: null,
      errors: []
    };
    
    // Critical: Product data (must have)
    try {
      result.product = await this.fetchProduct(productId);
    } catch (error) {
      // Critical failure - can't continue
      throw new Error('Product not found');
    }
    
    // Non-critical: Reviews (nice to have)
    try {
      result.reviews = await withTimeout(
        this.fetchReviews(productId),
        2000
      );
    } catch (error) {
      result.errors.push('Reviews unavailable');
      result.reviews = [];  // Empty but don't fail
    }
    
    // Non-critical: Recommendations (nice to have)
    try {
      result.recommendations = await withTimeout(
        this.fetchRecommendations(productId),
        3000
      );
    } catch (error) {
      result.errors.push('Recommendations unavailable');
      result.recommendations = [];  // Use empty array
    }
    
    // Non-critical: Real-time inventory (nice to have)
    try {
      result.inventory = await withTimeout(
        this.checkInventory(productId),
        1000
      );
    } catch (error) {
      result.errors.push('Inventory check unavailable');
      result.inventory = { available: true, quantity: null };  // Assume available
    }
    
    return result;
  }
}

// User still sees:
// ✅ Product details (critical)
// ✅ Add to cart button works
// ⚠️ Reviews section shows "Currently unavailable"
// ⚠️ "You might also like" section hidden
// ⚠️ Exact quantity not shown

// Better than:
// ❌ Entire page failing
// ❌ User can't complete purchase
```

### Feature Flags for Degradation

```typescript
class FeatureFlags {
  private redis: Redis;
  
  async isEnabled(feature: string): Promise<boolean> {
    try {
      const value = await this.redis.get(`feature:${feature}`);
      return value === 'true';
    } catch (error) {
      // If Redis is down, use safe defaults
      return this.getSafeDefault(feature);
    }
  }
  
  private getSafeDefault(feature: string): boolean {
    const defaults = {
      'product_recommendations': false,  // Disable if uncertain
      'user_reviews': false,
      'real_time_inventory': false,
      'checkout': true,  // Critical - keep enabled
      'search': true
    };
    
    return defaults[feature] ?? false;
  }
}

// Usage
const features = new FeatureFlags(redis);

app.get('/api/products/:id', async (req, res) => {
  const product = await getProduct(req.params.id);
  const response = { product };
  
  // Only include recommendations if feature enabled AND working
  if (await features.isEnabled('product_recommendations')) {
    try {
      response.recommendations = await getRecommendations(req.params.id);
    } catch (error) {
      // Gracefully degrade - disable feature temporarily
      await features.disable('product_recommendations', duration: 300);
    }
  }
  
  res.json(response);
});
```

---

## 7. Rate Limiting

**Definition:** Limit number of requests to protect system from overload.

### Token Bucket Algorithm

```python
# ============================================
# Token Bucket Rate Limiter
# ============================================
import time
import threading

class TokenBucket:
    def __init__(self, capacity, refill_rate):
        """
        capacity: Maximum tokens in bucket
        refill_rate: Tokens added per second
        """
        self.capacity = capacity
        self.refill_rate = refill_rate
        self.tokens = capacity
        self.last_refill = time.time()
        self.lock = threading.Lock()
    
    def _refill(self):
        """Refill tokens based on time elapsed"""
        now = time.time()
        elapsed = now - self.last_refill
        
        # Calculate tokens to add
        tokens_to_add = elapsed * self.refill_rate
        
        # Add tokens (up to capacity)
        self.tokens = min(self.capacity, self.tokens + tokens_to_add)
        self.last_refill = now
    
    def consume(self, tokens=1):
        """Try to consume tokens"""
        with self.lock:
            self._refill()
            
            if self.tokens >= tokens:
                self.tokens -= tokens
                return True
            
            return False
    
    def get_wait_time(self, tokens=1):
        """Get time to wait before tokens available"""
        with self.lock:
            self._refill()
            
            if self.tokens >= tokens:
                return 0
            
            tokens_needed = tokens - self.tokens
            wait_time = tokens_needed / self.refill_rate
            
            return wait_time

# Usage
limiter = TokenBucket(capacity=100, refill_rate=10)  # 100 tokens, 10/sec refill

def handle_request():
    if limiter.consume(1):
        # Process request
        return process_request()
    else:
        # Rate limited
        wait_time = limiter.get_wait_time(1)
        return {
            'error': 'Rate limit exceeded',
            'retry_after': wait_time
        }

# Burst handling:
# 100 requests at once → First 100 succeed
# 101st request → Rate limited
# After 10 seconds → 100 more tokens available
```

### Sliding Window Rate Limiter

```typescript
// ============================================
// Sliding Window Rate Limiter (Redis)
// ============================================
import Redis from 'ioredis';

class SlidingWindowRateLimiter {
  private redis: Redis;
  
  constructor() {
    this.redis = new Redis();
  }
  
  async isAllowed(
    userId: string,
    maxRequests: number,
    windowSeconds: number
  ): Promise<{ allowed: boolean; remaining: number; resetAt: Date }> {
    const key = `ratelimit:${userId}`;
    const now = Date.now();
    const windowStart = now - (windowSeconds * 1000);
    
    // Remove old entries (outside window)
    await this.redis.zremrangebyscore(key, 0, windowStart);
    
    // Count requests in current window
    const count = await this.redis.zcard(key);
    
    if (count < maxRequests) {
      // Add current request
      await this.redis.zadd(key, now, `${now}-${Math.random()}`);
      await this.redis.expire(key, windowSeconds);
      
      return {
        allowed: true,
        remaining: maxRequests - count - 1,
        resetAt: new Date(now + (windowSeconds * 1000))
      };
    }
    
    // Rate limited
    const oldestEntry = await this.redis.zrange(key, 0, 0, 'WITHSCORES');
    const resetAt = new Date(parseInt(oldestEntry[1]) + (windowSeconds * 1000));
    
    return {
      allowed: false,
      remaining: 0,
      resetAt
    };
  }
}

// Usage
const limiter = new SlidingWindowRateLimiter();

app.use(async (req, res, next) => {
  const userId = req.user?.id || req.ip;
  
  const result = await limiter.isAllowed(userId, 100, 60);  // 100 req/min
  
  res.set({
    'X-RateLimit-Limit': '100',
    'X-RateLimit-Remaining': result.remaining.toString(),
    'X-RateLimit-Reset': result.resetAt.toISOString()
  });
  
  if (!result.allowed) {
    return res.status(429).json({
      error: 'Too many requests',
      retryAfter: result.resetAt
    });
  }
  
  next();
});
```

---

## 8. Fallback Patterns

### Fallback to Cache

```python
async def get_user_with_fallback(user_id):
    """Try database, fallback to cache"""
    try:
        # Try primary source
        user = await database.query(
            'SELECT * FROM users WHERE id = ?',
            [user_id]
        )
        
        # Update cache
        await cache.set(f'user:{user_id}', json.dumps(user), ex=3600)
        
        return user
        
    except Exception as e:
        print(f"Database error: {e}")
        
        # Fallback to cache
        cached = await cache.get(f'user:{user_id}')
        if cached:
            print("Serving from cache (database unavailable)")
            return json.loads(cached)
        
        # No fallback available
        raise Exception("User data unavailable")
```

### Fallback to Default Data

```javascript
async function getRecommendations(userId) {
  try {
    // Try ML recommendation service
    return await fetch(
      `https://ml-api.com/recommendations/${userId}`,
      { timeout: 2000 }
    ).then(r => r.json());
    
  } catch (error) {
    console.log('ML service unavailable, using fallback');
    
    // Fallback: Simple popularity-based recommendations
    return await db.query(`
      SELECT product_id, name, price
      FROM products
      ORDER BY sales_count DESC
      LIMIT 10
    `);
  }
}
```

### Fallback Chain

```typescript
// ============================================
// Multiple Fallback Levels
// ============================================

class FallbackChain {
  async execute<T>(operations: Array<() => Promise<T>>): Promise<T> {
    let lastError: any;
    
    for (let i = 0; i < operations.length; i++) {
      try {
        console.log(`Trying fallback level ${i + 1}`);
        return await operations[i]();
        
      } catch (error) {
        console.log(`Fallback level ${i + 1} failed: ${error.message}`);
        lastError = error;
      }
    }
    
    throw new Error(
      `All fallback levels failed. Last error: ${lastError.message}`
    );
  }
}

// Usage
const fallbackChain = new FallbackChain();

async function getUserProfile(userId: string) {
  return await fallbackChain.execute([
    // Level 1: Real-time from database
    async () => {
      return await db.query('SELECT * FROM users WHERE id = $1', [userId]);
    },
    
    // Level 2: Recent cache
    async () => {
      const cached = await redis.get(`user:${userId}`);
      if (cached) return JSON.parse(cached);
      throw new Error('Not in cache');
    },
    
    // Level 3: Stale cache (backup Redis)
    async () => {
      const cached = await backupRedis.get(`user:${userId}`);
      if (cached) return JSON.parse(cached);
      throw new Error('Not in backup cache');
    },
    
    // Level 4: Default/skeleton data
    async () => {
      return {
        id: userId,
        name: 'User',
        email: null,
        isPartial: true
      };
    }
  ]);
}
```

---

## 9. Combining Patterns

### Circuit Breaker + Retry + Timeout + Fallback

```typescript
// ============================================
// Resilient Service Client
// ============================================

interface ResilientOptions {
  timeout: number;
  maxRetries: number;
  circuitBreakerThreshold: number;
  circuitBreakerTimeout: number;
}

class ResilientServiceClient {
  private circuitBreaker: CircuitBreaker;
  
  constructor(
    private serviceName: string,
    private serviceUrl: string,
    private options: ResilientOptions
  ) {
    this.circuitBreaker = new CircuitBreaker(serviceName, {
      failureThreshold: options.circuitBreakerThreshold,
      successThreshold: 2,
      timeout: options.circuitBreakerTimeout,
      monitoringPeriod: 60000
    });
  }
  
  async call<T>(
    endpoint: string,
    fallback?: () => Promise<T>
  ): Promise<T> {
    // Wrap with circuit breaker
    return await this.circuitBreaker.execute(async () => {
      // Wrap with retry
      return await this.retryWithBackoff(async () => {
        // Wrap with timeout
        return await withTimeout(
          this.makeRequest<T>(endpoint),
          this.options.timeout
        );
      });
    }).catch(async (error) => {
      // If all else fails, use fallback
      if (fallback) {
        console.log(`Using fallback for ${endpoint}`);
        return await fallback();
      }
      throw error;
    });
  }
  
  private async retryWithBackoff<T>(
    operation: () => Promise<T>
  ): Promise<T> {
    let lastError: any;
    
    for (let attempt = 0; attempt < this.options.maxRetries; attempt++) {
      try {
        return await operation();
      } catch (error: any) {
        lastError = error;
        
        // Don't retry on client errors (4xx)
        if (error.status >= 400 && error.status < 500) {
          throw error;
        }
        
        if (attempt < this.options.maxRetries - 1) {
          const delay = Math.min(1000 * (2 ** attempt), 10000);
          await new Promise(resolve => setTimeout(resolve, delay));
        }
      }
    }
    
    throw lastError;
  }
  
  private async makeRequest<T>(endpoint: string): Promise<T> {
    const response = await fetch(`${this.serviceUrl}${endpoint}`);
    
    if (!response.ok) {
      throw new Error(`HTTP ${response.status}: ${response.statusText}`);
    }
    
    return await response.json();
  }
}

// ============================================
// Usage
// ============================================

const paymentClient = new ResilientServiceClient(
  'PaymentService',
  'https://payment-api.com',
  {
    timeout: 5000,
    maxRetries: 3,
    circuitBreakerThreshold: 5,
    circuitBreakerTimeout: 60000
  }
);

async function processPayment(orderId: string, amount: number) {
  try {
    return await paymentClient.call(
      `/charge`,
      // Fallback: Queue for later processing
      async () => {
        await queue.enqueue('payment_queue', { orderId, amount });
        return { status: 'queued', message: 'Payment queued for processing' };
      }
    );
    
  } catch (error) {
    console.error('Payment failed:', error);
    throw error;
  }
}
```

---

## 10. Real-World Implementation

### Netflix Hystrix Pattern

```java
// ============================================
// Hystrix-Style Fault Tolerance
// ============================================
import com.netflix.hystrix.*;

public class GetUserCommand extends HystrixCommand<User> {
    private final String userId;
    private final UserService userService;
    
    public GetUserCommand(String userId, UserService userService) {
        super(HystrixCommandGroupKey.Factory.asKey("UserService"));
        this.userId = userId;
        this.userService = userService;
    }
    
    @Override
    protected User run() throws Exception {
        // Primary execution
        return userService.getUser(userId);
    }
    
    @Override
    protected User getFallback() {
        // Fallback when primary fails
        System.out.println("Using fallback for user " + userId);
        
        // Try cache
        User cachedUser = cache.get(userId);
        if (cachedUser != null) {
            return cachedUser;
        }
        
        // Return default user
        return new User(userId, "Unknown User", null);
    }
}

// Usage
User user = new GetUserCommand("123", userService).execute();

// Hystrix provides:
// - Circuit breaker
// - Timeout
// - Thread pool bulkhead
// - Metrics
// - Fallback
// All in one!
```

---

## 11. Testing Fault Tolerance

### Chaos Testing

```python
# ============================================
# Chaos Testing Framework
# ============================================
import random
import requests

class ChaosTest:
    def __init__(self, base_url):
        self.base_url = base_url
    
    def test_service_failure(self):
        """Test system behavior when service fails"""
        print("\n=== Test: Service Failure ===")
        
        # Normal operation
        response = requests.get(f"{self.base_url}/api/products")
        assert response.status_code == 200
        print("✓ Normal operation works")
        
        # Kill product service
        self.kill_service('product-service')
        
        # System should degrade gracefully
        response = requests.get(f"{self.base_url}/api/products")
        
        # Might return cached data or partial results
        assert response.status_code in [200, 503]
        print("✓ Graceful degradation works")
        
        # Restore service
        self.restore_service('product-service')
    
    def test_database_failure(self):
        """Test database failover"""
        print("\n=== Test: Database Failure ===")
        
        # Kill primary database
        self.kill_database('primary')
        
        # Should failover to replica
        response = requests.get(f"{self.base_url}/api/users/123")
        assert response.status_code == 200
        print("✓ Database failover works")
        
        # Restore primary
        self.restore_database('primary')
    
    def test_network_latency(self):
        """Test high latency handling"""
        print("\n=== Test: Network Latency ===")
        
        # Inject 5s latency
        self.inject_latency('payment-service', delay_ms=5000)
        
        # Request should timeout and use fallback
        start = time.time()
        response = requests.get(f"{self.base_url}/api/checkout")
        duration = time.time() - start
        
        # Should timeout quickly (not wait 5s)
        assert duration < 3.0
        print(f"✓ Timeout works (returned in {duration:.2f}s)")
        
        # Remove latency
        self.remove_latency('payment-service')
    
    def test_partial_failure(self):
        """Test when some instances fail"""
        print("\n=== Test: Partial Failure ===")
        
        # Kill 2 out of 5 app servers
        self.kill_instances('api-server', count=2)
        
        # System should continue with 3 servers
        for i in range(100):
            response = requests.get(f"{self.base_url}/api/health")
            assert response.status_code == 200
        
        print("✓ System handles partial failure (60% capacity)")
        
        # Restore instances
        self.restore_instances('api-server', count=2)

# Run tests
chaos = ChaosTest('https://staging.example.com')
chaos.test_service_failure()
chaos.test_database_failure()
chaos.test_network_latency()
chaos.test_partial_failure()
```

---

## Chapter 20 Summary

### Key Patterns

1. **Circuit Breaker** - Stop calling failing service
2. **Retry** - Try again with exponential backoff
3. **Bulkhead** - Isolate resources
4. **Timeout** - Don't wait forever
5. **Graceful Degradation** - Partial functionality > total failure
6. **Rate Limiting** - Protect from overload
7. **Fallback** - Alternative when primary fails

### Pattern Comparison

| Pattern | Purpose | When to Use |
|---------|---------|-------------|
| **Circuit Breaker** | Prevent cascading failures | External service calls |
| **Retry** | Handle transient failures | Network errors |
| **Bulkhead** | Resource isolation | Multiple dependencies |
| **Timeout** | Prevent hanging | All I/O operations |
| **Graceful Degradation** | Partial functionality | Non-critical features |
| **Rate Limiting** | Prevent overload | Public APIs |
| **Fallback** | Alternative path | When primary fails |

### Combining Patterns

```
Typical API Call Stack:

1. Rate Limiter (protect your system)
   ↓
2. Circuit Breaker (stop calling if broken)
   ↓
3. Bulkhead (isolate resources)
   ↓
4. Timeout (don't wait forever)
   ↓
5. Retry (transient failures)
   ↓
6. Fallback (if all else fails)
```

### Implementation Checklist

For each external dependency:
- [ ] Set timeout (5-30s typical)
- [ ] Implement retry with backoff
- [ ] Add circuit breaker
- [ ] Define fallback behavior
- [ ] Isolate with bulkhead
- [ ] Add health checks
- [ ] Monitor and alert
- [ ] Document failure modes

### Interview Tips

**Common Questions:**
1. "Explain circuit breaker pattern"
2. "How do you handle external service failures?"
3. "What is the bulkhead pattern?"
4. "Retry strategies?"

**How to Answer:**
- Draw state diagrams (circuit breaker states)
- Give concrete examples with code
- Explain when to use each pattern
- Discuss combining patterns
- Mention real frameworks (Hystrix, Resilience4j)

### Fault Tolerance Libraries

| Language | Library |
|----------|---------|
| **Java** | Hystrix, Resilience4j |
| **JavaScript/TypeScript** | opossum, cockatiel |
| **Python** | pybreaker, tenacity |
| **Go** | gobreaker, resilience |
| **.NET** | Polly |

### Best Practices

1. **Design for failure** - Assume everything will fail
2. **Set appropriate timeouts** - Based on SLA
3. **Retry idempotent operations only** - Safe to repeat
4. **Use circuit breakers for external services** - Prevent cascade
5. **Implement graceful degradation** - Degrade, don't break
6. **Monitor circuit breaker states** - Alert when open
7. **Test failure scenarios** - Chaos engineering
8. **Document fallback behavior** - Team should understand

Congratulations! You've completed Chapters 17-20 covering Performance Optimization, Content Delivery, High Availability, and Fault Tolerance - essential topics for building production-grade systems!