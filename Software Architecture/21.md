# Chapter 21: Disaster Recovery

## Table of Contents
1. Introduction to Disaster Recovery
2. Backup Strategies
3. RPO and RTO
4. Multi-Region Deployment
5. Failover and Failback
6. Data Replication
7. Backup Testing and Validation
8. Disaster Recovery Plan
9. Chaos Engineering
10. Real-World DR Scenarios
11. DR Best Practices

---

## 1. Introduction to Disaster Recovery

**Definition:** Disaster Recovery (DR) is a set of policies, tools, and procedures to enable recovery or continuation of vital technology infrastructure after a disaster.

### Types of Disasters

**1. Natural Disasters**
- Earthquakes, floods, hurricanes
- Power outages
- Fire

**2. Technical Disasters**
- Hardware failures
- Data center outages
- Network failures
- Software bugs

**3. Human-Caused Disasters**
- Cyber attacks
- Accidental deletions
- Configuration errors
- Malicious insiders

### DR vs High Availability

```
High Availability (HA):
- Prevents downtime from component failures
- Automatic failover
- Seconds to minutes recovery
- Same region

Disaster Recovery (DR):
- Recovers from catastrophic failures
- Manual or automatic failover
- Minutes to hours recovery
- Different region/data center
```

---

## 2. Backup Strategies

### Types of Backups

**1. Full Backup**
- Complete copy of all data
- Slowest, most storage
- Independent restore

**2. Incremental Backup**
- Only changes since last backup (any type)
- Fast, less storage
- Requires all incremental backups to restore

**3. Differential Backup**
- Changes since last full backup
- Medium speed and storage
- Requires only last full + last differential

```
Timeline:
Sunday:    Full Backup (100 GB)
Monday:    Incremental (5 GB) - changes since Sunday
Tuesday:   Incremental (3 GB) - changes since Monday
Wednesday: Incremental (4 GB) - changes since Tuesday

To restore Wednesday data:
Incremental: Need Sunday + Monday + Tuesday + Wednesday = 4 backups
Differential: Need Sunday + Wednesday = 2 backups
```

### Implementation

```python
# ============================================
# Backup Manager
# ============================================
import subprocess
import boto3
from datetime import datetime, timedelta
import os
import gzip

class BackupManager:
    def __init__(self, db_config, s3_bucket):
        self.db_config = db_config
        self.s3_bucket = s3_bucket
        self.s3_client = boto3.client('s3')
    
    def create_full_backup(self):
        """Create full database backup"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_file = f'/backups/full_backup_{timestamp}.sql'
        compressed_file = f'{backup_file}.gz'
        
        print(f"Creating full backup: {backup_file}")
        
        # PostgreSQL dump
        subprocess.run([
            'pg_dump',
            '-h', self.db_config['host'],
            '-U', self.db_config['user'],
            '-d', self.db_config['database'],
            '-F', 'c',  # Custom format (compressed)
            '-f', backup_file
        ], check=True)
        
        # Compress
        with open(backup_file, 'rb') as f_in:
            with gzip.open(compressed_file, 'wb') as f_out:
                f_out.writelines(f_in)
        
        # Upload to S3
        self.upload_to_s3(compressed_file, 'full')
        
        # Cleanup local file
        os.remove(backup_file)
        os.remove(compressed_file)
        
        print(f"Full backup completed: {compressed_file}")
        return compressed_file
    
    def create_incremental_backup(self):
        """Create incremental backup using WAL"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_dir = f'/backups/incremental_{timestamp}'
        
        print(f"Creating incremental backup: {backup_dir}")
        
        # PostgreSQL base backup with WAL
        subprocess.run([
            'pg_basebackup',
            '-h', self.db_config['host'],
            '-U', self.db_config['user'],
            '-D', backup_dir,
            '-F', 't',  # Tar format
            '-X', 'stream',  # Include WAL
            '-P',  # Progress
            '-v'   # Verbose
        ], check=True)
        
        # Compress and upload
        compressed = f'{backup_dir}.tar.gz'
        subprocess.run(['tar', '-czf', compressed, backup_dir])
        
        self.upload_to_s3(compressed, 'incremental')
        
        print(f"Incremental backup completed: {compressed}")
        return compressed
    
    def upload_to_s3(self, file_path, backup_type):
        """Upload backup to S3"""
        file_name = os.path.basename(file_path)
        s3_key = f'backups/{backup_type}/{file_name}'
        
        print(f"Uploading to S3: s3://{self.s3_bucket}/{s3_key}")
        
        # Upload with metadata
        self.s3_client.upload_file(
            file_path,
            self.s3_bucket,
            s3_key,
            ExtraArgs={
                'StorageClass': 'STANDARD_IA',  # Infrequent Access
                'Metadata': {
                    'backup_type': backup_type,
                    'timestamp': datetime.now().isoformat(),
                    'database': self.db_config['database']
                }
            }
        )
        
        print(f"Upload completed: {s3_key}")
    
    def restore_from_backup(self, backup_file):
        """Restore database from backup"""
        print(f"Restoring from backup: {backup_file}")
        
        # Download from S3 if needed
        if backup_file.startswith('s3://'):
            backup_file = self.download_from_s3(backup_file)
        
        # Decompress if needed
        if backup_file.endswith('.gz'):
            decompressed = backup_file[:-3]
            with gzip.open(backup_file, 'rb') as f_in:
                with open(decompressed, 'wb') as f_out:
                    f_out.write(f_in.read())
            backup_file = decompressed
        
        # Restore PostgreSQL
        subprocess.run([
            'pg_restore',
            '-h', self.db_config['host'],
            '-U', self.db_config['user'],
            '-d', self.db_config['database'],
            '-c',  # Clean before restore
            '-v',  # Verbose
            backup_file
        ], check=True)
        
        print("Database restored successfully")
    
    def cleanup_old_backups(self, retention_days=30):
        """Delete backups older than retention period"""
        cutoff_date = datetime.now() - timedelta(days=retention_days)
        
        # List all backups
        response = self.s3_client.list_objects_v2(
            Bucket=self.s3_bucket,
            Prefix='backups/'
        )
        
        deleted_count = 0
        
        for obj in response.get('Contents', []):
            # Get object metadata
            metadata = self.s3_client.head_object(
                Bucket=self.s3_bucket,
                Key=obj['Key']
            )
            
            # Check age
            if obj['LastModified'].replace(tzinfo=None) < cutoff_date:
                # Delete old backup
                self.s3_client.delete_object(
                    Bucket=self.s3_bucket,
                    Key=obj['Key']
                )
                deleted_count += 1
                print(f"Deleted old backup: {obj['Key']}")
        
        print(f"Cleanup completed: {deleted_count} backups deleted")
    
    def verify_backup(self, backup_file):
        """Verify backup integrity"""
        print(f"Verifying backup: {backup_file}")
        
        # Test restore to temporary database
        test_db = f"{self.db_config['database']}_test"
        
        # Create test database
        subprocess.run([
            'createdb',
            '-h', self.db_config['host'],
            '-U', self.db_config['user'],
            test_db
        ])
        
        try:
            # Restore to test database
            subprocess.run([
                'pg_restore',
                '-h', self.db_config['host'],
                '-U', self.db_config['user'],
                '-d', test_db,
                backup_file
            ], check=True)
            
            # Verify data
            # Run some queries to ensure data is valid
            
            print("Backup verification successful")
            return True
            
        except Exception as e:
            print(f"Backup verification failed: {e}")
            return False
            
        finally:
            # Drop test database
            subprocess.run([
                'dropdb',
                '-h', self.db_config['host'],
                '-U', self.db_config['user'],
                test_db
            ])

# ============================================
# Automated Backup Schedule
# ============================================
import schedule

backup_manager = BackupManager(
    db_config={
        'host': 'localhost',
        'user': 'postgres',
        'database': 'myapp'
    },
    s3_bucket='myapp-backups'
)

# Schedule full backup daily at 2 AM
schedule.every().day.at("02:00").do(backup_manager.create_full_backup)

# Schedule incremental backup every 4 hours
schedule.every(4).hours.do(backup_manager.create_incremental_backup)

# Schedule cleanup monthly
schedule.every().month.do(backup_manager.cleanup_old_backups, retention_days=90)

# Verify last backup daily
schedule.every().day.at("03:00").do(
    lambda: backup_manager.verify_backup('/backups/latest_full.sql.gz')
)

# Run scheduler
while True:
    schedule.run_pending()
    time.sleep(60)
```

---

## 3. RPO and RTO

### Recovery Point Objective (RPO)

**Definition:** Maximum acceptable amount of data loss measured in time.

```
RPO = 1 hour
Meaning: Can lose up to 1 hour of data

Timeline:
12:00 PM - Normal operation
1:00 PM - Disaster strikes
Recover to: 12:00 PM (lost 1 hour of data)

Solution: Backup every hour or less
```

### Recovery Time Objective (RTO)

**Definition:** Maximum acceptable time to restore service.

```
RTO = 4 hours
Meaning: Must restore service within 4 hours

Timeline:
1:00 PM - Disaster strikes
5:00 PM - Service must be restored
```

### RPO/RTO Combinations

```
Scenario 1: Banking System
RPO: 0 seconds (zero data loss)
RTO: 1 minute (near-instant recovery)
Solution: Synchronous replication + automatic failover
Cost: Very High

Scenario 2: E-Commerce
RPO: 5 minutes
RTO: 1 hour
Solution: Asynchronous replication + manual failover
Cost: Medium

Scenario 3: Internal Tools
RPO: 24 hours
RTO: 8 hours
Solution: Daily backups + restore from backup
Cost: Low
```

### Implementation Based on RPO/RTO

```python
# ============================================
# RPO: Near-Zero (Synchronous Replication)
# ============================================
class ZeroRPODatabase:
    def __init__(self, primary_conn, replica_conn):
        self.primary = primary_conn
        self.replica = replica_conn
    
    async def write(self, query, params):
        """Synchronous write to both databases"""
        try:
            # Write to primary
            primary_result = await self.primary.execute(query, params)
            
            # Write to replica synchronously (wait for confirmation)
            replica_result = await self.replica.execute(query, params)
            
            # Commit both
            await self.primary.commit()
            await self.replica.commit()
            
            return primary_result
            
        except Exception as e:
            # Rollback both
            await self.primary.rollback()
            await self.replica.rollback()
            raise e

# RPO = 0 (no data loss, but slower writes)

# ============================================
# RPO: 5 Minutes (Asynchronous Replication)
# ============================================
class FiveMinuteRPODatabase:
    def __init__(self, primary_conn):
        self.primary = primary_conn
        # Replica syncs every 5 minutes
    
    async def write(self, query, params):
        """Write only to primary (fast)"""
        result = await self.primary.execute(query, params)
        await self.primary.commit()
        return result
        
        # Replica syncs in background
        # RPO = 5 minutes (might lose up to 5 min of data)

# ============================================
# RTO: 1 Minute (Hot Standby)
# ============================================
class HotStandbyFailover:
    def __init__(self):
        self.primary_healthy = True
        self.standby_ready = True
    
    async def failover_if_needed(self):
        """Check health and failover if needed"""
        if not self.primary_healthy and self.standby_ready:
            print("Primary unhealthy - failing over to standby")
            
            # Update DNS (very fast with low TTL)
            await self.update_dns(
                'api.example.com',
                new_ip='standby-ip-address'
            )
            
            # Promote standby to primary
            await self.promote_standby()
            
            print("Failover completed in 30 seconds")
            # RTO achieved: < 1 minute

# ============================================
# RTO: 4 Hours (Cold Standby)
# ============================================
class ColdStandbyRestore:
    async def restore_from_disaster(self):
        """Restore from cold backup"""
        print("Starting disaster recovery...")
        
        # 1. Provision new infrastructure (30 min)
        await self.provision_servers()
        
        # 2. Restore database from backup (2 hours)
        await self.restore_database_from_s3()
        
        # 3. Deploy application (30 min)
        await self.deploy_application()
        
        # 4. Update DNS (5 min)
        await self.update_dns_to_dr_site()
        
        # 5. Verify and test (55 min)
        await self.run_smoke_tests()
        
        print("Recovery completed in ~4 hours")
        # RTO achieved: 4 hours
```

---

## 4. Multi-Region Deployment

### Active-Passive Setup

```python
# ============================================
# Multi-Region Active-Passive
# ============================================
import boto3

class MultiRegionDR:
    def __init__(self):
        self.primary_region = 'us-east-1'
        self.dr_region = 'us-west-2'
        
        self.primary_rds = boto3.client('rds', region_name=self.primary_region)
        self.dr_rds = boto3.client('rds', region_name=self.dr_region)
        self.route53 = boto3.client('route53')
    
    def setup_dr_environment(self):
        """Setup disaster recovery environment"""
        
        # 1. Create read replica in DR region
        print("Creating cross-region read replica...")
        
        self.primary_rds.create_db_instance_read_replica(
            DBInstanceIdentifier='myapp-dr-replica',
            SourceDBInstanceIdentifier='arn:aws:rds:us-east-1:123456789:db:myapp-primary',
            DBInstanceClass='db.m5.large',
            AvailabilityZone='us-west-2a',
            PubliclyAccessible=False
        )
        
        # 2. Setup application servers in DR region (stopped)
        # Keep AMIs ready but instances stopped to save cost
        
        # 3. Configure Route 53 health checks
        health_check = self.route53.create_health_check(
            HealthCheckConfig={
                'Type': 'HTTPS',
                'ResourcePath': '/health',
                'FullyQualifiedDomainName': 'api.example.com',
                'Port': 443,
                'RequestInterval': 30,
                'FailureThreshold': 3
            }
        )
        
        # 4. Setup DNS failover
        self.route53.change_resource_record_sets(
            HostedZoneId='Z1234567890',
            ChangeBatch={
                'Changes': [
                    {
                        'Action': 'CREATE',
                        'ResourceRecordSet': {
                            'Name': 'api.example.com',
                            'Type': 'A',
                            'SetIdentifier': 'Primary',
                            'Failover': 'PRIMARY',
                            'AliasTarget': {
                                'HostedZoneId': 'Z11111111',
                                'DNSName': 'primary-alb.us-east-1.elb.amazonaws.com',
                                'EvaluateTargetHealth': True
                            },
                            'HealthCheckId': health_check['HealthCheck']['Id']
                        }
                    },
                    {
                        'Action': 'CREATE',
                        'ResourceRecordSet': {
                            'Name': 'api.example.com',
                            'Type': 'A',
                            'SetIdentifier': 'DR',
                            'Failover': 'SECONDARY',
                            'AliasTarget': {
                                'HostedZoneId': 'Z22222222',
                                'DNSName': 'dr-alb.us-west-2.elb.amazonaws.com',
                                'EvaluateTargetHealth': False
                            }
                        }
                    }
                ]
            }
        )
        
        print("DR environment setup complete")
    
    def initiate_failover(self):
        """Manually initiate failover to DR region"""
        print("Initiating failover to DR region...")
        
        # 1. Promote read replica to standalone database
        self.dr_rds.promote_read_replica(
            DBInstanceIdentifier='myapp-dr-replica'
        )
        
        print("Database promoted in DR region")
        
        # 2. Start application servers in DR region
        ec2_dr = boto3.client('ec2', region_name=self.dr_region)
        
        # Get stopped instances
        instances = ec2_dr.describe_instances(
            Filters=[
                {'Name': 'tag:Environment', 'Values': ['DR']},
                {'Name': 'instance-state-name', 'Values': ['stopped']}
            ]
        )
        
        instance_ids = [
            i['InstanceId'] 
            for r in instances['Reservations'] 
            for i in r['Instances']
        ]
        
        if instance_ids:
            ec2_dr.start_instances(InstanceIds=instance_ids)
            print(f"Started {len(instance_ids)} instances in DR region")
        
        # 3. DNS will automatically failover based on health checks
        # Or manually update if needed
        
        print("Failover initiated - ETA: 10-15 minutes")
    
    def failback_to_primary(self):
        """Return to primary region after disaster resolved"""
        print("Initiating failback to primary region...")
        
        # 1. Ensure primary region is healthy
        # 2. Sync data from DR to primary
        # 3. Update DNS to point back to primary
        # 4. Stop DR instances
        
        print("Failback completed")
```

---

## 5. Failover and Failback

### Automated Failover

```javascript
// ============================================
// Automated Failover Script
// ============================================

class AutomatedFailover {
  private healthCheckInterval: NodeJS.Timeout;
  private failureCount: number = 0;
  private readonly maxFailures: number = 3;
  
  constructor() {
    this.startHealthChecks();
  }
  
  private startHealthChecks(): void {
    // Check primary health every 30 seconds
    this.healthCheckInterval = setInterval(async () => {
      await this.checkAndFailover();
    }, 30000);
  }
  
  private async checkAndFailover(): Promise<void> {
    const isPrimaryHealthy = await this.checkPrimaryHealth();
    
    if (!isPrimaryHealthy) {
      this.failureCount++;
      console.log(
        `Primary unhealthy (${this.failureCount}/${this.maxFailures})`
      );
      
      if (this.failureCount >= this.maxFailures) {
        console.log('Threshold reached - initiating failover');
        await this.executeFailover();
      }
    } else {
      this.failureCount = 0;  // Reset on success
    }
  }
  
  private async checkPrimaryHealth(): Promise<boolean> {
    try {
      const response = await fetch('https://primary.example.com/health', {
        timeout: 5000
      });
      return response.status === 200;
    } catch (error) {
      return false;
    }
  }
  
  private async executeFailover(): Promise<void> {
    console.log('=== EXECUTING FAILOVER ===');
    
    // 1. Verify DR site is ready
    const isDRReady = await this.checkDRHealth();
    if (!isDRReady) {
      console.error('DR site not ready - manual intervention required');
      await this.alertOncall('DR failover failed - DR site unhealthy');
      return;
    }
    
    // 2. Promote DR database
    await this.promoteDRDatabase();
    
    // 3. Update DNS
    await this.updateDNS('api.example.com', 'dr-region-ip');
    
    // 4. Start DR application servers
    await this.startDRServers();
    
    // 5. Verify DR is serving traffic
    await this.verifyDRTraffic();
    
    // 6. Send notifications
    await this.notifyStakeholders('Failover to DR completed');
    
    console.log('=== FAILOVER COMPLETE ===');
    
    // Stop health checks for primary
    clearInterval(this.healthCheckInterval);
  }
  
  private async checkDRHealth(): Promise<boolean> {
    try {
      const response = await fetch('https://dr.example.com/health', {
        timeout: 5000
      });
      return response.status === 200;
    } catch (error) {
      return false;
    }
  }
}

// Start automated failover monitor
const failoverMonitor = new AutomatedFailover();
```

### Failback Process

```python
# ============================================
# Failback to Primary Region
# ============================================

class FailbackManager:
    def __init__(self):
        self.current_active = 'DR'  # Currently on DR
        self.target_active = 'PRIMARY'
    
    async def execute_failback(self):
        """Safely failback to primary region"""
        
        print("=== STARTING FAILBACK ===")
        
        # 1. Verify primary region is healthy
        if not await self.verify_primary_healthy():
            raise Exception("Primary region not healthy - cannot failback")
        
        # 2. Stop writes to DR database
        print("Setting DR database to read-only...")
        await self.set_database_readonly('DR')
        
        # 3. Sync final changes from DR to primary
        print("Syncing final changes to primary...")
        await self.sync_databases('DR', 'PRIMARY')
        
        # 4. Verify data consistency
        print("Verifying data consistency...")
        if not await self.verify_data_consistency():
            raise Exception("Data inconsistency detected - aborting failback")
        
        # 5. Update DNS to primary (gradual shift)
        print("Shifting traffic to primary (weighted routing)...")
        
        # 10% of traffic
        await self.update_dns_weights('PRIMARY', 10, 'DR', 90)
        await asyncio.sleep(300)  # Wait 5 minutes
        
        # 50% of traffic
        await self.update_dns_weights('PRIMARY', 50, 'DR', 50)
        await asyncio.sleep(300)
        
        # 100% of traffic
        await self.update_dns_weights('PRIMARY', 100, 'DR', 0)
        
        # 6. Set primary to read-write
        print("Setting primary to read-write...")
        await self.set_database_readwrite('PRIMARY')
        
        # 7. Shutdown DR active components
        print("Shutting down DR active mode...")
        await self.deactivate_dr_site()
        
        # 8. Re-establish replication PRIMARY → DR
        print("Re-establishing replication...")
        await self.setup_replication('PRIMARY', 'DR')
        
        print("=== FAILBACK COMPLETE ===")
        self.current_active = 'PRIMARY'
```

---

## 6. Data Replication

### Synchronous vs Asynchronous Replication

```
Synchronous Replication:
Write → Primary DB → Replicate to Standby → Acknowledge to App
        (waits for standby confirmation)
        
Pros: Zero data loss (RPO = 0)
Cons: Slower writes (network latency)

Asynchronous Replication:
Write → Primary DB → Acknowledge to App
        ↓ (background)
        Replicate to Standby

Pros: Fast writes
Cons: Potential data loss (RPO = replication lag)
```

### MySQL Replication Setup

```sql
-- ============================================
-- Master Configuration (my.cnf)
-- ============================================
[mysqld]
server-id=1
log-bin=mysql-bin
binlog-do-db=myapp
binlog-format=ROW

-- Create replication user
CREATE USER 'replicator'@'%' IDENTIFIED BY 'password';
GRANT REPLICATION SLAVE ON *.* TO 'replicator'@'%';
FLUSH PRIVILEGES;

-- Get master status
SHOW MASTER STATUS;
-- Note: File = mysql-bin.000001, Position = 154

-- ============================================
-- Slave Configuration (my.cnf)
-- ============================================
[mysqld]
server-id=2
relay-log=relay-bin
read-only=1

-- Configure replication
CHANGE MASTER TO
  MASTER_HOST='master.db.example.com',
  MASTER_USER='replicator',
  MASTER_PASSWORD='password',
  MASTER_LOG_FILE='mysql-bin.000001',
  MASTER_LOG_POS=154;

-- Start replication
START SLAVE;

-- Check status
SHOW SLAVE STATUS\G
-- Look for: Slave_IO_Running: Yes, Slave_SQL_Running: Yes
```

### PostgreSQL Streaming Replication

```bash
# ============================================
# Primary Server (postgresql.conf)
# ============================================
wal_level = replica
max_wal_senders = 10
wal_keep_size = 1GB
synchronous_commit = on  # For synchronous replication

# Create replication user
psql -U postgres -c "CREATE ROLE replicator WITH REPLICATION LOGIN PASSWORD 'password';"

# pg_hba.conf
host replication replicator standby-ip/32 md5

# ============================================
# Standby Server
# ============================================

# Take base backup
pg_basebackup -h primary.db.example.com -U replicator -D /var/lib/postgresql/data -P -X stream

# Create standby.signal file
touch /var/lib/postgresql/data/standby.signal

# Configure (postgresql.conf)
primary_conninfo = 'host=primary.db.example.com port=5432 user=replicator password=password'
promote_trigger_file = '/tmp/promote_to_primary'

# Start PostgreSQL
systemctl start postgresql

# Check replication status
psql -c "SELECT * FROM pg_stat_replication;"
```

---

## 7. Backup Testing and Validation

### Automated Backup Testing

```python
# ============================================
# Backup Validation System
# ============================================
import subprocess
import psycopg2
import smtplib
from email.mime.text import MIMEText

class BackupValidator:
    def __init__(self, backup_file, test_db_config, notification_email):
        self.backup_file = backup_file
        self.test_db_config = test_db_config
        self.notification_email = notification_email
    
    def validate_backup(self):
        """Complete backup validation"""
        results = {
            'backup_file': self.backup_file,
            'timestamp': datetime.now().isoformat(),
            'tests': {}
        }
        
        try:
            # Test 1: File integrity
            results['tests']['file_integrity'] = self.test_file_integrity()
            
            # Test 2: Restore to test database
            results['tests']['restore'] = self.test_restore()
            
            # Test 3: Data verification
            results['tests']['data_verification'] = self.test_data_verification()
            
            # Test 4: Application compatibility
            results['tests']['app_compatibility'] = self.test_application()
            
            # All tests passed
            results['overall'] = 'PASSED'
            print("✅ Backup validation PASSED")
            
        except Exception as e:
            results['overall'] = 'FAILED'
            results['error'] = str(e)
            print(f"❌ Backup validation FAILED: {e}")
        
        finally:
            # Cleanup test database
            self.cleanup_test_database()
            
            # Send notification
            self.send_notification(results)
        
        return results
    
    def test_file_integrity(self):
        """Verify backup file is not corrupted"""
        print("Testing file integrity...")
        
        # Check file exists
        if not os.path.exists(self.backup_file):
            raise Exception(f"Backup file not found: {self.backup_file}")
        
        # Check file size (should be > 1MB for real backup)
        file_size = os.path.getsize(self.backup_file)
        if file_size < 1024 * 1024:
            raise Exception(f"Backup file too small: {file_size} bytes")
        
        # Try to decompress if gzipped
        if self.backup_file.endswith('.gz'):
            try:
                with gzip.open(self.backup_file, 'rb') as f:
                    f.read(1024)  # Read first 1KB
            except Exception as e:
                raise Exception(f"Backup file corrupted: {e}")
        
        return 'PASSED'
    
    def test_restore(self):
        """Test restoring backup to test database"""
        print("Testing restore...")
        
        # Create test database
        subprocess.run([
            'createdb',
            '-h', self.test_db_config['host'],
            '-U', self.test_db_config['user'],
            'backup_test_db'
        ], check=True)
        
        # Restore backup
        subprocess.run([
            'pg_restore',
            '-h', self.test_db_config['host'],
            '-U', self.test_db_config['user'],
            '-d', 'backup_test_db',
            '-v',  # Verbose
            self.backup_file
        ], check=True)
        
        return 'PASSED'
    
    def test_data_verification(self):
        """Verify critical data exists"""
        print("Verifying data...")
        
        conn = psycopg2.connect(
            host=self.test_db_config['host'],
            database='backup_test_db',
            user=self.test_db_config['user']
        )
        cursor = conn.cursor()
        
        # Check 1: Tables exist
        cursor.execute("""
            SELECT COUNT(*) 
            FROM information_schema.tables 
            WHERE table_schema = 'public'
        """)
        table_count = cursor.fetchone()[0]
        
        if table_count == 0:
            raise Exception("No tables found in backup")
        
        # Check 2: Critical tables have data
        critical_tables = ['users', 'orders', 'products']
        for table in critical_tables:
            cursor.execute(f"SELECT COUNT(*) FROM {table}")
            count = cursor.fetchone()[0]
            
            if count == 0:
                raise Exception(f"Critical table {table} is empty")
            
            print(f"  {table}: {count} rows")
        
        # Check 3: Foreign key constraints valid
        cursor.execute("SELECT COUNT(*) FROM orders WHERE user_id NOT IN (SELECT id FROM users)")
        orphaned = cursor.fetchone()[0]
        
        if orphaned > 0:
            raise Exception(f"Found {orphaned} orphaned orders")
        
        conn.close()
        
        return 'PASSED'
    
    def send_notification(self, results):
        """Send email notification with results"""
        subject = f"Backup Validation: {results['overall']}"
        
        body = f"""
        Backup Validation Report
        ========================
        
        Backup File: {results['backup_file']}
        Timestamp: {results['timestamp']}
        Overall Result: {results['overall']}
        
        Test Results:
        - File Integrity: {results['tests'].get('file_integrity', 'N/A')}
        - Restore: {results['tests'].get('restore', 'N/A')}
        - Data Verification: {results['tests'].get('data_verification', 'N/A')}
        
        {"Error: " + results.get('error', '') if results['overall'] == 'FAILED' else ''}
        """
        
        msg = MIMEText(body)
        msg['Subject'] = subject
        msg['From'] = 'backups@example.com'
        msg['To'] = self.notification_email
        
        # Send email
        with smtplib.SMTP('smtp.example.com') as server:
            server.send_message(msg)

# Schedule validation
validator = BackupValidator(
    backup_file='/backups/full_backup_20240115.sql.gz',
    test_db_config={'host': 'test-db.example.com', 'user': 'postgres'},
    notification_email='devops@example.com'
)

# Run daily
schedule.every().day.at("04:00").do(validator.validate_backup)
```

---

## 8. Disaster Recovery Plan

### DR Runbook Template

```markdown
# Disaster Recovery Runbook

## 1. Detection & Notification (Target: 5 minutes)

### Automatic Detection
- Monitoring alerts trigger (Datadog, PagerDuty)
- Health check failures (3 consecutive)
- Customer reports

### Manual Detection
- On-call engineer verifies issue
- Determines if DR needed

### Notification
- Page DR team via PagerDuty
- Update status page (status.example.com)
- Notify executives if customer-impacting

## 2. Assessment (Target: 10 minutes)

### Verify Disaster Scope
- [ ] Is primary region completely down?
- [ ] Is it affecting all services or partial?
- [ ] What is estimated recovery time for primary?

### Check DR Readiness
- [ ] Is DR site healthy?
- [ ] Is database replication up-to-date?
- [ ] Are DR servers ready?

### Decision Point
- If primary can recover < 15 minutes → Wait
- If primary recovery > 15 minutes → Initiate DR

## 3. Failover Execution (Target: 30 minutes)

### Database Failover
```bash
# Promote DR replica to primary
aws rds promote-read-replica \
  --db-instance-identifier myapp-dr-replica \
  --region us-west-2

# Wait for promotion (5-10 minutes)
aws rds wait db-instance-available \
  --db-instance-identifier myapp-dr-replica
```

### Application Failover
```bash
# Start application servers in DR
aws ec2 start-instances \
  --instance-ids i-1234567890abcdef0 i-0987654321fedcba0

# Wait for instances
aws ec2 wait instance-running \
  --instance-ids i-1234567890abcdef0 i-0987654321fedcba0

# Verify health
curl https://dr-api.example.com/health
```

### DNS Update
```bash
# Update Route 53 to point to DR
aws route53 change-resource-record-sets \
  --hosted-zone-id Z1234567890 \
  --change-batch file://failover-dns.json
```

## 4. Verification (Target: 15 minutes)

### Smoke Tests
- [ ] Homepage loads
- [ ] User can login
- [ ] User can view data
- [ ] Critical workflows work (checkout, payment)

### Monitoring
- [ ] All health checks passing
- [ ] Error rates normal
- [ ] Latency acceptable

## 5. Communication

### Internal
- Update #incident-response Slack channel
- Post in company all-hands

### External
- Update status page
- Send customer email if needed
- Post on social media

## 6. Post-Incident

### Immediate (within 24 hours)
- [ ] Document timeline
- [ ] Preserve logs
- [ ] Initial root cause analysis

### Short-term (within 1 week)
- [ ] Full post-mortem
- [ ] Identify action items
- [ ] Update runbook based on learnings

### Long-term
- [ ] Implement improvements
- [ ] Test DR plan again
- [ ] Update documentation
```

---

## 9. Chaos Engineering

### Chaos Experiments

```python
# ============================================
# Chaos Engineering Experiments
# ============================================

class ChaosExperiments:
    def __init__(self, environment='staging'):
        self.environment = environment
        self.aws = boto3.client('ec2')
    
    def experiment_1_kill_random_instance(self):
        """Kill random application instance"""
        print("\n=== Experiment 1: Kill Random Instance ===")
        print("Hypothesis: System should remain available with N-1 instances")
        
        # Get running instances
        instances = self.get_running_instances('api-server')
        
        if len(instances) < 2:
            print("Not enough instances for this experiment")
            return
        
        # Kill random instance
        victim = random.choice(instances)
        print(f"Killing instance: {victim}")
        
        self.aws.terminate_instances(InstanceIds=[victim])
        
        # Monitor system
        time.sleep(10)
        
        # Verify system still healthy
        for i in range(10):
            response = requests.get('https://api.example.com/health')
            assert response.status_code == 200, "System unhealthy!"
            time.sleep(1)
        
        print("✅ System remained healthy")
    
    def experiment_2_database_latency(self):
        """Inject database latency"""
        print("\n=== Experiment 2: Database Latency ===")
        print("Hypothesis: System should timeout and use cache")
        
        # Inject 10s latency to database
        self.inject_network_latency('database', delay_ms=10000)
        
        # Make API call
        start = time.time()
        response = requests.get('https://api.example.com/api/users/123')
        duration = time.time() - start
        
        # Should timeout quickly (not wait 10s)
        assert duration < 5, f"Timeout not working: {duration}s"
        
        # Should use cached data
        assert response.status_code == 200, "Fallback failed"
        
        print(f"✅ Request completed in {duration:.2f}s using cache")
        
        # Remove latency
        self.remove_network_latency('database')
    
    def experiment_3_disk_full(self):
        """Fill disk to test behavior"""
        print("\n=== Experiment 3: Disk Full ===")
        print("Hypothesis: System should alert and stop gracefully")
        
        # Fill disk to 95%
        self.fill_disk_to_percent(95)
        
        # System should:
        # 1. Trigger disk space alert
        # 2. Stop accepting writes
        # 3. Return appropriate errors
        
        response = requests.post('https://api.example.com/api/upload')
        assert response.status_code == 507, "Should return 507 Insufficient Storage"
        
        print("✅ System handled disk full correctly")
        
        # Cleanup
        self.cleanup_disk()
    
    def experiment_4_regional_failure(self):
        """Simulate entire region failure"""
        print("\n=== Experiment 4: Regional Failure ===")
        print("Hypothesis: Should failover to DR region")
        
        # Simulate primary region down
        self.simulate_region_outage('us-east-1')
        
        # Wait for DNS failover
        time.sleep(60)
        
        # Verify DR region serving traffic
        for i in range(20):
            response = requests.get('https://api.example.com/health')
            assert response.status_code == 200
            time.sleep(1)
        
        print("✅ Failover to DR region successful")
        
        # Restore primary region
        self.restore_region('us-east-1')

# ============================================
# Scheduled Chaos Tests
# ============================================

def run_weekly_chaos():
    """Run chaos experiments weekly"""
    experiments = ChaosExperiments(environment='staging')
    
    # Week 1: Instance failure
    experiments.experiment_1_kill_random_instance()
    
    # Week 2: Database latency
    experiments.experiment_2_database_latency()
    
    # Week 3: Disk full
    experiments.experiment_3_disk_full()
    
    # Week 4: Regional failure
    experiments.experiment_4_regional_failure()

# Schedule for every Friday at 2 PM
schedule.every().friday.at("14:00").do(run_weekly_chaos)
```

---

## 10. Real-World DR Scenarios

### Scenario 1: AWS Region Failure

```python
# ============================================
# Real Event: AWS US-EAST-1 Outage
# ============================================

"""
Date: December 7, 2021
Duration: ~5 hours
Affected: Many major services (Netflix, Disney+, Robinhood)

Timeline:
11:00 AM - Region begins experiencing issues
11:15 AM - Multiple availability zones impacted
11:30 AM - Companies begin failover to other regions
4:00 PM - Services gradually restored

Lessons Learned:
1. Multi-region deployment is critical
2. Automated failover saves time
3. DNS TTL should be low (60s not 300s)
4. Regular DR drills prepare teams
5. Communication plan essential
"""

class AWSRegionFailover:
    def handle_region_outage(self):
        """Response to region outage"""
        
        # 1. Confirm outage (check AWS status page)
        # 2. Initiate failover to us-west-2
        # 3. Promote read replica
        # 4. Start application servers
        # 5. Update Route 53
        # 6. Monitor and verify
        # 7. Communicate to customers
        
        # Companies with good DR:
        # - Failed over in < 15 minutes
        # - Minimal customer impact
        
        # Companies without DR:
        # - Down for 5 hours
        # - Significant revenue loss
```

### Scenario 2: Database Corruption

```python
# ============================================
# Scenario: Accidental Data Deletion
# ============================================

"""
Event: Engineer runs DELETE without WHERE clause
Result: Deleted 100,000 user records

Recovery Steps:
"""

def recover_from_accidental_deletion():
    """Point-in-time recovery"""
    
    # 1. Immediately stop all writes
    print("Setting database to read-only...")
    db.execute("ALTER DATABASE myapp SET default_transaction_read_only = on;")
    
    # 2. Identify deletion time
    deletion_time = '2024-01-15 14:30:00'
    
    # 3. Restore to point just before deletion
    print("Restoring to point-in-time...")
    
    # AWS RDS Point-in-Time Restore
    rds = boto3.client('rds')
    rds.restore_db_instance_to_point_in_time(
        SourceDBInstanceIdentifier='myapp-production',
        TargetDBInstanceIdentifier='myapp-recovery',
        RestoreTime=datetime.fromisoformat(deletion_time),
        DBInstanceClass='db.m5.2xlarge'
    )
    
    # 4. Wait for restore (30-60 minutes)
    print("Waiting for restore to complete...")
    
    # 5. Verify data
    print("Verifying restored data...")
    # Check user count, run validation queries
    
    # 6. Switch traffic to recovered database
    print("Switching to recovered database...")
    
    # 7. Restore read-write mode
    db.execute("ALTER DATABASE myapp SET default_transaction_read_only = off;")
    
    print("Recovery complete")

# Prevention:
# - Require WHERE clause in DELETE
# - Use soft deletes (deleted_at column)
# - Restrict DELETE permissions
# - Audit all deletions
```

---

## 11. DR Best Practices

### DR Tiers

```
Tier 0: No DR
- No backups
- No recovery plan
Cost: $0
RTO: Unknown
RPO: Unknown (total data loss)

Tier 1: Backup and Restore
- Regular backups to different region
- Manual restore process
Cost: Low ($100-500/month)
RTO: 24+ hours
RPO: 24 hours

Tier 2: Pilot Light
- Minimal DR environment always running
- Database replication
- Servers stopped, AMIs ready
Cost: Medium ($500-2000/month)
RTO: 4 hours
RPO: 1 hour

Tier 3: Warm Standby
- DR environment scaled down but running
- Database replication
- Can scale up quickly
Cost: High ($2000-5000/month)
RTO: 1 hour
RPO: 5 minutes

Tier 4: Active-Active
- Full production in multiple regions
- Both serve traffic
Cost: Very High ($10000+/month)
RTO: < 1 minute
RPO: Near-zero
```

### DR Checklist

```python
# ============================================
# Pre-Disaster Checklist
# ============================================

DR_CHECKLIST = {
    'Backups': [
        'Full backups scheduled',
        'Incremental backups scheduled',
        'Backups stored in different region',
        'Backup restoration tested monthly',
        'Backup retention policy defined',
        'Automated backup monitoring'
    ],
    
    'Replication': [
        'Database replication configured',
        'Object storage replication enabled',
        'Replication lag monitored',
        'Cross-region replication tested'
    ],
    
    'Infrastructure': [
        'DR region infrastructure provisioned',
        'AMIs/images available in DR region',
        'Network configured in DR region',
        'DNS failover configured',
        'Load balancers in DR region'
    ],
    
    'Monitoring': [
        'Health checks on primary',
        'Health checks on DR',
        'Automated alerting configured',
        'Runbook accessible 24/7'
    ],
    
    'Documentation': [
        'DR plan documented',
        'Runbook updated',
        'Contact list current',
        'Escalation procedures defined'
    ],
    
    'Testing': [
        'DR drill scheduled quarterly',
        'Last drill completed successfully',
        'Issues from last drill resolved',
        'New team members trained'
    ],
    
    'Communication': [
        'Status page configured',
        'Customer communication template',
        'Internal communication plan',
        'External communication plan'
    ]
}

def verify_dr_readiness():
    """Verify DR readiness"""
    total_items = sum(len(items) for items in DR_CHECKLIST.values())
    completed_items = 0
    
    for category, items in DR_CHECKLIST.items():
        print(f"\n{category}:")
        for item in items:
            # Check if item completed
            status = check_item_status(item)
            print(f"  {'✅' if status else '❌'} {item}")
            if status:
                completed_items += 1
    
    readiness_percent = (completed_items / total_items) * 100
    print(f"\nDR Readiness: {readiness_percent:.1f}%")
    
    if readiness_percent < 80:
        print("⚠️  WARNING: DR readiness below 80%")
    else:
        print("✅ DR readiness acceptable")
```

---

## Chapter 21 Summary

### Key Concepts

1. **Disaster Recovery** - Plan for catastrophic failures
2. **Backup Types** - Full, Incremental, Differential
3. **RPO** - Recovery Point Objective (data loss)
4. **RTO** - Recovery Time Objective (downtime)
5. **Replication** - Synchronous vs Asynchronous
6. **Failover** - Switch to DR site
7. **Failback** - Return to primary site
8. **Testing** - Regular DR drills

### Backup Strategy

| Frequency | Type | Retention | Storage |
|-----------|------|-----------|---------|
| **Daily** | Full | 30 days | S3 Standard |
| **4 hours** | Incremental | 7 days | S3 Standard |
| **Continuous** | WAL/Binlog | 24 hours | S3 Standard |
| **Monthly** | Full | 7 years | S3 Glacier |

### DR Tiers Decision

```
Choose based on:
- Business criticality
- Budget
- Acceptable downtime
- Acceptable data loss

E-commerce: Tier 3-4 (revenue loss high)
Internal tools: Tier 1-2 (can tolerate downtime)
Financial systems: Tier 4 (regulatory requirements)
```

### Interview Tips

**Common Questions:**
1. "What is RPO and RTO?"
2. "How do you design a disaster recovery plan?"
3. "Explain backup strategies"
4. "How do you test DR?"

**How to Answer:**
- Define RPO/RTO with examples
- Draw multi-region architecture
- Explain different DR tiers
- Discuss testing importance
- Give real scenarios (AWS outage)

### Critical Success Factors

1. **Test regularly** - DR drill every quarter
2. **Automate** - Manual processes fail under pressure
3. **Document** - Clear runbooks accessible 24/7
4. **Monitor** - Know immediately when disaster strikes
5. **Communicate** - Internal and external stakeholders
6. **Update** - Learn from each incident/drill

### Next Steps

Chapter 22 will cover **Authentication & Authorization** - securing your application with proper identity and access management.