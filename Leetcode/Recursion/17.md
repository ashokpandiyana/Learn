# Chapter 17: Complexity Analysis of Recursion

## 17.1 Recurrence Relations

### What is a Recurrence Relation?

A **recurrence relation** expresses the running time of a recursive algorithm in terms of its input size.

**General Form:**
```
T(n) = Time for problem of size n
T(n) = work_outside_recursion + recursive_calls
```

### Common Recurrence Patterns

#### Pattern 1: Linear Reduction (n â†’ n-1)

**Example: Factorial**
```python
def factorial(n):
    if n <= 1:
        return 1
    return n * factorial(n - 1)
```

**Recurrence:**
```
T(n) = T(n-1) + O(1)
       â†‘       â†‘
    recursive constant
      call    work

T(1) = O(1) [base case]
```

**Expansion:**
```
T(n) = T(n-1) + c
     = T(n-2) + c + c
     = T(n-3) + c + c + c
     = ...
     = T(1) + (n-1)c
     = O(1) + O(n)
     = O(n)
```

**Time Complexity: O(n)**

---

#### Pattern 2: Linear Reduction with Linear Work (n â†’ n-1, O(n) work)

**Example: Print all prefixes**
```python
def print_prefixes(s, i=0):
    if i >= len(s):
        return
    print(s[:i+1])  # O(i) work
    print_prefixes(s, i + 1)
```

**Recurrence:**
```
T(n) = T(n-1) + O(n)

T(n) = T(n-1) + n
     = T(n-2) + (n-1) + n
     = T(n-3) + (n-2) + (n-1) + n
     = ...
     = 1 + 2 + 3 + ... + n
     = n(n+1)/2
     = O(nÂ²)
```

**Time Complexity: O(nÂ²)**

---

#### Pattern 3: Binary Reduction (n â†’ n/2)

**Example: Binary Search**
```python
def binary_search(arr, target, left, right):
    if left > right:
        return -1
    
    mid = (left + right) // 2
    if arr[mid] == target:
        return mid
    elif target < arr[mid]:
        return binary_search(arr, target, left, mid - 1)
    else:
        return binary_search(arr, target, mid + 1, right)
```

**Recurrence:**
```
T(n) = T(n/2) + O(1)
       â†‘        â†‘
    search   constant
    half     work

T(1) = O(1)
```

**Expansion:**
```
T(n) = T(n/2) + c
     = T(n/4) + c + c
     = T(n/8) + c + c + c
     = ...
     = T(1) + logâ‚‚(n) Â· c
     = O(log n)
```

**Time Complexity: O(log n)**

---

#### Pattern 4: Binary Recursion (Two calls, n â†’ n-1)

**Example: Fibonacci**
```python
def fibonacci(n):
    if n <= 1:
        return n
    return fibonacci(n - 1) + fibonacci(n - 2)
```

**Recurrence:**
```
T(n) = T(n-1) + T(n-2) + O(1)
       â†‘        â†‘        â†‘
    two recursive    constant
      calls          work

T(0) = T(1) = O(1)
```

**Expansion (approximate):**
```
Each call makes 2 recursive calls
Height of tree: n
Nodes at level i: 2^i

Total nodes â‰ˆ 2^0 + 2^1 + 2^2 + ... + 2^n
            = 2^(n+1) - 1
            = O(2^n)
```

**Time Complexity: O(2^n)** (exponential!)

---

#### Pattern 5: Divide and Conquer (Split + Merge)

**Example: Merge Sort**
```python
def merge_sort(arr):
    if len(arr) <= 1:
        return arr
    
    mid = len(arr) // 2
    left = merge_sort(arr[:mid])     # T(n/2)
    right = merge_sort(arr[mid:])    # T(n/2)
    return merge(left, right)        # O(n)
```

**Recurrence:**
```
T(n) = 2T(n/2) + O(n)
       â†‘         â†‘
    two calls  merge
    of n/2     work

T(1) = O(1)
```

**Time Complexity: O(n log n)** (by Master Theorem)

---

## 17.2 Master Theorem

### The Master Theorem Formula

For recurrences of the form:
```
T(n) = aT(n/b) + f(n)
```

Where:
- **a** â‰¥ 1: number of recursive calls
- **b** > 1: factor by which problem size is divided
- **f(n)**: work done outside recursive calls

**Key Value:** n^(log_b(a))

### The Three Cases

**Case 1:** f(n) = O(n^c) where c < log_b(a)
```
T(n) = Î˜(n^(log_b(a)))
```
*Recursive calls dominate*

**Case 2:** f(n) = Î˜(n^c) where c = log_b(a)
```
T(n) = Î˜(n^c Â· log n)
```
*Balanced: recursive calls and work are equal*

**Case 3:** f(n) = Î©(n^c) where c > log_b(a)
```
T(n) = Î˜(f(n))
```
*Work outside dominates*

---

### Example 1: Binary Search

```python
def binary_search(arr, target, left, right):
    # T(n) = T(n/2) + O(1)
```

**Analysis:**
```
a = 1 (one recursive call)
b = 2 (divide by 2)
f(n) = O(1)

log_b(a) = logâ‚‚(1) = 0
f(n) = O(1) = O(n^0)

Compare: c = 0, log_b(a) = 0
â†’ c = log_b(a) â†’ Case 2

T(n) = Î˜(n^0 Â· log n) = Î˜(log n)
```

**Result: O(log n)**

---

### Example 2: Merge Sort

```python
def merge_sort(arr):
    # T(n) = 2T(n/2) + O(n)
```

**Analysis:**
```
a = 2 (two recursive calls)
b = 2 (divide by 2)
f(n) = O(n)

log_b(a) = logâ‚‚(2) = 1
f(n) = O(n) = O(n^1)

Compare: c = 1, log_b(a) = 1
â†’ c = log_b(a) â†’ Case 2

T(n) = Î˜(n^1 Â· log n) = Î˜(n log n)
```

**Result: O(n log n)**

---

### Example 3: Binary Tree Traversal

```python
def traverse(node):
    # T(n) = 2T(n/2) + O(1)
```

**Analysis:**
```
a = 2 (visit both children)
b = 2 (each subtree has n/2 nodes)
f(n) = O(1)

log_b(a) = logâ‚‚(2) = 1
f(n) = O(1) = O(n^0)

Compare: c = 0, log_b(a) = 1
â†’ c < log_b(a) â†’ Case 1

T(n) = Î˜(n^(logâ‚‚(2))) = Î˜(n^1) = Î˜(n)
```

**Result: O(n)**

---

### Example 4: Karatsuba Multiplication

```python
def karatsuba(x, y):
    # T(n) = 3T(n/2) + O(n)
```

**Analysis:**
```
a = 3 (three recursive calls)
b = 2 (divide by 2)
f(n) = O(n)

log_b(a) = logâ‚‚(3) â‰ˆ 1.585
f(n) = O(n) = O(n^1)

Compare: c = 1, log_b(a) â‰ˆ 1.585
â†’ c < log_b(a) â†’ Case 1

T(n) = Î˜(n^(logâ‚‚(3))) = Î˜(n^1.585)
```

**Result: O(n^1.585)** (better than O(nÂ²) naive multiplication)

---

### Example 5: Searching in 2D Sorted Matrix

```python
def search_matrix(matrix, target):
    # T(n) = 3T(n/4) + O(1)
```

**Analysis:**
```
a = 3 (search 3 quadrants)
b = 4 (each quadrant is n/4)
f(n) = O(1)

log_b(a) = logâ‚„(3) â‰ˆ 0.793
f(n) = O(1) = O(n^0)

Compare: c = 0, log_b(a) â‰ˆ 0.793
â†’ c < log_b(a) â†’ Case 1

T(n) = Î˜(n^(logâ‚„(3))) â‰ˆ Î˜(n^0.793)
```

**Result: O(n^0.793)** (sublinear!)

---

## 17.3 Recursion Tree Method

### Understanding Recursion Trees

A **recursion tree** visualizes recursive calls:
- Each node represents work at that call
- Children are recursive calls
- Height = recursion depth
- Total work = sum of all nodes

### Method Steps

1. **Draw the tree** showing recursive structure
2. **Calculate work per level**
3. **Determine tree height**
4. **Sum all levels**

---

### Example 1: T(n) = 2T(n/2) + n (Merge Sort)

**Tree Structure:**
```
Level 0:               n              Work: n
                    /     \
Level 1:         n/2      n/2         Work: n/2 + n/2 = n
                 / \       / \
Level 2:      n/4 n/4  n/4 n/4       Work: 4(n/4) = n
              ...
Level h:      n/2^h nodes            Work: 2^h(n/2^h) = n

Height h = logâ‚‚(n) (when n/2^h = 1)
```

**Total Work:**
```
Total = Work per level Ã— Number of levels
      = n Ã— logâ‚‚(n)
      = O(n log n)
```

---

### Example 2: T(n) = T(n-1) + n (Bubble Sort)

**Tree Structure:**
```
Level 0:    n                Work: n
            |
Level 1:    n-1              Work: n-1
            |
Level 2:    n-2              Work: n-2
            |
            ...
Level n-1:  1                Work: 1

Height = n
```

**Total Work:**
```
Total = n + (n-1) + (n-2) + ... + 1
      = n(n+1)/2
      = O(nÂ²)
```

---

### Example 3: T(n) = 2T(n-1) + 1 (Binary Strings)

**Tree Structure:**
```
Level 0:           1              Nodes: 1, Work: 1
                 /   \
Level 1:        1     1            Nodes: 2, Work: 2
               / \   / \
Level 2:      1  1  1  1           Nodes: 4, Work: 4
              ...
Level n:      2^n nodes            Nodes: 2^n, Work: 2^n

Height = n
```

**Total Work:**
```
Total = 1 + 2 + 4 + ... + 2^n
      = 2^(n+1) - 1
      = O(2^n)
```

---

### Example 4: T(n) = 3T(n/2) + n

**Tree Structure:**
```
Level 0:              n                    Work: n
                   /  |  \
Level 1:        n/2  n/2  n/2             Work: 3(n/2) = 1.5n
              / | \ / | \ / | \
Level 2:    n/4 ... (9 nodes)             Work: 9(n/4) = 2.25n
            ...

At level i: 3^i nodes of size n/2^i
Work per level: 3^i Â· (n/2^i) = n(3/2)^i

Height = logâ‚‚(n)
```

**Total Work:**
```
Total = n[1 + (3/2) + (3/2)Â² + ... + (3/2)^(logâ‚‚n)]
      = n[(3/2)^(logâ‚‚n+1) - 1] / [(3/2) - 1]
      â‰ˆ n Â· n^(logâ‚‚3) / (1/2)
      = O(n^(logâ‚‚3))
      â‰ˆ O(n^1.585)
```

---

## 17.4 Space Complexity Analysis

### Understanding Space for Recursion

**Space = Auxiliary Space + Recursion Stack Space**

- **Auxiliary Space:** Extra space used (arrays, variables)
- **Stack Space:** Space for recursive calls

### Calculating Stack Space

**Rule:** Maximum recursion depth Ã— space per call

---

### Example 1: Linear Recursion

```python
def factorial(n):
    if n <= 1:
        return 1
    return n * factorial(n - 1)
```

**Space Analysis:**
```
Maximum depth: n
Space per call: O(1)

Total space: O(n) Ã— O(1) = O(n)
```

---

### Example 2: Binary Recursion

```python
def fibonacci(n):
    if n <= 1:
        return n
    return fibonacci(n - 1) + fibonacci(n - 2)
```

**Space Analysis:**
```
Tree height: n (longest path is n-1, n-2, n-3, ..., 1)
Space per call: O(1)

Total space: O(n)

Note: Even though we make 2^n calls total,
only n calls are on stack at once (the path from root to current leaf)
```

---

### Example 3: Divide and Conquer

```python
def merge_sort(arr):
    if len(arr) <= 1:
        return arr
    
    mid = len(arr) // 2
    left = merge_sort(arr[:mid])
    right = merge_sort(arr[mid:])
    return merge(left, right)
```

**Space Analysis:**
```
Recursion depth: log(n)
Space per call: O(n) for creating subarrays

Total space: O(n) auxiliary + O(log n) stack
           = O(n)

Note: If we use indices instead of creating arrays,
      space becomes O(log n)
```

---

### Example 4: Tree Traversal

```python
def inorder(root):
    if root is None:
        return
    inorder(root.left)
    print(root.val)
    inorder(root.right)
```

**Space Analysis:**
```
Best case (balanced tree):
  Height: log(n)
  Space: O(log n)

Worst case (skewed tree):
  Height: n
  Space: O(n)

Average case:
  Space: O(h) where h is tree height
```

---

## 17.5 Complexity Quick Reference

### Common Recurrences

| Recurrence | Complexity | Example |
|------------|------------|---------|
| T(n) = T(n-1) + O(1) | O(n) | Factorial, Sum |
| T(n) = T(n-1) + O(n) | O(nÂ²) | Bubble sort |
| T(n) = 2T(n-1) + O(1) | O(2^n) | Fibonacci |
| T(n) = T(n/2) + O(1) | O(log n) | Binary search |
| T(n) = T(n/2) + O(n) | O(n) | Median finding |
| T(n) = 2T(n/2) + O(1) | O(n) | Tree traversal |
| T(n) = 2T(n/2) + O(n) | O(n log n) | Merge sort |
| T(n) = T(n-1) + T(n-2) + O(1) | O(2^n) | Fibonacci |

---

### Space Complexity Patterns

| Pattern | Space | Example |
|---------|-------|---------|
| Linear recursion | O(n) | Factorial |
| Binary recursion | O(h) | Tree DFS |
| Tail recursion | O(n)* | Sum (tail) |
| Divide & Conquer | O(log n) | Binary search |
| With memoization | O(states) | DP problems |

*O(1) if tail call optimization is supported

---

## Key Takeaways from Chapter 17

### 1. Setting Up Recurrence Relations

**Steps:**
1. Identify base case time
2. Count recursive calls
3. Determine size of subproblems
4. Calculate non-recursive work
5. Write recurrence equation

### 2. Choosing Analysis Method

```
Is it T(n) = aT(n/b) + f(n)?
â”œâ”€ YES â†’ Use Master Theorem
â””â”€ NO â†’ Use Recursion Tree or Expansion
```

### 3. Master Theorem Decision Tree

```
Compare f(n) with n^(log_b(a)):
â”œâ”€ f(n) smaller â†’ Case 1 â†’ T(n) = Î˜(n^(log_b(a)))
â”œâ”€ f(n) equal â†’ Case 2 â†’ T(n) = Î˜(n^c log n)
â””â”€ f(n) larger â†’ Case 3 â†’ T(n) = Î˜(f(n))
```

### 4. Space Analysis Checklist

âœ… Recursion depth (maximum)  
âœ… Space per recursive call  
âœ… Auxiliary space used  
âœ… Total = max(depth Ã— per_call, auxiliary)  

---

## Practice Problems

### Easy
1. Analyze time for linear search
2. Analyze space for sum function
3. Find complexity of power(x, n)
4. Analyze tree height calculation

### Medium
5. Prove merge sort is O(n log n)
6. Analyze quick sort (best/average/worst)
7. Find complexity of Strassen's algorithm
8. Analyze space for DFS vs BFS

### Hard
9. Analyze complex recursive functions
10. Prove master theorem for specific cases
11. Analyze space for memoized solutions
12. Compare recursive vs iterative space

### Challenge
13. Derive recurrence for custom algorithm
14. Prove tightness of bounds
15. Analyze amortized complexity
16. Study practical vs theoretical complexity

---

## Common Mistakes

âŒ **Forgetting base case cost**
```
T(n) = T(n-1) + 1
// Missing T(1) = 1
```

âŒ **Counting total calls as time**
```
Fibonacci makes 2^n calls
But time for each call is O(1)
So time = 2^n Ã— O(1) = O(2^n) âœ“
Not O(2^n Ã— n) âœ—
```

âŒ **Confusing depth with node count**
```
Space = depth of recursion (max stack size)
Not total number of recursive calls!
```

âŒ **Wrong Master Theorem application**
```
T(n) = T(n-1) + n
// This is NOT T(n) = aT(n/b) + f(n)
// Can't use Master Theorem!
```

Master complexity analysis and you'll understand exactly how your recursive algorithms perform! ðŸ“Š
