# Chapter 18: Monitoring and Observability

> **Goal**: Learn how to understand what's happening in your distributed system

---

## 18.1 Monitoring vs Observability

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           MONITORING vs OBSERVABILITY                        â”‚
â”‚                                                              â”‚
â”‚  MONITORING                                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                 â”‚
â”‚  "Is the system working?"                                   â”‚
â”‚                                                              â”‚
â”‚  â€¢ Predefined dashboards and alerts                         â”‚
â”‚  â€¢ Known failure modes                                      â”‚
â”‚  â€¢ Black-box approach                                       â”‚
â”‚  â€¢ "Is CPU > 80%?" â†’ Alert                                 â”‚
â”‚                                                              â”‚
â”‚                                                              â”‚
â”‚  OBSERVABILITY                                              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                              â”‚
â”‚  "WHY is the system not working?"                           â”‚
â”‚                                                              â”‚
â”‚  â€¢ Explore and investigate unknown issues                   â”‚
â”‚  â€¢ Ask arbitrary questions                                  â”‚
â”‚  â€¢ White-box approach                                       â”‚
â”‚  â€¢ "Why did request X take 5 seconds?"                     â”‚
â”‚                                                              â”‚
â”‚                                                              â”‚
â”‚  In distributed systems, you need BOTH:                     â”‚
â”‚  â€¢ Monitoring: Know when something is wrong                 â”‚
â”‚  â€¢ Observability: Understand why it's wrong                 â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 18.2 Three Pillars of Observability

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            THREE PILLARS OF OBSERVABILITY                    â”‚
â”‚                                                              â”‚
â”‚       METRICS              LOGS               TRACES         â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚     â”‚ ðŸ“Š      â”‚        â”‚ ðŸ“      â”‚        â”‚ ðŸ”—      â”‚      â”‚
â”‚     â”‚ Numbers â”‚        â”‚ Events  â”‚        â”‚ Request â”‚      â”‚
â”‚     â”‚ over    â”‚        â”‚ with    â”‚        â”‚ Journey â”‚      â”‚
â”‚     â”‚ time    â”‚        â”‚ context â”‚        â”‚         â”‚      â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                              â”‚
â”‚     "WHAT"             "WHY"              "WHERE"           â”‚
â”‚     happened           it happened       in the system      â”‚
â”‚                                                              â”‚
â”‚                                                              â”‚
â”‚  METRICS                                                    â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€                                                   â”‚
â”‚  Numeric measurements aggregated over time.                 â”‚
â”‚  â€¢ CPU usage: 75%                                           â”‚
â”‚  â€¢ Requests/second: 1,234                                   â”‚
â”‚  â€¢ Error rate: 0.5%                                         â”‚
â”‚  â€¢ P99 latency: 150ms                                       â”‚
â”‚                                                              â”‚
â”‚  Pros: Efficient, good for alerting, trending               â”‚
â”‚  Cons: Loses individual event detail                        â”‚
â”‚                                                              â”‚
â”‚                                                              â”‚
â”‚  LOGS                                                       â”‚
â”‚  â”€â”€â”€â”€â”€                                                      â”‚
â”‚  Discrete events with context.                              â”‚
â”‚  â€¢ [ERROR] User 123 failed login: wrong password            â”‚
â”‚  â€¢ [INFO] Order 456 shipped to address XYZ                  â”‚
â”‚                                                              â”‚
â”‚  Pros: Rich context, debugging details                      â”‚
â”‚  Cons: Expensive at scale, hard to correlate                â”‚
â”‚                                                              â”‚
â”‚                                                              â”‚
â”‚  TRACES                                                     â”‚
â”‚  â”€â”€â”€â”€â”€â”€                                                     â”‚
â”‚  Request flow across services.                              â”‚
â”‚  â€¢ API Gateway â†’ User Service â†’ Database                    â”‚
â”‚  â€¢ Shows timing at each step                                â”‚
â”‚                                                              â”‚
â”‚  Pros: End-to-end visibility, find bottlenecks              â”‚
â”‚  Cons: Sampling needed at scale, complex setup              â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 18.3 Metrics

### Metric Types

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    METRIC TYPES                              â”‚
â”‚                                                              â”‚
â”‚  COUNTER                                                    â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                  â”‚
â”‚  Monotonically increasing value. Never decreases.           â”‚
â”‚                                                              â”‚
â”‚  Value                                                      â”‚
â”‚    â”‚        â•±                                               â”‚
â”‚    â”‚      â•±                                                 â”‚
â”‚    â”‚    â•±                                                   â”‚
â”‚    â”‚  â•±                                                     â”‚
â”‚    â”‚â•±                                                       â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Time                                  â”‚
â”‚                                                              â”‚
â”‚  Examples:                                                  â”‚
â”‚  â€¢ Total requests                                           â”‚
â”‚  â€¢ Total errors                                             â”‚
â”‚  â€¢ Bytes transferred                                        â”‚
â”‚                                                              â”‚
â”‚                                                              â”‚
â”‚  GAUGE                                                      â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€                                                    â”‚
â”‚  Point-in-time value. Can go up or down.                    â”‚
â”‚                                                              â”‚
â”‚  Value                                                      â”‚
â”‚    â”‚    â•±â•²      â•±â•²                                          â”‚
â”‚    â”‚  â•±    â•²  â•±    â•²                                        â”‚
â”‚    â”‚â•±        â•²        â•²                                     â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Time                              â”‚
â”‚                                                              â”‚
â”‚  Examples:                                                  â”‚
â”‚  â€¢ Current CPU usage                                        â”‚
â”‚  â€¢ Active connections                                       â”‚
â”‚  â€¢ Queue depth                                              â”‚
â”‚                                                              â”‚
â”‚                                                              â”‚
â”‚  HISTOGRAM                                                  â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                â”‚
â”‚  Distribution of values in buckets.                         â”‚
â”‚                                                              â”‚
â”‚  Count                                                      â”‚
â”‚    â”‚  â–ˆâ–ˆâ–ˆâ–ˆ                                                  â”‚
â”‚    â”‚  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                              â”‚
â”‚    â”‚  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                          â”‚
â”‚    â”‚  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                      â”‚
â”‚    â”‚  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                          â”‚
â”‚    â”‚  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                              â”‚
â”‚    â”‚  â–ˆâ–ˆâ–ˆâ–ˆ                                                  â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Latency (ms)                      â”‚
â”‚      0-10 10-50 50-100 100-500 500+                        â”‚
â”‚                                                              â”‚
â”‚  Examples:                                                  â”‚
â”‚  â€¢ Request latency distribution                             â”‚
â”‚  â€¢ Response size distribution                               â”‚
â”‚                                                              â”‚
â”‚                                                              â”‚
â”‚  SUMMARY                                                    â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                  â”‚
â”‚  Pre-calculated quantiles (percentiles).                    â”‚
â”‚                                                              â”‚
â”‚  â€¢ P50 (median): 45ms                                       â”‚
â”‚  â€¢ P90: 120ms                                               â”‚
â”‚  â€¢ P99: 450ms                                               â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Python Metrics with Prometheus

```python
# metrics.py
"""
Metrics implementation using Prometheus client.
"""

from prometheus_client import (
    Counter, Gauge, Histogram, Summary,
    generate_latest, CONTENT_TYPE_LATEST,
    CollectorRegistry, multiprocess, REGISTRY
)
from fastapi import FastAPI, Request, Response
from starlette.middleware.base import BaseHTTPMiddleware
import time
import psutil
import asyncio

# ============== METRIC DEFINITIONS ==============

# Counter: Total requests
REQUEST_COUNT = Counter(
    'http_requests_total',
    'Total HTTP requests',
    ['method', 'endpoint', 'status']
)

# Counter: Total errors
ERROR_COUNT = Counter(
    'http_errors_total',
    'Total HTTP errors',
    ['method', 'endpoint', 'error_type']
)

# Histogram: Request latency
REQUEST_LATENCY = Histogram(
    'http_request_duration_seconds',
    'HTTP request latency in seconds',
    ['method', 'endpoint'],
    buckets=[0.01, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0]
)

# Gauge: Active connections
ACTIVE_CONNECTIONS = Gauge(
    'http_active_connections',
    'Number of active HTTP connections'
)

# Gauge: System metrics
CPU_USAGE = Gauge('system_cpu_usage_percent', 'CPU usage percentage')
MEMORY_USAGE = Gauge('system_memory_usage_percent', 'Memory usage percentage')

# Summary: Request size
REQUEST_SIZE = Summary(
    'http_request_size_bytes',
    'HTTP request size in bytes',
    ['method', 'endpoint']
)

# Custom business metrics
ORDERS_CREATED = Counter(
    'business_orders_created_total',
    'Total orders created',
    ['customer_type']
)

ORDER_VALUE = Histogram(
    'business_order_value_dollars',
    'Order value in dollars',
    buckets=[10, 50, 100, 500, 1000, 5000]
)

INVENTORY_LEVEL = Gauge(
    'business_inventory_level',
    'Current inventory level',
    ['product_id']
)

# ============== MIDDLEWARE ==============

class MetricsMiddleware(BaseHTTPMiddleware):
    """Middleware to collect HTTP metrics"""
    
    async def dispatch(self, request: Request, call_next):
        # Skip metrics endpoint
        if request.url.path == "/metrics":
            return await call_next(request)
        
        # Track active connections
        ACTIVE_CONNECTIONS.inc()
        
        # Start timer
        start_time = time.perf_counter()
        
        # Get request size
        body = await request.body()
        request_size = len(body)
        
        try:
            response = await call_next(request)
            status = response.status_code
            
        except Exception as e:
            status = 500
            ERROR_COUNT.labels(
                method=request.method,
                endpoint=request.url.path,
                error_type=type(e).__name__
            ).inc()
            raise
        
        finally:
            # Record metrics
            duration = time.perf_counter() - start_time
            
            REQUEST_COUNT.labels(
                method=request.method,
                endpoint=request.url.path,
                status=status
            ).inc()
            
            REQUEST_LATENCY.labels(
                method=request.method,
                endpoint=request.url.path
            ).observe(duration)
            
            REQUEST_SIZE.labels(
                method=request.method,
                endpoint=request.url.path
            ).observe(request_size)
            
            ACTIVE_CONNECTIONS.dec()
        
        return response

# ============== SYSTEM METRICS COLLECTOR ==============

async def collect_system_metrics():
    """Periodically collect system metrics"""
    while True:
        CPU_USAGE.set(psutil.cpu_percent())
        MEMORY_USAGE.set(psutil.virtual_memory().percent)
        await asyncio.sleep(15)

# ============== FASTAPI APP ==============

app = FastAPI()
app.add_middleware(MetricsMiddleware)

@app.on_event("startup")
async def startup():
    # Start system metrics collection
    asyncio.create_task(collect_system_metrics())

@app.get("/metrics")
async def metrics():
    """Prometheus metrics endpoint"""
    return Response(
        content=generate_latest(REGISTRY),
        media_type=CONTENT_TYPE_LATEST
    )

@app.post("/orders")
async def create_order(request: Request):
    """Example endpoint with business metrics"""
    data = await request.json()
    
    # Record business metrics
    ORDERS_CREATED.labels(
        customer_type=data.get('customer_type', 'standard')
    ).inc()
    
    ORDER_VALUE.observe(data.get('total', 0))
    
    return {"order_id": "123", "status": "created"}

# ============== RED METRICS PATTERN ==============

class REDMetrics:
    """
    RED Metrics: Rate, Errors, Duration
    The golden signals for microservices.
    """
    
    def __init__(self, service_name: str):
        self.service = service_name
        
        # Rate: Request throughput
        self.rate = Counter(
            f'{service_name}_requests_total',
            f'Total requests to {service_name}',
            ['method', 'endpoint']
        )
        
        # Errors: Error count
        self.errors = Counter(
            f'{service_name}_errors_total',
            f'Total errors in {service_name}',
            ['method', 'endpoint', 'error_code']
        )
        
        # Duration: Request latency
        self.duration = Histogram(
            f'{service_name}_request_duration_seconds',
            f'Request duration for {service_name}',
            ['method', 'endpoint']
        )
    
    def record_request(
        self,
        method: str,
        endpoint: str,
        duration: float,
        error_code: str = None
    ):
        """Record a request with RED metrics"""
        self.rate.labels(method=method, endpoint=endpoint).inc()
        self.duration.labels(method=method, endpoint=endpoint).observe(duration)
        
        if error_code:
            self.errors.labels(
                method=method,
                endpoint=endpoint,
                error_code=error_code
            ).inc()

# ============== USE METRICS PATTERN ==============

class USEMetrics:
    """
    USE Metrics: Utilization, Saturation, Errors
    The golden signals for resources.
    """
    
    def __init__(self, resource_name: str):
        self.resource = resource_name
        
        # Utilization: How busy is the resource?
        self.utilization = Gauge(
            f'{resource_name}_utilization_percent',
            f'Utilization of {resource_name}'
        )
        
        # Saturation: How much extra work is queued?
        self.saturation = Gauge(
            f'{resource_name}_saturation',
            f'Saturation of {resource_name} (queued work)'
        )
        
        # Errors: Error count
        self.errors = Counter(
            f'{resource_name}_errors_total',
            f'Errors from {resource_name}'
        )
    
    def record(
        self,
        utilization: float,
        saturation: float,
        error: bool = False
    ):
        """Record USE metrics"""
        self.utilization.set(utilization)
        self.saturation.set(saturation)
        if error:
            self.errors.inc()

# Example: Database pool metrics
db_pool_metrics = USEMetrics('database_pool')

async def record_db_pool_metrics(pool):
    """Record database connection pool metrics"""
    while True:
        # Utilization: active connections / max connections
        utilization = (pool.get_size() / pool.get_max_size()) * 100
        
        # Saturation: waiting requests
        saturation = pool.get_idle_size()  # Or pending requests
        
        db_pool_metrics.record(utilization, saturation)
        await asyncio.sleep(10)
```

---

## 18.4 Distributed Tracing

### Tracing Concepts

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              DISTRIBUTED TRACING CONCEPTS                    â”‚
â”‚                                                              â”‚
â”‚  TRACE                                                      â”‚
â”‚  â”€â”€â”€â”€â”€                                                      â”‚
â”‚  The entire journey of a request through the system.        â”‚
â”‚                                                              â”‚
â”‚  SPAN                                                       â”‚
â”‚  â”€â”€â”€â”€                                                       â”‚
â”‚  A single operation within a trace.                         â”‚
â”‚  Has: start time, duration, name, tags, logs                â”‚
â”‚                                                              â”‚
â”‚  TRACE CONTEXT                                              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                              â”‚
â”‚  Propagated headers that link spans together.               â”‚
â”‚  â€¢ Trace ID: Unique ID for the entire request              â”‚
â”‚  â€¢ Span ID: Unique ID for this operation                   â”‚
â”‚  â€¢ Parent Span ID: ID of the calling span                  â”‚
â”‚                                                              â”‚
â”‚                                                              â”‚
â”‚  Example Trace:                                             â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                             â”‚
â”‚                                                              â”‚
â”‚  [Trace ID: abc123]                                         â”‚
â”‚                                                              â”‚
â”‚  â”œâ”€ Span: HTTP GET /order/123 (50ms)                       â”‚
â”‚  â”‚  â”‚                                                       â”‚
â”‚  â”‚  â”œâ”€ Span: Auth Check (5ms)                              â”‚
â”‚  â”‚  â”‚                                                       â”‚
â”‚  â”‚  â”œâ”€ Span: Get Order from DB (15ms)                      â”‚
â”‚  â”‚  â”‚  â”‚                                                    â”‚
â”‚  â”‚  â”‚  â””â”€ Span: PostgreSQL Query (12ms)                    â”‚
â”‚  â”‚  â”‚                                                       â”‚
â”‚  â”‚  â”œâ”€ Span: Get User Details (20ms)                       â”‚
â”‚  â”‚  â”‚  â”‚                                                    â”‚
â”‚  â”‚  â”‚  â””â”€ Span: HTTP GET /user/456 (18ms)                  â”‚
â”‚  â”‚  â”‚     â”‚                                                 â”‚
â”‚  â”‚  â”‚     â””â”€ Span: Redis Cache Hit (2ms)                   â”‚
â”‚  â”‚  â”‚                                                       â”‚
â”‚  â”‚  â””â”€ Span: Build Response (5ms)                          â”‚
â”‚                                                              â”‚
â”‚  Timeline View:                                             â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                             â”‚
â”‚  |â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€| HTTP GET   â”‚
â”‚  |â”€â”€| Auth                                                  â”‚
â”‚     |â”€â”€â”€â”€â”€â”€â”€â”€| DB Query                                     â”‚
â”‚              |â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€| Get User                      â”‚
â”‚                             |â”€â”€| Cache                       â”‚
â”‚                                   |â”€â”€â”€| Build Response      â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Python Tracing with OpenTelemetry

```python
# tracing.py
"""
Distributed tracing with OpenTelemetry.
"""

from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import (
    BatchSpanProcessor,
    ConsoleSpanExporter
)
from opentelemetry.sdk.resources import Resource
from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor
from opentelemetry.instrumentation.httpx import HTTPXClientInstrumentor
from opentelemetry.instrumentation.asyncpg import AsyncPGInstrumentor
from opentelemetry.propagate import inject, extract
from opentelemetry.trace import Status, StatusCode
from opentelemetry.trace.propagation.tracecontext import TraceContextTextMapPropagator

from fastapi import FastAPI, Request
from contextlib import asynccontextmanager
import httpx
import asyncio
from typing import Dict, Any

# ============== SETUP ==============

def setup_tracing(service_name: str):
    """Initialize OpenTelemetry tracing"""
    
    # Create resource with service info
    resource = Resource.create({
        "service.name": service_name,
        "service.version": "1.0.0",
        "deployment.environment": "production"
    })
    
    # Create tracer provider
    provider = TracerProvider(resource=resource)
    
    # Add exporters
    # Console exporter for debugging
    provider.add_span_processor(
        BatchSpanProcessor(ConsoleSpanExporter())
    )
    
    # In production, add Jaeger/Zipkin exporter:
    # from opentelemetry.exporter.jaeger.thrift import JaegerExporter
    # provider.add_span_processor(
    #     BatchSpanProcessor(JaegerExporter(
    #         agent_host_name="jaeger",
    #         agent_port=6831
    #     ))
    # )
    
    # Set as global tracer provider
    trace.set_tracer_provider(provider)
    
    return trace.get_tracer(service_name)

# Initialize tracer
tracer = setup_tracing("order-service")

# ============== MANUAL TRACING ==============

class TracedService:
    """Example of manual tracing in a service"""
    
    def __init__(self):
        self.tracer = trace.get_tracer(__name__)
    
    async def process_order(self, order_id: str) -> Dict[str, Any]:
        """Process an order with tracing"""
        
        # Create a span for this operation
        with self.tracer.start_as_current_span("process_order") as span:
            # Add attributes (tags)
            span.set_attribute("order.id", order_id)
            span.set_attribute("order.type", "standard")
            
            try:
                # Child span: Validate order
                with self.tracer.start_as_current_span("validate_order"):
                    await self._validate(order_id)
                
                # Child span: Check inventory
                with self.tracer.start_as_current_span("check_inventory") as inv_span:
                    available = await self._check_inventory(order_id)
                    inv_span.set_attribute("inventory.available", available)
                
                # Child span: Process payment
                with self.tracer.start_as_current_span("process_payment"):
                    payment_id = await self._process_payment(order_id)
                    span.set_attribute("payment.id", payment_id)
                
                # Add event (log within span)
                span.add_event("Order processed successfully", {
                    "order_id": order_id,
                    "payment_id": payment_id
                })
                
                return {"order_id": order_id, "status": "completed"}
                
            except Exception as e:
                # Record error
                span.set_status(Status(StatusCode.ERROR, str(e)))
                span.record_exception(e)
                raise
    
    async def _validate(self, order_id: str):
        await asyncio.sleep(0.01)  # Simulate work
    
    async def _check_inventory(self, order_id: str) -> bool:
        await asyncio.sleep(0.02)
        return True
    
    async def _process_payment(self, order_id: str) -> str:
        await asyncio.sleep(0.05)
        return f"pay_{order_id}"

# ============== CONTEXT PROPAGATION ==============

class TracedHTTPClient:
    """HTTP client with trace context propagation"""
    
    def __init__(self):
        self.client = httpx.AsyncClient()
        self.propagator = TraceContextTextMapPropagator()
    
    async def get(self, url: str, **kwargs) -> httpx.Response:
        """Make GET request with trace context"""
        headers = kwargs.pop('headers', {})
        
        # Inject trace context into headers
        inject(headers)
        
        with tracer.start_as_current_span(
            f"HTTP GET {url}",
            kind=trace.SpanKind.CLIENT
        ) as span:
            span.set_attribute("http.method", "GET")
            span.set_attribute("http.url", url)
            
            response = await self.client.get(url, headers=headers, **kwargs)
            
            span.set_attribute("http.status_code", response.status_code)
            
            if response.status_code >= 400:
                span.set_status(Status(StatusCode.ERROR))
            
            return response
    
    async def post(self, url: str, **kwargs) -> httpx.Response:
        """Make POST request with trace context"""
        headers = kwargs.pop('headers', {})
        inject(headers)
        
        with tracer.start_as_current_span(
            f"HTTP POST {url}",
            kind=trace.SpanKind.CLIENT
        ) as span:
            span.set_attribute("http.method", "POST")
            span.set_attribute("http.url", url)
            
            response = await self.client.post(url, headers=headers, **kwargs)
            span.set_attribute("http.status_code", response.status_code)
            
            return response

# ============== FASTAPI WITH TRACING ==============

app = FastAPI()

# Auto-instrument FastAPI
FastAPIInstrumentor.instrument_app(app)

# Auto-instrument HTTPX
HTTPXClientInstrumentor().instrument()

@app.middleware("http")
async def add_trace_id_header(request: Request, call_next):
    """Add trace ID to response headers for debugging"""
    response = await call_next(request)
    
    span = trace.get_current_span()
    if span:
        trace_id = format(span.get_span_context().trace_id, '032x')
        response.headers["X-Trace-ID"] = trace_id
    
    return response

service = TracedService()
http_client = TracedHTTPClient()

@app.get("/orders/{order_id}")
async def get_order(order_id: str):
    """Endpoint with automatic tracing"""
    
    # Manual span for business logic
    with tracer.start_as_current_span("get_order_logic") as span:
        span.set_attribute("order.id", order_id)
        
        # Call other service (trace context propagated)
        user_response = await http_client.get(
            f"http://user-service/users/123"
        )
        
        return {
            "order_id": order_id,
            "user": user_response.json() if user_response.status_code == 200 else None
        }

@app.post("/orders")
async def create_order(request: Request):
    """Create order with full tracing"""
    data = await request.json()
    
    result = await service.process_order(data.get("order_id", "new"))
    return result

# ============== TRACE CONTEXT DECORATOR ==============

from functools import wraps

def traced(name: str = None, attributes: Dict[str, Any] = None):
    """Decorator for automatic tracing"""
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            span_name = name or func.__name__
            
            with tracer.start_as_current_span(span_name) as span:
                if attributes:
                    for key, value in attributes.items():
                        span.set_attribute(key, value)
                
                try:
                    result = await func(*args, **kwargs)
                    return result
                except Exception as e:
                    span.set_status(Status(StatusCode.ERROR))
                    span.record_exception(e)
                    raise
        
        return wrapper
    return decorator

# Usage
@traced("calculate_total", {"calculation.type": "order_total"})
async def calculate_order_total(items: list) -> float:
    await asyncio.sleep(0.01)
    return sum(item.get("price", 0) * item.get("quantity", 0) for item in items)
```

---

## 18.5 Structured Logging

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              STRUCTURED LOGGING                              â”‚
â”‚                                                              â”‚
â”‚  Traditional Log:                                           â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                          â”‚
â”‚  2024-01-15 10:30:00 ERROR Failed to process order 123     â”‚
â”‚  for user 456, error: connection timeout                   â”‚
â”‚                                                              â”‚
â”‚  Problems:                                                  â”‚
â”‚  â€¢ Hard to parse                                            â”‚
â”‚  â€¢ Hard to search                                           â”‚
â”‚  â€¢ Hard to aggregate                                        â”‚
â”‚                                                              â”‚
â”‚                                                              â”‚
â”‚  Structured Log (JSON):                                     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                     â”‚
â”‚  {                                                          â”‚
â”‚    "timestamp": "2024-01-15T10:30:00.123Z",                â”‚
â”‚    "level": "error",                                        â”‚
â”‚    "message": "Failed to process order",                    â”‚
â”‚    "service": "order-service",                              â”‚
â”‚    "trace_id": "abc123",                                    â”‚
â”‚    "order_id": "123",                                       â”‚
â”‚    "user_id": "456",                                        â”‚
â”‚    "error_type": "ConnectionTimeout",                       â”‚
â”‚    "error_message": "connection timeout",                   â”‚
â”‚    "duration_ms": 5000                                      â”‚
â”‚  }                                                          â”‚
â”‚                                                              â”‚
â”‚  Benefits:                                                  â”‚
â”‚  â€¢ Machine parseable                                        â”‚
â”‚  â€¢ Searchable (find all errors for user 456)               â”‚
â”‚  â€¢ Can correlate with traces                                â”‚
â”‚  â€¢ Easy to aggregate                                        â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

```python
# structured_logging.py
"""
Structured logging with context propagation.
"""

import structlog
import logging
import json
import sys
from datetime import datetime
from typing import Any, Dict
from contextvars import ContextVar
from fastapi import FastAPI, Request
from starlette.middleware.base import BaseHTTPMiddleware
import uuid

# Context variables for request-scoped data
request_id_var: ContextVar[str] = ContextVar('request_id', default='')
user_id_var: ContextVar[str] = ContextVar('user_id', default='')
trace_id_var: ContextVar[str] = ContextVar('trace_id', default='')

# ============== STRUCTLOG CONFIGURATION ==============

def add_timestamp(logger, method_name, event_dict):
    """Add ISO timestamp to log"""
    event_dict['timestamp'] = datetime.utcnow().isoformat() + 'Z'
    return event_dict

def add_context(logger, method_name, event_dict):
    """Add context variables to log"""
    event_dict['request_id'] = request_id_var.get()
    event_dict['user_id'] = user_id_var.get()
    event_dict['trace_id'] = trace_id_var.get()
    return event_dict

def add_service_info(logger, method_name, event_dict):
    """Add service metadata"""
    event_dict['service'] = 'order-service'
    event_dict['version'] = '1.0.0'
    event_dict['environment'] = 'production'
    return event_dict

def configure_logging():
    """Configure structlog for JSON output"""
    
    structlog.configure(
        processors=[
            # Add context
            structlog.contextvars.merge_contextvars,
            # Add log level
            structlog.stdlib.add_log_level,
            # Add timestamp
            add_timestamp,
            # Add request context
            add_context,
            # Add service info
            add_service_info,
            # Format as JSON
            structlog.processors.JSONRenderer()
        ],
        context_class=dict,
        logger_factory=structlog.PrintLoggerFactory(),
        wrapper_class=structlog.stdlib.BoundLogger,
        cache_logger_on_first_use=True,
    )

# Initialize
configure_logging()
logger = structlog.get_logger()

# ============== LOG LEVELS WITH CONTEXT ==============

class ContextLogger:
    """Logger that automatically includes context"""
    
    def __init__(self, name: str):
        self.logger = structlog.get_logger(name)
    
    def _add_context(self, **kwargs) -> Dict[str, Any]:
        """Add default context to all logs"""
        return {
            'request_id': request_id_var.get(),
            'user_id': user_id_var.get(),
            'trace_id': trace_id_var.get(),
            **kwargs
        }
    
    def debug(self, message: str, **kwargs):
        self.logger.debug(message, **self._add_context(**kwargs))
    
    def info(self, message: str, **kwargs):
        self.logger.info(message, **self._add_context(**kwargs))
    
    def warning(self, message: str, **kwargs):
        self.logger.warning(message, **self._add_context(**kwargs))
    
    def error(self, message: str, error: Exception = None, **kwargs):
        if error:
            kwargs['error_type'] = type(error).__name__
            kwargs['error_message'] = str(error)
        self.logger.error(message, **self._add_context(**kwargs))
    
    def critical(self, message: str, **kwargs):
        self.logger.critical(message, **self._add_context(**kwargs))

# ============== FASTAPI INTEGRATION ==============

app = FastAPI()

class LoggingMiddleware(BaseHTTPMiddleware):
    """Middleware for request logging and context setup"""
    
    async def dispatch(self, request: Request, call_next):
        # Generate request ID
        request_id = request.headers.get('X-Request-ID', str(uuid.uuid4()))
        trace_id = request.headers.get('X-Trace-ID', str(uuid.uuid4()))
        
        # Set context variables
        request_id_token = request_id_var.set(request_id)
        trace_id_token = trace_id_var.set(trace_id)
        
        # Extract user ID from auth header (simplified)
        user_id = request.headers.get('X-User-ID', 'anonymous')
        user_id_token = user_id_var.set(user_id)
        
        # Log request start
        log = ContextLogger('http')
        log.info(
            "Request started",
            method=request.method,
            path=request.url.path,
            query=str(request.query_params)
        )
        
        start_time = datetime.utcnow()
        
        try:
            response = await call_next(request)
            
            # Log request complete
            duration_ms = (datetime.utcnow() - start_time).total_seconds() * 1000
            log.info(
                "Request completed",
                method=request.method,
                path=request.url.path,
                status_code=response.status_code,
                duration_ms=round(duration_ms, 2)
            )
            
            # Add request ID to response
            response.headers['X-Request-ID'] = request_id
            
            return response
            
        except Exception as e:
            duration_ms = (datetime.utcnow() - start_time).total_seconds() * 1000
            log.error(
                "Request failed",
                error=e,
                method=request.method,
                path=request.url.path,
                duration_ms=round(duration_ms, 2)
            )
            raise
        
        finally:
            # Reset context
            request_id_var.reset(request_id_token)
            trace_id_var.reset(trace_id_token)
            user_id_var.reset(user_id_token)

app.add_middleware(LoggingMiddleware)

# ============== USAGE IN APPLICATION ==============

log = ContextLogger('order_service')

@app.post("/orders")
async def create_order(request: Request):
    data = await request.json()
    
    log.info("Creating order", 
             customer_id=data.get('customer_id'),
             items_count=len(data.get('items', [])))
    
    try:
        # Process order...
        order_id = "ord_123"
        
        log.info("Order created successfully",
                 order_id=order_id,
                 total=data.get('total', 0))
        
        return {"order_id": order_id}
    
    except Exception as e:
        log.error("Failed to create order",
                  error=e,
                  customer_id=data.get('customer_id'))
        raise

# ============== LOG AGGREGATION PATTERNS ==============

class LogAggregator:
    """
    Pattern: Aggregate logs before sending to reduce volume.
    Useful for high-throughput services.
    """
    
    def __init__(self, flush_interval: float = 5.0, max_buffer: int = 100):
        self.buffer: list = []
        self.flush_interval = flush_interval
        self.max_buffer = max_buffer
    
    def log(self, level: str, message: str, **kwargs):
        """Buffer a log entry"""
        entry = {
            'level': level,
            'message': message,
            'timestamp': datetime.utcnow().isoformat(),
            **kwargs
        }
        
        self.buffer.append(entry)
        
        if len(self.buffer) >= self.max_buffer:
            self._flush()
    
    def _flush(self):
        """Send buffered logs"""
        if self.buffer:
            # In production: send to Elasticsearch, Loki, etc.
            for entry in self.buffer:
                print(json.dumps(entry))
            self.buffer.clear()

# Example output:
# {"timestamp": "2024-01-15T10:30:00.123Z", "level": "info", "message": "Request started", 
#  "service": "order-service", "request_id": "abc-123", "method": "POST", "path": "/orders"}
```

---

## 18.6 Alerting Best Practices

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ALERTING BEST PRACTICES                         â”‚
â”‚                                                              â”‚
â”‚  ALERT ON SYMPTOMS, NOT CAUSES                              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                             â”‚
â”‚                                                              â”‚
â”‚  BAD: Alert when CPU > 80%                                  â”‚
â”‚       (CPU might be high but users are fine)                â”‚
â”‚                                                              â”‚
â”‚  GOOD: Alert when error rate > 1%                           â”‚
â”‚        Alert when P99 latency > 500ms                       â”‚
â”‚        (These affect users!)                                â”‚
â”‚                                                              â”‚
â”‚                                                              â”‚
â”‚  ALERT SEVERITY LEVELS                                      â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                      â”‚
â”‚                                                              â”‚
â”‚  CRITICAL (Page immediately):                               â”‚
â”‚  â€¢ Service is down                                          â”‚
â”‚  â€¢ Data loss occurring                                      â”‚
â”‚  â€¢ Security breach                                          â”‚
â”‚                                                              â”‚
â”‚  WARNING (Fix within hours):                                â”‚
â”‚  â€¢ Error rate elevated                                      â”‚
â”‚  â€¢ Disk space low                                           â”‚
â”‚  â€¢ Performance degraded                                     â”‚
â”‚                                                              â”‚
â”‚  INFO (Review next business day):                           â”‚
â”‚  â€¢ Unusual traffic pattern                                  â”‚
â”‚  â€¢ Non-critical job failed                                  â”‚
â”‚  â€¢ Certificate expiring in 30 days                          â”‚
â”‚                                                              â”‚
â”‚                                                              â”‚
â”‚  ALERT FATIGUE PREVENTION                                   â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                   â”‚
â”‚                                                              â”‚
â”‚  â€¢ Only alert on actionable issues                          â”‚
â”‚  â€¢ Group related alerts                                     â”‚
â”‚  â€¢ Use appropriate thresholds                               â”‚
â”‚  â€¢ Implement alert suppression during incidents             â”‚
â”‚  â€¢ Review and tune alerts regularly                         â”‚
â”‚  â€¢ Include runbook links in alerts                          â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

```python
# alerting.py
"""
Alerting patterns and implementations.
"""

from dataclasses import dataclass
from datetime import datetime, timedelta
from typing import Dict, List, Callable, Optional
from enum import Enum
import asyncio

class Severity(Enum):
    CRITICAL = "critical"
    WARNING = "warning"
    INFO = "info"

@dataclass
class Alert:
    name: str
    severity: Severity
    message: str
    labels: Dict[str, str]
    timestamp: datetime
    runbook_url: Optional[str] = None
    
    def __str__(self):
        return f"[{self.severity.value.upper()}] {self.name}: {self.message}"

@dataclass
class AlertRule:
    """Define when to alert"""
    name: str
    condition: Callable[[], bool]  # Returns True when alert should fire
    severity: Severity
    message_template: str
    for_duration: timedelta = timedelta(minutes=0)  # Must be true for this long
    labels: Dict[str, str] = None
    runbook_url: str = None
    
    _triggered_at: datetime = None
    _firing: bool = False

class AlertManager:
    """Manages alert rules and notifications"""
    
    def __init__(self):
        self.rules: List[AlertRule] = []
        self.active_alerts: Dict[str, Alert] = {}
        self.notification_channels: List[Callable] = []
        self.suppressed_alerts: set = set()
    
    def add_rule(self, rule: AlertRule):
        """Add an alert rule"""
        self.rules.append(rule)
    
    def add_notification_channel(self, channel: Callable):
        """Add notification channel (Slack, PagerDuty, etc.)"""
        self.notification_channels.append(channel)
    
    def suppress_alert(self, alert_name: str, duration: timedelta):
        """Suppress an alert temporarily"""
        self.suppressed_alerts.add(alert_name)
        asyncio.create_task(self._unsuppress_after(alert_name, duration))
    
    async def _unsuppress_after(self, alert_name: str, duration: timedelta):
        await asyncio.sleep(duration.total_seconds())
        self.suppressed_alerts.discard(alert_name)
    
    async def evaluate_rules(self):
        """Evaluate all alert rules"""
        for rule in self.rules:
            try:
                condition_met = rule.condition()
                
                if condition_met:
                    if not rule._firing:
                        if rule._triggered_at is None:
                            rule._triggered_at = datetime.utcnow()
                        
                        # Check if condition met for required duration
                        elapsed = datetime.utcnow() - rule._triggered_at
                        if elapsed >= rule.for_duration:
                            rule._firing = True
                            await self._fire_alert(rule)
                else:
                    # Reset trigger
                    rule._triggered_at = None
                    if rule._firing:
                        rule._firing = False
                        await self._resolve_alert(rule)
                        
            except Exception as e:
                print(f"Error evaluating rule {rule.name}: {e}")
    
    async def _fire_alert(self, rule: AlertRule):
        """Fire an alert"""
        if rule.name in self.suppressed_alerts:
            print(f"Alert {rule.name} suppressed")
            return
        
        alert = Alert(
            name=rule.name,
            severity=rule.severity,
            message=rule.message_template,
            labels=rule.labels or {},
            timestamp=datetime.utcnow(),
            runbook_url=rule.runbook_url
        )
        
        self.active_alerts[rule.name] = alert
        
        # Send notifications
        for channel in self.notification_channels:
            try:
                await channel(alert, "firing")
            except Exception as e:
                print(f"Notification error: {e}")
    
    async def _resolve_alert(self, rule: AlertRule):
        """Resolve an alert"""
        if rule.name in self.active_alerts:
            alert = self.active_alerts.pop(rule.name)
            
            for channel in self.notification_channels:
                try:
                    await channel(alert, "resolved")
                except Exception as e:
                    print(f"Notification error: {e}")

# ============== NOTIFICATION CHANNELS ==============

async def slack_notification(alert: Alert, status: str):
    """Send alert to Slack"""
    emoji = "ðŸš¨" if alert.severity == Severity.CRITICAL else "âš ï¸"
    color = "danger" if alert.severity == Severity.CRITICAL else "warning"
    
    message = {
        "text": f"{emoji} Alert {status.upper()}: {alert.name}",
        "attachments": [{
            "color": color,
            "fields": [
                {"title": "Severity", "value": alert.severity.value, "short": True},
                {"title": "Time", "value": alert.timestamp.isoformat(), "short": True},
                {"title": "Message", "value": alert.message},
            ]
        }]
    }
    
    if alert.runbook_url:
        message["attachments"][0]["fields"].append({
            "title": "Runbook",
            "value": f"<{alert.runbook_url}|View Runbook>"
        })
    
    # In production: send via Slack API
    print(f"Slack: {message}")

async def pagerduty_notification(alert: Alert, status: str):
    """Send alert to PagerDuty"""
    if alert.severity != Severity.CRITICAL:
        return  # Only page for critical
    
    event = {
        "routing_key": "YOUR_PAGERDUTY_KEY",
        "event_action": "trigger" if status == "firing" else "resolve",
        "dedup_key": alert.name,
        "payload": {
            "summary": f"{alert.name}: {alert.message}",
            "severity": "critical",
            "source": "monitoring",
        }
    }
    
    # In production: send via PagerDuty API
    print(f"PagerDuty: {event}")

# ============== EXAMPLE USAGE ==============

# Metrics store (in production, query Prometheus)
class MetricsStore:
    error_rate: float = 0.0
    p99_latency_ms: float = 50.0
    active_connections: int = 100
    disk_usage_percent: float = 70.0

metrics = MetricsStore()

# Create alert manager
alert_manager = AlertManager()

# Add notification channels
alert_manager.add_notification_channel(slack_notification)
alert_manager.add_notification_channel(pagerduty_notification)

# Define alert rules
alert_manager.add_rule(AlertRule(
    name="HighErrorRate",
    condition=lambda: metrics.error_rate > 0.01,  # > 1%
    severity=Severity.CRITICAL,
    message_template="Error rate is above 1%",
    for_duration=timedelta(minutes=2),
    labels={"team": "backend"},
    runbook_url="https://runbooks.example.com/high-error-rate"
))

alert_manager.add_rule(AlertRule(
    name="HighLatency",
    condition=lambda: metrics.p99_latency_ms > 500,
    severity=Severity.WARNING,
    message_template="P99 latency is above 500ms",
    for_duration=timedelta(minutes=5),
    labels={"team": "backend"}
))

alert_manager.add_rule(AlertRule(
    name="DiskSpaceLow",
    condition=lambda: metrics.disk_usage_percent > 85,
    severity=Severity.WARNING,
    message_template="Disk usage is above 85%",
    for_duration=timedelta(minutes=10),
    labels={"team": "infrastructure"}
))

async def run_alert_loop():
    """Continuously evaluate alert rules"""
    while True:
        await alert_manager.evaluate_rules()
        await asyncio.sleep(30)  # Check every 30 seconds

# Demo
async def demo_alerting():
    print("=== Alerting Demo ===\n")
    
    # Simulate high error rate
    print("Simulating high error rate...")
    metrics.error_rate = 0.05  # 5%
    
    # Evaluate (won't fire immediately due to for_duration)
    await alert_manager.evaluate_rules()
    print("First evaluation (within for_duration)")
    
    # Simulate time passing
    for rule in alert_manager.rules:
        if rule.name == "HighErrorRate":
            rule._triggered_at = datetime.utcnow() - timedelta(minutes=3)
    
    # Evaluate again (should fire now)
    await alert_manager.evaluate_rules()
    print("\nSecond evaluation (after for_duration)")
    
    # Resolve
    print("\nResolving error rate...")
    metrics.error_rate = 0.001
    await alert_manager.evaluate_rules()

# asyncio.run(demo_alerting())
```

---

## 18.7 Observability Stack

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             COMMON OBSERVABILITY STACKS                      â”‚
â”‚                                                              â”‚
â”‚  METRICS                                                    â”‚
â”‚  â”œâ”€â”€ Collection: Prometheus, StatsD, Telegraf               â”‚
â”‚  â”œâ”€â”€ Storage: Prometheus, InfluxDB, TimescaleDB             â”‚
â”‚  â””â”€â”€ Visualization: Grafana, Datadog                        â”‚
â”‚                                                              â”‚
â”‚  LOGS                                                       â”‚
â”‚  â”œâ”€â”€ Collection: Fluentd, Fluent Bit, Logstash              â”‚
â”‚  â”œâ”€â”€ Storage: Elasticsearch, Loki, Splunk                   â”‚
â”‚  â””â”€â”€ Visualization: Kibana, Grafana                         â”‚
â”‚                                                              â”‚
â”‚  TRACES                                                     â”‚
â”‚  â”œâ”€â”€ Collection: OpenTelemetry, Jaeger Agent                â”‚
â”‚  â”œâ”€â”€ Storage: Jaeger, Zipkin, Tempo                         â”‚
â”‚  â””â”€â”€ Visualization: Jaeger UI, Grafana                      â”‚
â”‚                                                              â”‚
â”‚                                                              â”‚
â”‚  Popular Combinations:                                      â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                      â”‚
â”‚                                                              â”‚
â”‚  PLG Stack (Open Source):                                   â”‚
â”‚  â€¢ Prometheus (metrics)                                     â”‚
â”‚  â€¢ Loki (logs)                                              â”‚
â”‚  â€¢ Grafana (visualization)                                  â”‚
â”‚  â€¢ Tempo (traces)                                           â”‚
â”‚                                                              â”‚
â”‚  ELK Stack:                                                 â”‚
â”‚  â€¢ Elasticsearch (logs, metrics)                            â”‚
â”‚  â€¢ Logstash (log processing)                                â”‚
â”‚  â€¢ Kibana (visualization)                                   â”‚
â”‚                                                              â”‚
â”‚  Commercial:                                                â”‚
â”‚  â€¢ Datadog (all-in-one)                                     â”‚
â”‚  â€¢ New Relic (all-in-one)                                   â”‚
â”‚  â€¢ Splunk (logs focus)                                      â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Summary

| Pillar | Purpose | Tools | Best For |
|--------|---------|-------|----------|
| **Metrics** | Aggregated measurements | Prometheus, Grafana | Alerting, trends |
| **Logs** | Event details | ELK, Loki | Debugging, audit |
| **Traces** | Request flow | Jaeger, Zipkin | Latency analysis |

### Key Takeaways

1. **Metrics for alerting** - fast, efficient, trends
2. **Logs for debugging** - rich context, searchable
3. **Traces for bottlenecks** - see the whole picture
4. **Correlate all three** - use trace IDs in logs
5. **Alert on symptoms** - not causes
6. **Avoid alert fatigue** - actionable alerts only

---

## Practice Exercises

1. Implement RED metrics for an API
2. Set up distributed tracing with OpenTelemetry
3. Create structured logging with context propagation
4. Define alerting rules for a microservice
5. Build a dashboard with metrics, logs, and traces

**Next Chapter**: Deployment strategies and operations!
