# Master Distributed Systems: Complete Guide
## From Frontend Developer to Distributed Systems Expert

---

# Part I: Foundation (Weeks 1-4)

## Chapter 1: Prerequisites for Frontend Developers

### 1.1 Backend Fundamentals with Python
- **Python Basics for Backend**
  - Virtual environments (venv, pipenv, poetry)
  - Package management (pip, requirements.txt)
  - Async programming (asyncio, async/await)
  - Type hints and Pydantic for validation

- **Web Frameworks**
  - FastAPI (recommended - async, modern, type-safe)
  - Flask (lightweight, good for learning)
  - Django (batteries-included, ORM)

> ğŸ’¡ **Important**: Start with FastAPI - it has excellent async support crucial for distributed systems

### 1.2 Networking Essentials
- **OSI Model** (Focus on Layers 4-7)
  - Transport Layer (TCP/UDP)
  - Application Layer (HTTP/HTTPS, WebSocket)
  
- **Key Concepts**
  - IP addressing and DNS resolution
  - Ports and sockets
  - TLS/SSL handshake
  - HTTP/1.1 vs HTTP/2 vs HTTP/3

- **Latency Components**
  - Propagation delay
  - Transmission delay
  - Processing delay
  - Queuing delay

> âš ï¸ **Interview Tip**: Know that light travels ~200km/ms in fiber. US coast-to-coast RTT â‰ˆ 40-60ms minimum

### 1.3 Operating Systems Concepts
- Processes vs Threads vs Coroutines
- Context switching overhead
- Memory management basics
- File descriptors and I/O
- Signals and IPC (Inter-Process Communication)

### 1.4 Database Fundamentals
- **Relational Databases (PostgreSQL)**
  - SQL basics, JOINs, indexes
  - ACID properties
  - Transactions and isolation levels
  
- **NoSQL Introduction**
  - Document stores (MongoDB)
  - Key-value stores (Redis)
  - When to use SQL vs NoSQL

```python
# Example: Basic async database connection with Python
import asyncpg

async def fetch_user(user_id: int):
    conn = await asyncpg.connect('postgresql://localhost/mydb')
    row = await conn.fetchrow('SELECT * FROM users WHERE id = $1', user_id)
    await conn.close()
    return row
```

---

## Chapter 2: What is a Distributed System?

### 2.1 Definition and Characteristics
- **Definition**: A system where components located on networked computers communicate and coordinate by passing messages

- **Key Characteristics**
  - Concurrency of components
  - Lack of global clock
  - Independent failure of components
  - Heterogeneity

### 2.2 Why Distributed Systems?
- **Scalability**: Handle more load
- **Reliability**: No single point of failure
- **Performance**: Geographic distribution
- **Cost**: Horizontal scaling with commodity hardware

### 2.3 Challenges (The Hard Parts)
- Network is unreliable
- Latency is non-zero
- Bandwidth is finite
- Network is insecure
- Topology changes
- There are multiple administrators
- Transport cost is non-zero
- Network is heterogeneous

> ğŸ”´ **Critical**: These are the "8 Fallacies of Distributed Computing" - memorize them!

### 2.4 Types of Distributed Systems
- **Distributed Computing Systems** (clusters, grids)
- **Distributed Information Systems** (databases, web)
- **Distributed Pervasive Systems** (IoT, mobile)

---

## Chapter 3: Core Concepts and Terminology

### 3.1 Nodes and Clusters
- **Node**: Single machine/process in the system
- **Cluster**: Group of nodes working together
- **Data Center**: Physical location housing clusters
- **Region**: Geographic area with one or more data centers
- **Availability Zone (AZ)**: Isolated location within a region

### 3.2 Client-Server vs Peer-to-Peer
| Aspect | Client-Server | Peer-to-Peer |
|--------|---------------|--------------|
| Architecture | Centralized | Decentralized |
| Scalability | Limited by server | Highly scalable |
| Management | Easier | Complex |
| Examples | Web apps, APIs | BitTorrent, Blockchain |

### 3.3 Synchronous vs Asynchronous
- **Synchronous**: Caller waits for response
- **Asynchronous**: Caller continues, handles response later

```python
# Synchronous
response = requests.get('http://api.example.com/data')
process(response)

# Asynchronous
async def fetch_data():
    async with aiohttp.ClientSession() as session:
        async with session.get('http://api.example.com/data') as response:
            data = await response.json()
            return data
```

### 3.4 Stateful vs Stateless Services
- **Stateless**: Each request is independent (easier to scale)
- **Stateful**: Server maintains client state (complex but powerful)

> ğŸ’¡ **Best Practice**: Prefer stateless services; externalize state to databases/caches

---

# Part II: Communication (Weeks 5-8)

## Chapter 4: Communication Patterns

### 4.1 Request-Response (Synchronous)
- HTTP/REST APIs
- gRPC (Protocol Buffers)
- GraphQL

**REST Best Practices**:
- Use proper HTTP methods (GET, POST, PUT, DELETE, PATCH)
- Meaningful status codes (200, 201, 400, 404, 500)
- Versioning (/api/v1/)
- HATEOAS for discoverability

```python
# FastAPI REST Example
from fastapi import FastAPI, HTTPException

app = FastAPI()

@app.get("/users/{user_id}")
async def get_user(user_id: int):
    user = await db.fetch_user(user_id)
    if not user:
        raise HTTPException(status_code=404, detail="User not found")
    return user
```

### 4.2 Message Queues (Asynchronous)
- **Concepts**
  - Producer â†’ Queue â†’ Consumer
  - Decoupling of services
  - Buffering and load leveling
  
- **Popular Systems**
  - RabbitMQ (traditional, feature-rich)
  - Apache Kafka (high-throughput, log-based)
  - Amazon SQS (managed, simple)
  - Redis Streams (lightweight)

- **Delivery Semantics**
  - At-most-once: Fire and forget
  - At-least-once: Retry until acknowledged
  - Exactly-once: Hard to achieve (idempotency required)

> âš ï¸ **Interview Favorite**: "How do you achieve exactly-once delivery?" 
> Answer: Idempotent consumers + deduplication + transactional outbox pattern

### 4.3 Publish-Subscribe Pattern
- Publishers send to topics
- Subscribers receive from topics
- Fan-out capability
- Use cases: Event notification, real-time updates

```python
# Kafka Producer Example
from kafka import KafkaProducer
import json

producer = KafkaProducer(
    bootstrap_servers=['localhost:9092'],
    value_serializer=lambda v: json.dumps(v).encode('utf-8')
)

producer.send('user-events', {'event': 'signup', 'user_id': 123})
producer.flush()
```

### 4.4 Remote Procedure Calls (RPC)
- **gRPC** (Google's RPC framework)
  - Uses Protocol Buffers (protobuf)
  - HTTP/2 based
  - Bi-directional streaming
  - Strong typing

```protobuf
// user.proto
service UserService {
  rpc GetUser (GetUserRequest) returns (User);
  rpc ListUsers (ListUsersRequest) returns (stream User);
}

message GetUserRequest {
  int32 user_id = 1;
}

message User {
  int32 id = 1;
  string name = 2;
  string email = 3;
}
```

### 4.5 Event-Driven Architecture
- Events as first-class citizens
- Event sourcing
- CQRS (Command Query Responsibility Segregation)
- Saga pattern for distributed transactions

---

## Chapter 5: Serialization and Protocols

### 5.1 Data Serialization Formats
| Format | Human Readable | Size | Speed | Schema |
|--------|---------------|------|-------|--------|
| JSON | Yes | Large | Slow | No |
| Protocol Buffers | No | Small | Fast | Yes |
| Avro | No | Small | Fast | Yes |
| MessagePack | No | Medium | Fast | No |

### 5.2 Schema Evolution
- **Backward Compatibility**: New code reads old data
- **Forward Compatibility**: Old code reads new data
- **Full Compatibility**: Both directions

> ğŸ”´ **Critical**: Always version your APIs and schemas

### 5.3 API Design Principles
- Idempotency (safe to retry)
- Pagination for large datasets
- Rate limiting
- Versioning strategies

---

## Chapter 6: Service Discovery and Load Balancing

### 6.1 Service Discovery
- **Problem**: How do services find each other?
- **Solutions**:
  - DNS-based (simple, caching issues)
  - Service Registry (Consul, etcd, ZooKeeper)
  - Platform-native (Kubernetes DNS)

```python
# Using Consul for service discovery
import consul

c = consul.Consul()
# Register service
c.agent.service.register('user-service', port=8080)

# Discover service
_, services = c.health.service('user-service', passing=True)
for service in services:
    address = service['Service']['Address']
    port = service['Service']['Port']
```

### 6.2 Load Balancing
- **Layer 4 (Transport)**: TCP/UDP level (HAProxy, AWS NLB)
- **Layer 7 (Application)**: HTTP level (Nginx, AWS ALB)

**Algorithms**:
- Round Robin
- Weighted Round Robin
- Least Connections
- IP Hash (sticky sessions)
- Random

### 6.3 Health Checks
- **Liveness**: Is the service running?
- **Readiness**: Is the service ready to accept traffic?
- **Implementation**: HTTP endpoints, TCP checks, gRPC health

```python
# FastAPI health endpoints
@app.get("/health/live")
async def liveness():
    return {"status": "alive"}

@app.get("/health/ready")
async def readiness():
    # Check dependencies
    db_ok = await check_database()
    cache_ok = await check_cache()
    if db_ok and cache_ok:
        return {"status": "ready"}
    raise HTTPException(status_code=503)
```

---

# Part III: Data Management (Weeks 9-14)

## Chapter 7: Distributed Data Storage

### 7.1 Replication
- **Purpose**: Redundancy, read scalability, low latency

- **Strategies**:
  - **Single-Leader (Master-Slave)**
    - One writer, multiple readers
    - Simple, but leader is bottleneck
  
  - **Multi-Leader**
    - Multiple writers
    - Conflict resolution needed
    
  - **Leaderless (Dynamo-style)**
    - Any node accepts writes
    - Quorum-based consistency

```
Quorum Formula:
W + R > N (for strong consistency)

W = Write quorum
R = Read quorum  
N = Total replicas

Example: N=3, W=2, R=2 â†’ guarantees overlap
```

> ğŸ’¡ **Interview Tip**: Be ready to discuss trade-offs of each replication strategy

### 7.2 Partitioning (Sharding)
- **Why**: Single machine can't hold all data
- **Strategies**:
  - **Hash Partitioning**: hash(key) % num_partitions
  - **Range Partitioning**: Alphabetical/numerical ranges
  - **Consistent Hashing**: Minimizes redistribution

**Consistent Hashing Deep Dive**:
```
Traditional: hash(key) % N
Problem: Adding server redistributes ~all keys

Consistent Hashing: 
- Hash ring (0 to 2^32)
- Hash both keys and servers
- Key goes to next server clockwise
- Adding server only moves keys in one segment
```

> ğŸ”´ **Critical**: Consistent hashing is asked in 90% of system design interviews

### 7.3 Hot Spots and Skew
- **Problem**: Uneven distribution (celebrity problem)
- **Solutions**:
  - Add random suffix to hot keys
  - Application-level splitting
  - Caching hot data

---

## Chapter 8: Consistency Models

### 8.1 The CAP Theorem
- **Consistency**: All nodes see same data
- **Availability**: Every request gets response
- **Partition Tolerance**: System works despite network partitions

> âš ï¸ **Reality**: You MUST tolerate partitions, so choose between C and A

**Real-world choices**:
- CP: MongoDB, HBase, Redis Cluster
- AP: Cassandra, DynamoDB, CouchDB

### 8.2 Consistency Levels Spectrum
```
Strong â†â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â†’ Weak

Linearizability
  â†“
Sequential Consistency
  â†“
Causal Consistency
  â†“
Read-your-writes
  â†“
Eventual Consistency
```

### 8.3 Eventual Consistency
- Updates propagate eventually
- No guaranteed ordering
- Conflicts possible

**Conflict Resolution**:
- Last-Write-Wins (LWW)
- Version vectors
- CRDTs (Conflict-free Replicated Data Types)
- Application-level resolution

### 8.4 Linearizability
- Strongest consistency model
- Operations appear instantaneous
- Expensive (coordination required)
- Use cases: Leader election, locks, counters

### 8.5 PACELC Theorem (Extended CAP)
```
If Partition:
  Choose Availability or Consistency
Else (normal operation):
  Choose Latency or Consistency
```

---

## Chapter 9: Distributed Transactions

### 9.1 ACID in Distributed Systems
- **Atomicity**: All or nothing
- **Consistency**: Valid state transitions
- **Isolation**: Concurrent transactions don't interfere
- **Durability**: Committed data survives crashes

### 9.2 Two-Phase Commit (2PC)
```
Phase 1 (Prepare):
  Coordinator â†’ Participants: "Can you commit?"
  Participants â†’ Coordinator: "Yes/No"

Phase 2 (Commit/Abort):
  Coordinator â†’ Participants: "Commit" or "Abort"
```

**Problems with 2PC**:
- Blocking: Coordinator failure blocks everyone
- Not partition tolerant
- Performance overhead

### 9.3 Three-Phase Commit (3PC)
- Adds "pre-commit" phase
- Non-blocking (but not partition tolerant)
- Rarely used in practice

### 9.4 Saga Pattern (Preferred in Microservices)
- Sequence of local transactions
- Each has compensating transaction
- Eventually consistent

```
Order Saga:
1. Create Order â†’ (Compensate: Cancel Order)
2. Reserve Inventory â†’ (Compensate: Release Inventory)
3. Process Payment â†’ (Compensate: Refund Payment)
4. Ship Order
```

**Saga Orchestration vs Choreography**:
| Aspect | Orchestration | Choreography |
|--------|--------------|--------------|
| Control | Central orchestrator | Event-driven |
| Coupling | Loose (through orchestrator) | Very loose |
| Complexity | Orchestrator is complex | Hard to track flow |
| Debugging | Easier | Harder |

> ğŸ’¡ **Best Practice**: Use Orchestration for complex flows, Choreography for simple ones

---

## Chapter 10: Caching Strategies

### 10.1 Cache Fundamentals
- **Why Cache**: Reduce latency, reduce load
- **Cache Hit/Miss Ratio**: Target >95% hit rate

### 10.2 Caching Patterns
- **Cache-Aside (Lazy Loading)**
```python
def get_user(user_id):
    # Try cache first
    user = cache.get(f"user:{user_id}")
    if user is None:
        user = db.query(f"SELECT * FROM users WHERE id = {user_id}")
        cache.set(f"user:{user_id}", user, ttl=3600)
    return user
```

- **Write-Through**
```python
def update_user(user_id, data):
    db.update(user_id, data)
    cache.set(f"user:{user_id}", data)
```

- **Write-Behind (Write-Back)**
  - Write to cache, async write to DB
  - Risk of data loss

- **Read-Through**
  - Cache handles DB reads transparently

### 10.3 Cache Invalidation
- **TTL (Time-To-Live)**: Simple but stale data possible
- **Event-based**: Invalidate on write
- **Version-based**: Include version in key

> ğŸ”´ **Famous Quote**: "There are only two hard things in CS: cache invalidation and naming things"

### 10.4 Distributed Caching
- **Redis Cluster**: Automatic partitioning
- **Memcached**: Simple, multi-threaded
- **Local + Distributed**: Two-tier caching

### 10.5 Cache Stampede Prevention
- **Problem**: Cache expires, all requests hit DB
- **Solutions**:
  - Locking (only one fetches)
  - Probabilistic early expiration
  - Background refresh

```python
# Locking approach
def get_with_lock(key):
    value = cache.get(key)
    if value is None:
        lock = acquire_lock(f"lock:{key}")
        if lock:
            value = fetch_from_db(key)
            cache.set(key, value)
            release_lock(lock)
        else:
            # Wait and retry
            time.sleep(0.1)
            return get_with_lock(key)
    return value
```

---

# Part IV: Consensus and Coordination (Weeks 15-18)

## Chapter 11: Time and Ordering

### 11.1 The Problem with Time
- No global clock in distributed systems
- Clock skew between machines
- NTP accuracy: ~100ms over internet, ~1ms on LAN

### 11.2 Logical Clocks
- **Lamport Timestamps**
  - Each process has counter
  - Increment on event
  - On receive: max(local, received) + 1
  - Establishes "happened-before" relationship

```python
class LamportClock:
    def __init__(self):
        self.time = 0
    
    def tick(self):
        self.time += 1
        return self.time
    
    def receive(self, msg_time):
        self.time = max(self.time, msg_time) + 1
        return self.time
```

- **Vector Clocks**
  - Track causality precisely
  - Each node maintains vector of all clocks
  - Can detect concurrent events

### 11.3 Hybrid Logical Clocks (HLC)
- Combines physical and logical time
- Used in CockroachDB, MongoDB

### 11.4 TrueTime (Google Spanner)
- Hardware-based (GPS + atomic clocks)
- Returns time interval, not point
- Enables external consistency

> ğŸ’¡ **Interview Insight**: Know trade-offs between clock types

---

## Chapter 12: Consensus Algorithms

### 12.1 The Consensus Problem
- **Goal**: Get all nodes to agree on a value
- **Requirements**:
  - Agreement: All decide same value
  - Validity: Decided value was proposed
  - Termination: All eventually decide

### 12.2 FLP Impossibility
> "In an asynchronous system with even one faulty process, no consensus algorithm can guarantee termination"

**Workaround**: Use timeouts (partially synchronous model)

### 12.3 Paxos
- **Roles**: Proposers, Acceptors, Learners
- **Phases**:
  1. Prepare: Proposer gets promises
  2. Accept: Proposer sends value
  3. Learn: Acceptors notify learners

> âš ï¸ **Note**: Paxos is notoriously difficult to understand and implement

### 12.4 Raft (Recommended to Study)
- Designed for understandability
- **Leader Election**:
  - Nodes start as followers
  - Timeout â†’ become candidate
  - Request votes, majority wins
  
- **Log Replication**:
  - Leader appends to log
  - Replicates to followers
  - Commits when majority confirms

```
Raft States:
Follower â†’ (timeout) â†’ Candidate â†’ (majority) â†’ Leader
                            â†“
                      (higher term seen)
                            â†“
                        Follower
```

**Key Concepts**:
- **Term**: Logical time period (like epoch)
- **Log**: Ordered sequence of commands
- **Commit**: Entry replicated to majority

> ğŸ”´ **Must Know**: Raft is the most common interview topic for consensus

### 12.5 Byzantine Fault Tolerance (BFT)
- Handles malicious nodes
- Requires 3f+1 nodes to tolerate f Byzantine failures
- PBFT (Practical BFT): Used in permissioned blockchains
- Expensive: O(nÂ²) message complexity

---

## Chapter 13: Distributed Coordination

### 13.1 Leader Election
- **Why**: Coordination, write serialization
- **Algorithms**:
  - Bully algorithm
  - Ring algorithm
  - Raft/Paxos-based

### 13.2 Distributed Locks
- **Requirements**:
  - Mutual exclusion
  - Deadlock-free
  - Fault-tolerant

- **Redis Lock (Redlock)**:
```python
import redis
import uuid
import time

def acquire_lock(conn, lock_name, timeout=10):
    identifier = str(uuid.uuid4())
    lock_key = f"lock:{lock_name}"
    
    if conn.set(lock_key, identifier, nx=True, ex=timeout):
        return identifier
    return None

def release_lock(conn, lock_name, identifier):
    lock_key = f"lock:{lock_name}"
    # Use Lua script for atomicity
    script = """
    if redis.call('get', KEYS[1]) == ARGV[1] then
        return redis.call('del', KEYS[1])
    else
        return 0
    end
    """
    conn.eval(script, 1, lock_key, identifier)
```

> âš ï¸ **Warning**: Distributed locks are hard! Consider: lock expiration, GC pauses, clock skew

### 13.3 Fencing Tokens
- Monotonically increasing token with each lock
- Resource rejects older tokens
- Prevents "zombie" lock holders

### 13.4 ZooKeeper
- Distributed coordination service
- Hierarchical namespace (like filesystem)
- Used for: Config management, leader election, distributed locks

**ZooKeeper Guarantees**:
- Sequential consistency
- Atomicity
- Single system image
- Reliability
- Timeliness

---

# Part V: System Design Patterns (Weeks 19-24)

## Chapter 14: Microservices Architecture

### 14.1 Monolith vs Microservices
| Aspect | Monolith | Microservices |
|--------|----------|---------------|
| Deployment | Single unit | Independent |
| Scaling | Entire app | Per service |
| Technology | Single stack | Polyglot |
| Complexity | In code | In operations |
| Team | Single team | Multiple teams |

### 14.2 Service Boundaries
- **Domain-Driven Design (DDD)**
  - Bounded contexts
  - Ubiquitous language
  - Aggregates

- **Principles**:
  - Single responsibility
  - Loose coupling
  - High cohesion
  - Own your data

### 14.3 API Gateway Pattern
- Single entry point
- Responsibilities:
  - Request routing
  - Authentication
  - Rate limiting
  - Response aggregation
  - Protocol translation

```
Client â†’ API Gateway â†’ Service A
                    â†’ Service B
                    â†’ Service C
```

### 14.4 Service Mesh
- Infrastructure layer for service-to-service communication
- **Sidecar Proxy**: Envoy, Linkerd proxy
- **Control Plane**: Istio, Linkerd

**Features**:
- Traffic management
- Security (mTLS)
- Observability
- Retries and circuit breaking

### 14.5 Backend for Frontend (BFF)
- Separate backend per frontend type
- Mobile BFF, Web BFF, etc.
- Optimized responses for each client

---

## Chapter 15: Reliability Patterns

### 15.1 Circuit Breaker
```
States:
CLOSED â†’ (failures exceed threshold) â†’ OPEN
                                          â†“
                              (timeout expires)
                                          â†“
                                     HALF-OPEN
                                          â†“
                        (success) â†’ CLOSED
                        (failure) â†’ OPEN
```

```python
from enum import Enum
import time

class CircuitState(Enum):
    CLOSED = "closed"
    OPEN = "open"
    HALF_OPEN = "half_open"

class CircuitBreaker:
    def __init__(self, failure_threshold=5, timeout=30):
        self.failure_threshold = failure_threshold
        self.timeout = timeout
        self.failures = 0
        self.state = CircuitState.CLOSED
        self.last_failure_time = None
    
    def call(self, func, *args, **kwargs):
        if self.state == CircuitState.OPEN:
            if time.time() - self.last_failure_time > self.timeout:
                self.state = CircuitState.HALF_OPEN
            else:
                raise Exception("Circuit is OPEN")
        
        try:
            result = func(*args, **kwargs)
            self._on_success()
            return result
        except Exception as e:
            self._on_failure()
            raise e
    
    def _on_success(self):
        self.failures = 0
        self.state = CircuitState.CLOSED
    
    def _on_failure(self):
        self.failures += 1
        self.last_failure_time = time.time()
        if self.failures >= self.failure_threshold:
            self.state = CircuitState.OPEN
```

### 15.2 Retry with Exponential Backoff
```python
import time
import random

def retry_with_backoff(func, max_retries=5, base_delay=1):
    for attempt in range(max_retries):
        try:
            return func()
        except Exception as e:
            if attempt == max_retries - 1:
                raise e
            
            delay = base_delay * (2 ** attempt)
            jitter = random.uniform(0, delay * 0.1)
            time.sleep(delay + jitter)
```

> ğŸ’¡ **Important**: Always add jitter to prevent thundering herd

### 15.3 Bulkhead Pattern
- Isolate failures to prevent cascade
- Separate thread pools per service
- Resource quotas

### 15.4 Timeout Pattern
- Always set timeouts
- Consider: connection timeout vs read timeout
- Propagate deadlines

### 15.5 Idempotency
- Safe to retry same operation
- **Implementation**:
  - Idempotency key in request
  - Store result, return on retry
  
```python
@app.post("/payments")
async def create_payment(
    payment: PaymentRequest,
    idempotency_key: str = Header(...)
):
    # Check if already processed
    existing = await cache.get(f"payment:{idempotency_key}")
    if existing:
        return existing
    
    # Process payment
    result = await process_payment(payment)
    
    # Store result
    await cache.set(f"payment:{idempotency_key}", result, ttl=86400)
    return result
```

---

## Chapter 16: Scalability Patterns

### 16.1 Horizontal vs Vertical Scaling
- **Vertical**: Bigger machine (limited)
- **Horizontal**: More machines (preferred)

### 16.2 Database Scaling Strategies
```
Read Scaling:
Primary (writes) â†’ Replica 1 (reads)
                â†’ Replica 2 (reads)
                â†’ Replica 3 (reads)

Write Scaling:
Shard 1 (users A-M)
Shard 2 (users N-Z)
```

### 16.3 CQRS (Command Query Responsibility Segregation)
- Separate read and write models
- Different databases optimized for each
- Event sourcing often combined

```
Commands â†’ Write Model â†’ Event Store
                              â†“
                         Projections
                              â†“
                        Read Model â† Queries
```

### 16.4 Event Sourcing
- Store events, not current state
- Replay events to rebuild state
- Complete audit trail

```python
# Event Store Example
class EventStore:
    def __init__(self):
        self.events = []
    
    def append(self, event):
        self.events.append({
            'type': event['type'],
            'data': event['data'],
            'timestamp': time.time(),
            'version': len(self.events) + 1
        })
    
    def get_events(self, aggregate_id):
        return [e for e in self.events 
                if e['data'].get('aggregate_id') == aggregate_id]

# Rebuild state from events
def rebuild_order(events):
    order = {}
    for event in events:
        if event['type'] == 'OrderCreated':
            order = {'id': event['data']['order_id'], 'items': [], 'status': 'created'}
        elif event['type'] == 'ItemAdded':
            order['items'].append(event['data']['item'])
        elif event['type'] == 'OrderShipped':
            order['status'] = 'shipped'
    return order
```

### 16.5 Rate Limiting
**Algorithms**:
- Token Bucket
- Leaky Bucket
- Fixed Window
- Sliding Window

```python
# Token Bucket Implementation
import time

class TokenBucket:
    def __init__(self, capacity, fill_rate):
        self.capacity = capacity
        self.fill_rate = fill_rate  # tokens per second
        self.tokens = capacity
        self.last_fill = time.time()
    
    def consume(self, tokens=1):
        now = time.time()
        # Refill tokens
        elapsed = now - self.last_fill
        self.tokens = min(self.capacity, self.tokens + elapsed * self.fill_rate)
        self.last_fill = now
        
        if self.tokens >= tokens:
            self.tokens -= tokens
            return True
        return False
```

---

## Chapter 17: Data Pipeline Patterns

### 17.1 Batch Processing
- Process large amounts of data periodically
- Tools: Apache Spark, Hadoop MapReduce
- Use cases: Analytics, ML training, reports

### 17.2 Stream Processing
- Process data in real-time
- Tools: Apache Kafka Streams, Apache Flink, Apache Spark Streaming
- Use cases: Real-time analytics, alerting, fraud detection

### 17.3 Lambda Architecture
```
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Batch Layer   â”‚ â† All data, periodic processing
        â”‚   (accuracy)    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
Data â†’ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Serving Layer â†’ Queries
                 â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Speed Layer    â”‚ â† Recent data, real-time
        â”‚  (low latency)  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 17.4 Kappa Architecture
- Simplification of Lambda
- Stream processing only
- Reprocess by replaying stream

### 17.5 Change Data Capture (CDC)
- Capture database changes as events
- Tools: Debezium, Maxwell
- Use cases: Cache invalidation, search indexing, replication

---

# Part VI: Observability & Operations (Weeks 25-28)

## Chapter 18: Monitoring and Observability

### 18.1 Three Pillars of Observability
1. **Metrics**: Numeric measurements over time
2. **Logs**: Discrete events with context
3. **Traces**: Request flow across services

### 18.2 Metrics
- **Types**:
  - Counter: Only increases (requests, errors)
  - Gauge: Can go up/down (temperature, queue size)
  - Histogram: Distribution of values (latency)
  - Summary: Similar to histogram, with quantiles

- **Key Metrics (RED Method)**:
  - Rate: Requests per second
  - Errors: Failed requests
  - Duration: Latency distribution

- **Tools**: Prometheus, Grafana, Datadog

```python
# Prometheus metrics with FastAPI
from prometheus_client import Counter, Histogram
from prometheus_fastapi_instrumentator import Instrumentator

REQUEST_COUNT = Counter(
    'http_requests_total',
    'Total HTTP requests',
    ['method', 'endpoint', 'status']
)

REQUEST_LATENCY = Histogram(
    'http_request_duration_seconds',
    'HTTP request latency',
    ['method', 'endpoint']
)

@app.middleware("http")
async def metrics_middleware(request, call_next):
    start_time = time.time()
    response = await call_next(request)
    
    REQUEST_COUNT.labels(
        method=request.method,
        endpoint=request.url.path,
        status=response.status_code
    ).inc()
    
    REQUEST_LATENCY.labels(
        method=request.method,
        endpoint=request.url.path
    ).observe(time.time() - start_time)
    
    return response
```

### 18.3 Distributed Tracing
- Track requests across services
- **Concepts**:
  - Trace: Full request journey
  - Span: Single operation
  - Context propagation

- **Tools**: Jaeger, Zipkin, OpenTelemetry

```python
# OpenTelemetry tracing
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider

trace.set_tracer_provider(TracerProvider())
tracer = trace.get_tracer(__name__)

async def process_order(order_id):
    with tracer.start_as_current_span("process_order") as span:
        span.set_attribute("order.id", order_id)
        
        with tracer.start_as_current_span("validate_order"):
            await validate(order_id)
        
        with tracer.start_as_current_span("charge_payment"):
            await charge(order_id)
```

### 18.4 Structured Logging
```python
import structlog

logger = structlog.get_logger()

logger.info(
    "order_processed",
    order_id="12345",
    user_id="user_789",
    amount=99.99,
    duration_ms=45
)

# Output: {"event": "order_processed", "order_id": "12345", ...}
```

### 18.5 Alerting Best Practices
- Alert on symptoms, not causes
- Actionable alerts only
- Include runbook links
- Avoid alert fatigue

---

## Chapter 19: Deployment and Operations

### 19.1 Deployment Strategies
- **Blue-Green**: Two identical environments, switch traffic
- **Canary**: Gradually roll out to small percentage
- **Rolling**: Replace instances one by one
- **Feature Flags**: Toggle features without deployment

### 19.2 Containerization
- **Docker**: Package application with dependencies
- **Kubernetes**: Container orchestration

```yaml
# Kubernetes Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: user-service
spec:
  replicas: 3
  selector:
    matchLabels:
      app: user-service
  template:
    metadata:
      labels:
        app: user-service
    spec:
      containers:
      - name: user-service
        image: user-service:v1.2.3
        ports:
        - containerPort: 8080
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /health/live
            port: 8080
        readinessProbe:
          httpGet:
            path: /health/ready
            port: 8080
```

### 19.3 Configuration Management
- **12-Factor App**: Config in environment
- **Secret Management**: Vault, AWS Secrets Manager
- **Feature Flags**: LaunchDarkly, Unleash

### 19.4 Chaos Engineering
- Intentionally inject failures
- Discover weaknesses before production
- Tools: Chaos Monkey, Gremlin, LitmusChaos

**Principles**:
1. Define steady state
2. Hypothesize about steady state
3. Introduce realistic failures
4. Try to disprove hypothesis

---

# Part VII: System Design Interview (Weeks 29-32)

## Chapter 20: System Design Framework

### 20.1 The RESHADED Framework
1. **R**equirements: Clarify functional and non-functional
2. **E**stimation: Back-of-envelope calculations
3. **S**torage Schema: Data model design
4. **H**igh-level Design: Component diagram
5. **A**PI Design: Endpoints and contracts
6. **D**etailed Design: Deep dive into components
7. **E**valuate: Trade-offs and bottlenecks
8. **D**istinctive: Scaling, reliability, edge cases

### 20.2 Back-of-Envelope Calculations
```
Key Numbers to Memorize:
- 1 day = 86,400 seconds â‰ˆ 100,000 seconds
- 1 million requests/day â‰ˆ 12 requests/second
- 1 billion requests/day â‰ˆ 12,000 requests/second

Storage:
- 1 character = 1 byte (ASCII) or 2-4 bytes (UTF-8)
- 1 KB = 1,000 bytes (roughly 1 short paragraph)
- 1 MB = 1,000 KB (roughly 1 photo)
- 1 GB = 1,000 MB (roughly 1 hour of HD video)
- 1 TB = 1,000 GB

Network:
- 1 Gbps = 125 MB/s
- Datacenter RTT: ~0.5ms
- Same region: ~5ms
- Cross-country: ~50ms
- Cross-continent: ~150ms
```

### 20.3 Common Interview Questions

**URL Shortener (Easy)**
- Key: Hash function, base62 encoding
- Focus: Read-heavy, caching, analytics

**Twitter/Feed System (Medium)**
- Key: Fan-out on write vs fan-out on read
- Focus: Celebrity problem, timeline generation

**WhatsApp/Chat System (Medium)**
- Key: WebSocket, message delivery guarantees
- Focus: Presence, group chats, end-to-end encryption

**YouTube/Video Streaming (Hard)**
- Key: CDN, transcoding, chunked streaming
- Focus: Upload pipeline, recommendations

**Google Docs/Collaborative Editing (Hard)**
- Key: Operational Transformation or CRDTs
- Focus: Conflict resolution, real-time sync

**Uber/Ride Sharing (Hard)**
- Key: Geospatial indexing, matching algorithm
- Focus: ETA calculation, surge pricing

---

## Chapter 21: Deep Dive Case Studies

### 21.1 Designing a Rate Limiter
```
Requirements:
- Limit requests per user/IP
- Distributed (multiple servers)
- Low latency (<10ms overhead)

High-Level Design:
Client â†’ Rate Limiter â†’ Service

Options:
1. Token Bucket (allows bursts)
2. Sliding Window (smooth limiting)

Distributed Implementation:
- Redis for shared state
- Lua scripts for atomicity
```

```python
# Distributed Rate Limiter with Redis
class DistributedRateLimiter:
    def __init__(self, redis_client, limit, window_seconds):
        self.redis = redis_client
        self.limit = limit
        self.window = window_seconds
    
    def is_allowed(self, user_id):
        key = f"rate_limit:{user_id}"
        current_time = int(time.time())
        window_start = current_time - self.window
        
        # Lua script for atomic operation
        script = """
        redis.call('ZREMRANGEBYSCORE', KEYS[1], 0, ARGV[1])
        local count = redis.call('ZCARD', KEYS[1])
        if count < tonumber(ARGV[2]) then
            redis.call('ZADD', KEYS[1], ARGV[3], ARGV[3])
            redis.call('EXPIRE', KEYS[1], ARGV[4])
            return 1
        end
        return 0
        """
        
        result = self.redis.eval(
            script, 1, key,
            window_start, self.limit, current_time, self.window
        )
        return result == 1
```

### 21.2 Designing a Notification System
```
Requirements:
- Multiple channels (push, email, SMS)
- Millions of users
- Delivery guarantees
- Rate limiting per user

Components:
1. Notification Service (API)
2. Message Queue (Kafka)
3. Workers (per channel)
4. User Preference Store
5. Rate Limiter
6. Delivery Tracker

Flow:
Request â†’ Validation â†’ Queue â†’ Worker â†’ Channel Provider
                                  â†“
                          Delivery Tracking â†’ Retry if failed
```

### 21.3 Designing a Distributed Cache
```
Requirements:
- Sub-millisecond latency
- High availability
- Horizontal scalability

Design Decisions:
- Consistent hashing for partitioning
- Replication for availability
- LRU eviction policy

Architecture:
Client â†’ Proxy (optional) â†’ Cache Nodes
              â†“
        Configuration (etcd)
```

---

## Chapter 22: Interview Tips and Strategies

### 22.1 Communication
- Think out loud
- Ask clarifying questions
- State assumptions explicitly
- Discuss trade-offs

### 22.2 Time Management (45 min interview)
- 5 min: Requirements gathering
- 5 min: Estimations
- 15 min: High-level design
- 15 min: Detailed design
- 5 min: Wrap-up and questions

### 22.3 Common Mistakes to Avoid
- Jumping to solution without requirements
- Not considering scale
- Over-engineering for simple problems
- Ignoring failure scenarios
- Not discussing trade-offs

### 22.4 What Interviewers Look For
- Problem decomposition
- Knowledge of building blocks
- Trade-off analysis
- Handling ambiguity
- Communication skills

---

# Part VIII: Resources and Practice

## Chapter 23: Learning Resources

### 23.1 Must-Read Books
1. **Designing Data-Intensive Applications** - Martin Kleppmann (THE Bible)
2. **System Design Interview** - Alex Xu (Vol 1 & 2)
3. **Building Microservices** - Sam Newman
4. **Site Reliability Engineering** - Google
5. **Database Internals** - Alex Petrov

### 23.2 Online Courses
- MIT 6.824: Distributed Systems (Free)
- Designing Data-Intensive Applications (O'Reilly)
- Grokking the System Design Interview (Educative)

### 23.3 Practice Platforms
- LeetCode System Design
- Pramp (Mock Interviews)
- Interviewing.io

### 23.4 Blogs and Websites
- High Scalability Blog
- Netflix Tech Blog
- Uber Engineering Blog
- AWS Architecture Blog
- Martin Fowler's Blog

### 23.5 Papers to Read
- Google MapReduce
- Google Bigtable
- Amazon Dynamo
- Google Spanner
- Apache Kafka
- Raft Consensus

---

## Chapter 24: Study Plan

### Phase 1: Foundation (Weeks 1-4)
- [ ] Complete Python backend basics
- [ ] Build simple REST API with FastAPI
- [ ] Set up PostgreSQL, write basic queries
- [ ] Set up Redis, understand caching basics

### Phase 2: Communication (Weeks 5-8)
- [ ] Implement message queue with RabbitMQ
- [ ] Build producer/consumer with Kafka
- [ ] Create gRPC service
- [ ] Implement service discovery

### Phase 3: Data (Weeks 9-14)
- [ ] Study CAP theorem deeply
- [ ] Implement consistent hashing
- [ ] Study and implement caching patterns
- [ ] Build simple distributed counter

### Phase 4: Consensus (Weeks 15-18)
- [ ] Study Raft algorithm thoroughly
- [ ] Implement simple leader election
- [ ] Build distributed lock with Redis
- [ ] Study ZooKeeper concepts

### Phase 5: Patterns (Weeks 19-24)
- [ ] Implement circuit breaker
- [ ] Build saga pattern example
- [ ] Study microservices architecture
- [ ] Implement event sourcing example

### Phase 6: Operations (Weeks 25-28)
- [ ] Set up Prometheus + Grafana
- [ ] Implement distributed tracing
- [ ] Deploy to Kubernetes
- [ ] Practice incident response

### Phase 7: Interview Prep (Weeks 29-32)
- [ ] Solve 20+ system design problems
- [ ] Do mock interviews
- [ ] Review all concepts
- [ ] Practice whiteboarding

---

## Quick Reference Card

### Key Formulas
```
Availability = Uptime / (Uptime + Downtime)
99.9% = 8.76 hours downtime/year
99.99% = 52.6 minutes downtime/year

Throughput = Requests / Time
Latency = Time for single request

CAP: Choose 2 of 3 (really: choose C or A when P happens)
Quorum: W + R > N for strong consistency

Replication Factor: Usually 3
Partitions: Based on throughput needs
```

### Decision Matrix
| Scenario | Solution |
|----------|----------|
| Need strong consistency | Single leader + sync replication |
| Need high availability | Multi-leader or leaderless |
| Read heavy | Caching + read replicas |
| Write heavy | Sharding + async replication |
| Need transactions | 2PC or Saga pattern |
| Real-time updates | WebSocket + pub/sub |

---

**Final Note**: Distributed systems is a vast field. Focus on understanding WHY things are done a certain way, not just HOW. The principles remain constant even as technologies change.

Good luck with your interviews! ğŸš€
