# Chapter 10: Caching Strategies

> **Goal**: Master caching patterns to dramatically improve performance in distributed systems

---

## 10.1 Why Caching Matters

```
┌─────────────────────────────────────────────────────────────┐
│                  WHY CACHING?                                │
│                                                              │
│  Without Cache:                                             │
│  ──────────────                                             │
│  User Request → App Server → Database → Response            │
│                                  │                          │
│                            100ms latency                    │
│                            Limited throughput                │
│                                                              │
│  With Cache:                                                │
│  ────────────                                               │
│  User Request → App Server → Cache (HIT!) → Response        │
│                                  │                          │
│                             1ms latency                     │
│                             100x faster!                    │
│                                                              │
│                                                              │
│  Real-World Impact:                                         │
│  ──────────────────                                         │
│  ┌──────────────────────────────────────────────────┐      │
│  │ Operation              │ Without Cache │ With    │      │
│  ├──────────────────────────────────────────────────┤      │
│  │ Database query         │ 10-100ms      │ 0.1ms   │      │
│  │ API call to service    │ 50-500ms      │ 0.5ms   │      │
│  │ Complex computation    │ 1-10 seconds  │ 1ms     │      │
│  │ File read from disk    │ 10-20ms       │ 0.01ms  │      │
│  └──────────────────────────────────────────────────┘      │
│                                                              │
│  Cache Hit Ratio Target: >95%                               │
│  A 1% improvement can mean millions saved at scale          │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

---

## 10.2 Cache Fundamentals

### Cache Hit vs Miss

```
┌─────────────────────────────────────────────────────────────┐
│              CACHE HIT vs CACHE MISS                         │
│                                                              │
│  CACHE HIT (Fast Path):                                     │
│  ──────────────────────                                     │
│                                                              │
│  Client ──▶ Cache ──▶ Data Found! ──▶ Return to Client     │
│              │                                               │
│         [user:123] ✓                                        │
│                                                              │
│  Time: ~1ms                                                 │
│                                                              │
│                                                              │
│  CACHE MISS (Slow Path):                                    │
│  ───────────────────────                                    │
│                                                              │
│  Client ──▶ Cache ──▶ Not Found! ──▶ Database ──▶ Client   │
│              │              │              │                 │
│         [user:123] ✗    Fetch data    Store in cache       │
│                                                              │
│  Time: ~100ms (but next request will be fast)              │
│                                                              │
│                                                              │
│  Key Metrics:                                               │
│  ────────────                                               │
│  Hit Rate = Hits / (Hits + Misses)                         │
│                                                              │
│  Target: >95% for most applications                         │
│  Excellent: >99%                                            │
│  Poor: <80% (investigate!)                                  │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

### Cache Layers

```
┌─────────────────────────────────────────────────────────────┐
│                    CACHE LAYERS                              │
│                                                              │
│  Layer 1: CPU Cache (L1/L2/L3)                             │
│  ─────────────────────────────                              │
│  - Nanoseconds access                                       │
│  - Hardware managed                                         │
│  - KB to MB size                                            │
│                                                              │
│  Layer 2: Application Memory Cache                          │
│  ─────────────────────────────────                          │
│  - Microseconds access                                      │
│  - In-process (dict, LRU cache)                            │
│  - Limited to single instance                               │
│                                                              │
│  Layer 3: Distributed Cache (Redis, Memcached)             │
│  ─────────────────────────────────────────────              │
│  - Sub-millisecond access                                   │
│  - Shared across instances                                  │
│  - Network overhead                                         │
│                                                              │
│  Layer 4: CDN (CloudFront, Cloudflare)                     │
│  ─────────────────────────────────────                      │
│  - Milliseconds access                                      │
│  - Edge locations worldwide                                 │
│  - Static content, API responses                            │
│                                                              │
│  Layer 5: Database Query Cache                              │
│  ─────────────────────────────                              │
│  - Milliseconds access                                      │
│  - Built into database                                      │
│  - Limited control                                          │
│                                                              │
│                                                              │
│  Request Flow:                                              │
│  ┌─────┐  ┌─────┐  ┌─────┐  ┌─────┐  ┌─────┐             │
│  │ CDN │→│Local│→│Redis│→│ DB  │→│Disk │             │
│  │Cache│  │Cache│  │     │  │Cache│  │     │             │
│  └─────┘  └─────┘  └─────┘  └─────┘  └─────┘             │
│   1ms      0.1ms    0.5ms    5ms      20ms                 │
│                                                              │
│  Each layer is fallback for the one before it              │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

---

## 10.3 Caching Patterns

### 10.3.1 Cache-Aside (Lazy Loading)

```
┌─────────────────────────────────────────────────────────────┐
│              CACHE-ASIDE PATTERN                             │
│                                                              │
│  Application manages both cache and database.               │
│  Most common and flexible pattern.                          │
│                                                              │
│  READ FLOW:                                                 │
│  ──────────                                                 │
│                                                              │
│  1. App checks cache                                        │
│     ┌─────────┐        ┌─────────┐                         │
│     │   App   │──GET──▶│  Cache  │                         │
│     └────┬────┘        └────┬────┘                         │
│          │                  │                               │
│  2. If MISS, query DB       │ HIT? Return!                 │
│     │                       │                               │
│     │    ┌─────────┐        │                               │
│     └───▶│Database │        │                               │
│          └────┬────┘        │                               │
│               │             │                               │
│  3. Store in cache          │                               │
│               │             │                               │
│     ┌─────────┴─────────────┘                              │
│     │                                                       │
│     ▼                                                       │
│  Return to client                                           │
│                                                              │
│                                                              │
│  WRITE FLOW:                                                │
│  ───────────                                                │
│                                                              │
│  1. Write to database                                       │
│  2. Invalidate/delete from cache                            │
│     (NOT update - let next read populate)                   │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

```python
# cache_aside.py
import redis.asyncio as redis
import json
import asyncpg
from typing import Optional, Any, TypeVar, Callable
from functools import wraps
import hashlib

T = TypeVar('T')

class CacheAside:
    """
    Cache-Aside pattern implementation.
    Application controls cache population.
    """
    
    def __init__(
        self, 
        redis_client: redis.Redis,
        default_ttl: int = 3600  # 1 hour
    ):
        self.redis = redis_client
        self.default_ttl = default_ttl
    
    async def get(self, key: str) -> Optional[Any]:
        """Get from cache"""
        data = await self.redis.get(key)
        if data:
            return json.loads(data)
        return None
    
    async def set(
        self, 
        key: str, 
        value: Any, 
        ttl: int = None
    ):
        """Set in cache with TTL"""
        await self.redis.setex(
            key,
            ttl or self.default_ttl,
            json.dumps(value, default=str)
        )
    
    async def delete(self, key: str):
        """Invalidate cache entry"""
        await self.redis.delete(key)
    
    async def get_or_set(
        self,
        key: str,
        fetch_func: Callable,
        ttl: int = None
    ) -> Any:
        """
        Get from cache, or fetch and cache if missing.
        This is the core cache-aside pattern.
        """
        # Try cache first
        cached = await self.get(key)
        if cached is not None:
            print(f"Cache HIT: {key}")
            return cached
        
        print(f"Cache MISS: {key}")
        
        # Fetch from source
        data = await fetch_func()
        
        # Store in cache
        if data is not None:
            await self.set(key, data, ttl)
        
        return data

# ============== USAGE WITH REPOSITORY ==============

class UserRepository:
    def __init__(self, db: asyncpg.Pool, cache: CacheAside):
        self.db = db
        self.cache = cache
    
    async def get_user(self, user_id: int) -> Optional[dict]:
        """Get user with cache-aside pattern"""
        cache_key = f"user:{user_id}"
        
        async def fetch_from_db():
            async with self.db.acquire() as conn:
                row = await conn.fetchrow(
                    "SELECT * FROM users WHERE id = $1",
                    user_id
                )
                return dict(row) if row else None
        
        return await self.cache.get_or_set(cache_key, fetch_from_db)
    
    async def update_user(self, user_id: int, data: dict) -> bool:
        """Update user and invalidate cache"""
        async with self.db.acquire() as conn:
            await conn.execute(
                """UPDATE users 
                   SET name = $2, email = $3 
                   WHERE id = $1""",
                user_id, data['name'], data['email']
            )
        
        # Invalidate cache - next read will repopulate
        await self.cache.delete(f"user:{user_id}")
        return True
    
    async def delete_user(self, user_id: int) -> bool:
        """Delete user and invalidate cache"""
        async with self.db.acquire() as conn:
            await conn.execute(
                "DELETE FROM users WHERE id = $1",
                user_id
            )
        
        await self.cache.delete(f"user:{user_id}")
        return True

# ============== DECORATOR VERSION ==============

def cached(
    key_prefix: str,
    ttl: int = 3600,
    key_builder: Callable = None
):
    """
    Decorator for cache-aside pattern.
    """
    def decorator(func: Callable):
        @wraps(func)
        async def wrapper(self, *args, **kwargs):
            # Build cache key
            if key_builder:
                cache_key = key_builder(*args, **kwargs)
            else:
                # Default: use function name + args
                key_parts = [key_prefix, func.__name__]
                key_parts.extend(str(a) for a in args)
                key_parts.extend(f"{k}={v}" for k, v in sorted(kwargs.items()))
                cache_key = ":".join(key_parts)
            
            # Try cache
            cached_value = await self.cache.get(cache_key)
            if cached_value is not None:
                return cached_value
            
            # Call function
            result = await func(self, *args, **kwargs)
            
            # Cache result
            if result is not None:
                await self.cache.set(cache_key, result, ttl)
            
            return result
        
        return wrapper
    return decorator

# Usage with decorator
class ProductService:
    def __init__(self, db, cache: CacheAside):
        self.db = db
        self.cache = cache
    
    @cached(key_prefix="product", ttl=1800)
    async def get_product(self, product_id: int) -> Optional[dict]:
        """Automatically cached for 30 minutes"""
        async with self.db.acquire() as conn:
            row = await conn.fetchrow(
                "SELECT * FROM products WHERE id = $1",
                product_id
            )
            return dict(row) if row else None
```

### 10.3.2 Write-Through

```
┌─────────────────────────────────────────────────────────────┐
│              WRITE-THROUGH PATTERN                           │
│                                                              │
│  Writes go to cache AND database together.                  │
│  Cache is always up-to-date.                                │
│                                                              │
│  WRITE FLOW:                                                │
│  ───────────                                                │
│                                                              │
│     ┌─────────┐                                             │
│     │   App   │                                             │
│     └────┬────┘                                             │
│          │ Write                                            │
│          ▼                                                   │
│     ┌─────────┐        ┌─────────┐                         │
│     │  Cache  │───────▶│Database │                         │
│     │ (write) │  sync  │ (write) │                         │
│     └─────────┘        └─────────┘                         │
│                                                              │
│  Both writes happen together (or in transaction)           │
│                                                              │
│                                                              │
│  Pros:                                                      │
│  ├── Cache always consistent with DB                        │
│  ├── Simple read path (always hit cache)                    │
│  └── No stale data                                          │
│                                                              │
│  Cons:                                                      │
│  ├── Higher write latency (two writes)                      │
│  ├── Cache may store rarely-read data                       │
│  └── Both must succeed (complexity)                         │
│                                                              │
│  Use When:                                                  │
│  ├── Read-heavy workloads                                   │
│  ├── Can't tolerate stale reads                             │
│  └── Write latency is acceptable                            │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

```python
# write_through.py
import redis.asyncio as redis
import json
import asyncpg
from typing import Any, Optional

class WriteThrough:
    """
    Write-Through cache pattern.
    Writes update both cache and database synchronously.
    """
    
    def __init__(
        self, 
        redis_client: redis.Redis,
        db_pool: asyncpg.Pool,
        ttl: int = 3600
    ):
        self.redis = redis_client
        self.db = db_pool
        self.ttl = ttl
    
    async def read(self, key: str) -> Optional[Any]:
        """
        Read always goes to cache (assumed to be in sync).
        Falls back to DB only if cache is empty.
        """
        data = await self.redis.get(key)
        if data:
            return json.loads(data)
        
        # Cache miss - this shouldn't happen often with write-through
        # Populate from DB
        return await self._load_from_db(key)
    
    async def write(self, key: str, value: dict, table: str) -> bool:
        """
        Write to cache AND database together.
        """
        # Start with database write (source of truth)
        async with self.db.acquire() as conn:
            async with conn.transaction():
                # Build dynamic UPDATE query
                set_clause = ", ".join(
                    f"{k} = ${i+2}" 
                    for i, k in enumerate(value.keys()) 
                    if k != 'id'
                )
                
                query = f"""
                    INSERT INTO {table} (id, {', '.join(value.keys())})
                    VALUES ($1, {', '.join(f'${i+2}' for i in range(len(value)))})
                    ON CONFLICT (id) DO UPDATE SET {set_clause}
                """
                
                await conn.execute(query, key, *value.values())
        
        # Write to cache (after DB succeeds)
        await self.redis.setex(
            key,
            self.ttl,
            json.dumps(value, default=str)
        )
        
        return True
    
    async def delete(self, key: str, table: str) -> bool:
        """Delete from both cache and database"""
        async with self.db.acquire() as conn:
            await conn.execute(
                f"DELETE FROM {table} WHERE id = $1",
                key
            )
        
        await self.redis.delete(key)
        return True
    
    async def _load_from_db(self, key: str) -> Optional[dict]:
        """Load data from DB and populate cache"""
        # Implementation depends on key format
        # Example: "user:123" -> table=users, id=123
        parts = key.split(":")
        table = parts[0] + "s"  # Simple pluralization
        id_value = parts[1]
        
        async with self.db.acquire() as conn:
            row = await conn.fetchrow(
                f"SELECT * FROM {table} WHERE id = $1",
                int(id_value)
            )
            
            if row:
                data = dict(row)
                await self.redis.setex(key, self.ttl, json.dumps(data, default=str))
                return data
        
        return None
```

### 10.3.3 Write-Behind (Write-Back)

```
┌─────────────────────────────────────────────────────────────┐
│              WRITE-BEHIND PATTERN                            │
│                                                              │
│  Write to cache immediately, async write to database.       │
│  Fastest writes, but risk of data loss.                     │
│                                                              │
│  WRITE FLOW:                                                │
│  ───────────                                                │
│                                                              │
│     ┌─────────┐                                             │
│     │   App   │                                             │
│     └────┬────┘                                             │
│          │ Write (fast return)                              │
│          ▼                                                   │
│     ┌─────────┐                                             │
│     │  Cache  │                                             │
│     │ (write) │                                             │
│     └────┬────┘                                             │
│          │ Async (batched)                                  │
│          │ ┌────────────────────┐                          │
│          └─│  Background Writer │                          │
│            │  (batches writes,  │                          │
│            │   retries on fail) │                          │
│            └─────────┬──────────┘                          │
│                      │                                       │
│                      ▼                                       │
│               ┌─────────┐                                   │
│               │Database │                                   │
│               └─────────┘                                   │
│                                                              │
│                                                              │
│  Pros:                                                      │
│  ├── Very fast writes                                       │
│  ├── Batching reduces DB load                               │
│  └── Absorbs write spikes                                   │
│                                                              │
│  Cons:                                                      │
│  ├── DATA LOSS if cache fails before DB write              │
│  ├── Complex failure handling                               │
│  └── Eventual consistency with DB                           │
│                                                              │
│  Use When:                                                  │
│  ├── Write-heavy workloads                                  │
│  ├── Can tolerate some data loss                            │
│  ├── Write latency is critical                              │
│  └── Analytics, metrics, logs                               │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

```python
# write_behind.py
import redis.asyncio as redis
import json
import asyncio
import asyncpg
from typing import Any, Dict, List
from dataclasses import dataclass
from datetime import datetime
import time

@dataclass
class WriteOperation:
    key: str
    value: dict
    table: str
    timestamp: float
    operation: str  # 'SET' or 'DELETE'

class WriteBehindCache:
    """
    Write-Behind (Write-Back) cache pattern.
    Writes go to cache immediately, database is updated asynchronously.
    """
    
    def __init__(
        self,
        redis_client: redis.Redis,
        db_pool: asyncpg.Pool,
        flush_interval: float = 1.0,  # seconds
        batch_size: int = 100
    ):
        self.redis = redis_client
        self.db = db_pool
        self.flush_interval = flush_interval
        self.batch_size = batch_size
        
        self.write_queue: asyncio.Queue = asyncio.Queue()
        self.running = False
        self._flush_task = None
    
    async def start(self):
        """Start the background writer"""
        self.running = True
        self._flush_task = asyncio.create_task(self._flush_loop())
        print("Write-behind cache started")
    
    async def stop(self):
        """Stop and flush remaining writes"""
        self.running = False
        if self._flush_task:
            # Flush remaining items
            await self._flush_batch(drain=True)
            self._flush_task.cancel()
        print("Write-behind cache stopped")
    
    async def read(self, key: str) -> Any:
        """Read from cache"""
        data = await self.redis.get(key)
        return json.loads(data) if data else None
    
    async def write(self, key: str, value: dict, table: str):
        """
        Write to cache immediately, queue for DB write.
        Returns immediately - very fast!
        """
        # Write to cache (immediate)
        await self.redis.set(key, json.dumps(value, default=str))
        
        # Queue for async DB write
        op = WriteOperation(
            key=key,
            value=value,
            table=table,
            timestamp=time.time(),
            operation='SET'
        )
        await self.write_queue.put(op)
        
        return True
    
    async def delete(self, key: str, table: str):
        """Delete from cache and queue DB delete"""
        await self.redis.delete(key)
        
        op = WriteOperation(
            key=key,
            value={},
            table=table,
            timestamp=time.time(),
            operation='DELETE'
        )
        await self.write_queue.put(op)
    
    async def _flush_loop(self):
        """Background loop that flushes writes to database"""
        while self.running:
            try:
                await asyncio.sleep(self.flush_interval)
                await self._flush_batch()
            except asyncio.CancelledError:
                break
            except Exception as e:
                print(f"Flush error: {e}")
    
    async def _flush_batch(self, drain: bool = False):
        """Flush a batch of writes to database"""
        operations: List[WriteOperation] = []
        
        # Collect batch
        while len(operations) < self.batch_size:
            try:
                if drain:
                    op = self.write_queue.get_nowait()
                else:
                    op = await asyncio.wait_for(
                        self.write_queue.get(),
                        timeout=0.1
                    )
                operations.append(op)
            except (asyncio.QueueEmpty, asyncio.TimeoutError):
                break
        
        if not operations:
            return
        
        print(f"Flushing {len(operations)} operations to database")
        
        # Group by table for batch efficiency
        by_table: Dict[str, List[WriteOperation]] = {}
        for op in operations:
            by_table.setdefault(op.table, []).append(op)
        
        # Execute batches
        async with self.db.acquire() as conn:
            for table, ops in by_table.items():
                await self._execute_batch(conn, table, ops)
    
    async def _execute_batch(
        self, 
        conn: asyncpg.Connection,
        table: str,
        operations: List[WriteOperation]
    ):
        """Execute a batch of operations for one table"""
        for op in operations:
            try:
                if op.operation == 'SET':
                    # Upsert
                    columns = list(op.value.keys())
                    values = list(op.value.values())
                    
                    placeholders = [f"${i+1}" for i in range(len(values))]
                    update_clause = ", ".join(
                        f"{col} = EXCLUDED.{col}" 
                        for col in columns if col != 'id'
                    )
                    
                    query = f"""
                        INSERT INTO {table} ({', '.join(columns)})
                        VALUES ({', '.join(placeholders)})
                        ON CONFLICT (id) DO UPDATE SET {update_clause}
                    """
                    await conn.execute(query, *values)
                    
                elif op.operation == 'DELETE':
                    key_id = op.key.split(":")[-1]
                    await conn.execute(
                        f"DELETE FROM {table} WHERE id = $1",
                        int(key_id)
                    )
                    
            except Exception as e:
                print(f"Failed to write {op.key}: {e}")
                # In production: dead letter queue, retry logic

# ============== USAGE ==============

async def demo_write_behind():
    redis_client = redis.Redis()
    db_pool = await asyncpg.create_pool("postgresql://localhost/test")
    
    cache = WriteBehindCache(redis_client, db_pool)
    await cache.start()
    
    # Writes return immediately
    start = time.time()
    for i in range(1000):
        await cache.write(
            f"user:{i}",
            {"id": i, "name": f"User {i}", "score": i * 10},
            "users"
        )
    print(f"1000 writes queued in {time.time() - start:.3f}s")
    
    # Read back (from cache, instant)
    user = await cache.read("user:500")
    print(f"Read user: {user}")
    
    # Wait for background flush
    await asyncio.sleep(3)
    
    await cache.stop()
```

### 10.3.4 Read-Through

```
┌─────────────────────────────────────────────────────────────┐
│              READ-THROUGH PATTERN                            │
│                                                              │
│  Cache handles loading data transparently.                  │
│  App only talks to cache, never directly to DB.             │
│                                                              │
│  READ FLOW:                                                 │
│  ──────────                                                 │
│                                                              │
│     ┌─────────┐                                             │
│     │   App   │                                             │
│     └────┬────┘                                             │
│          │ Read                                             │
│          ▼                                                   │
│     ┌─────────┐                                             │
│     │  Cache  │                                             │
│     │ (smart) │                                             │
│     └────┬────┘                                             │
│          │                                                   │
│     HIT? │ Return immediately                               │
│          │                                                   │
│    MISS? │ Load from DB                                     │
│          ▼                                                   │
│     ┌─────────┐                                             │
│     │Database │                                             │
│     └────┬────┘                                             │
│          │                                                   │
│          ▼                                                   │
│     Store in cache, return to app                           │
│                                                              │
│                                                              │
│  Similar to Cache-Aside, but cache is "smarter"            │
│  and handles loading logic internally.                      │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

---

## 10.4 Cache Invalidation

```
┌─────────────────────────────────────────────────────────────┐
│                                                              │
│  "There are only two hard things in Computer Science:       │
│   cache invalidation and naming things."                    │
│                                        - Phil Karlton       │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

### Invalidation Strategies

```
┌─────────────────────────────────────────────────────────────┐
│           CACHE INVALIDATION STRATEGIES                      │
│                                                              │
│  1. TIME-BASED (TTL)                                        │
│  ────────────────────                                       │
│  Set expiration time. Simple but may serve stale data.      │
│                                                              │
│  cache.setex("user:123", 3600, data)  # Expires in 1 hour  │
│                                                              │
│  Pros: Simple, automatic cleanup                            │
│  Cons: Data stale until expiration                          │
│                                                              │
│                                                              │
│  2. EVENT-BASED                                             │
│  ───────────────                                            │
│  Invalidate when data changes. Requires events.             │
│                                                              │
│  def update_user(user_id, data):                            │
│      db.update(user_id, data)                               │
│      cache.delete(f"user:{user_id}")                        │
│      publish_event("user_updated", user_id)                 │
│                                                              │
│  Pros: Always fresh data                                    │
│  Cons: Complex, need event system                           │
│                                                              │
│                                                              │
│  3. VERSION-BASED                                           │
│  ─────────────────                                          │
│  Include version in key. Change version to invalidate all.  │
│                                                              │
│  key = f"user:{user_id}:v{version}"                        │
│  # Bump version to invalidate all user caches              │
│                                                              │
│  Pros: Bulk invalidation easy                               │
│  Cons: Old versions linger until TTL                        │
│                                                              │
│                                                              │
│  4. HYBRID (Recommended)                                    │
│  ────────────────────────                                   │
│  Combine TTL with event-based invalidation.                 │
│                                                              │
│  - Set reasonable TTL (e.g., 1 hour)                       │
│  - Also invalidate on updates                               │
│  - TTL acts as safety net                                   │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

```python
# cache_invalidation.py
import redis.asyncio as redis
import json
from typing import Set, Optional
import asyncio

class SmartCache:
    """
    Cache with multiple invalidation strategies.
    """
    
    def __init__(self, redis_client: redis.Redis):
        self.redis = redis_client
        self.pubsub = redis_client.pubsub()
        
        # Track dependencies: which cache keys depend on which entities
        # e.g., "user_list" depends on all individual users
        self.dependencies: dict[str, Set[str]] = {}
    
    async def get(self, key: str) -> Optional[dict]:
        data = await self.redis.get(key)
        return json.loads(data) if data else None
    
    async def set(
        self, 
        key: str, 
        value: dict, 
        ttl: int = 3600,
        depends_on: list[str] = None
    ):
        """Set with TTL and optional dependencies"""
        await self.redis.setex(key, ttl, json.dumps(value, default=str))
        
        # Track dependencies
        if depends_on:
            for dep in depends_on:
                if dep not in self.dependencies:
                    self.dependencies[dep] = set()
                self.dependencies[dep].add(key)
                
                # Also store in Redis for distributed invalidation
                await self.redis.sadd(f"deps:{dep}", key)
    
    async def invalidate(self, key: str):
        """Invalidate a single key"""
        await self.redis.delete(key)
        print(f"Invalidated: {key}")
    
    async def invalidate_entity(self, entity_type: str, entity_id: str):
        """
        Invalidate an entity and all dependent cache entries.
        
        Example: invalidate_entity("user", "123")
        - Deletes "user:123"
        - Also deletes any keys that depend on this user
        """
        entity_key = f"{entity_type}:{entity_id}"
        
        # Delete the entity itself
        await self.redis.delete(entity_key)
        
        # Find and delete dependent keys
        dep_key = f"deps:{entity_key}"
        dependent_keys = await self.redis.smembers(dep_key)
        
        if dependent_keys:
            await self.redis.delete(*dependent_keys)
            print(f"Invalidated {len(dependent_keys)} dependent keys")
        
        # Clean up dependency tracking
        await self.redis.delete(dep_key)
        
        # Publish invalidation event (for distributed systems)
        await self.redis.publish(
            "cache_invalidation",
            json.dumps({"entity": entity_key})
        )
    
    async def invalidate_pattern(self, pattern: str):
        """
        Invalidate all keys matching pattern.
        Use carefully - SCAN can be slow!
        """
        cursor = 0
        deleted = 0
        
        while True:
            cursor, keys = await self.redis.scan(
                cursor, 
                match=pattern, 
                count=100
            )
            
            if keys:
                await self.redis.delete(*keys)
                deleted += len(keys)
            
            if cursor == 0:
                break
        
        print(f"Invalidated {deleted} keys matching '{pattern}'")
        return deleted

# ============== EVENT-BASED INVALIDATION ==============

class CacheInvalidator:
    """
    Listens for data change events and invalidates cache.
    """
    
    def __init__(self, cache: SmartCache):
        self.cache = cache
        self.handlers = {
            "user_updated": self._handle_user_updated,
            "user_deleted": self._handle_user_deleted,
            "product_updated": self._handle_product_updated,
            "order_created": self._handle_order_created,
        }
    
    async def handle_event(self, event_type: str, payload: dict):
        """Process a data change event"""
        handler = self.handlers.get(event_type)
        if handler:
            await handler(payload)
        else:
            print(f"Unknown event type: {event_type}")
    
    async def _handle_user_updated(self, payload: dict):
        user_id = payload["user_id"]
        
        # Invalidate user cache
        await self.cache.invalidate_entity("user", user_id)
        
        # Also invalidate related caches
        await self.cache.invalidate(f"user_profile:{user_id}")
        await self.cache.invalidate("user_list")
    
    async def _handle_user_deleted(self, payload: dict):
        user_id = payload["user_id"]
        await self.cache.invalidate_entity("user", user_id)
        await self.cache.invalidate("user_list")
        await self.cache.invalidate("user_count")
    
    async def _handle_product_updated(self, payload: dict):
        product_id = payload["product_id"]
        await self.cache.invalidate_entity("product", product_id)
        
        # Invalidate category listings that might contain this product
        if "category_id" in payload:
            await self.cache.invalidate(
                f"category_products:{payload['category_id']}"
            )
    
    async def _handle_order_created(self, payload: dict):
        user_id = payload["user_id"]
        # Invalidate user's order history cache
        await self.cache.invalidate(f"user_orders:{user_id}")
        await self.cache.invalidate(f"user_stats:{user_id}")
```

---

## 10.5 Cache Stampede Prevention

```
┌─────────────────────────────────────────────────────────────┐
│              CACHE STAMPEDE (Thundering Herd)                │
│                                                              │
│  Problem:                                                   │
│  ─────────                                                  │
│  1. Popular cache key expires                               │
│  2. 1000 requests arrive simultaneously                     │
│  3. ALL 1000 hit database (cache miss)                     │
│  4. Database overwhelmed!                                   │
│                                                              │
│     Time T: Cache expires                                   │
│         │                                                    │
│         ▼                                                    │
│    ┌─────────┐   1000 requests                              │
│    │  Cache  │ ◀═══════════════════                         │
│    │ (empty) │   ALL MISS!                                  │
│    └────┬────┘                                              │
│         │                                                    │
│         ▼ 1000 DB queries!                                  │
│    ┌─────────┐                                              │
│    │Database │ ← OVERLOADED!                                │
│    └─────────┘                                              │
│                                                              │
│                                                              │
│  Solutions:                                                 │
│  ──────────                                                 │
│                                                              │
│  1. LOCKING                                                 │
│     Only one request fetches; others wait                   │
│                                                              │
│  2. PROBABILISTIC EARLY EXPIRATION                          │
│     Refresh before actual expiration                        │
│                                                              │
│  3. BACKGROUND REFRESH                                      │
│     Never expire; refresh in background                     │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

```python
# stampede_prevention.py
import redis.asyncio as redis
import json
import asyncio
import random
import time
from typing import Any, Callable, Optional

class StampedeProtectedCache:
    """
    Cache with stampede prevention mechanisms.
    """
    
    def __init__(self, redis_client: redis.Redis):
        self.redis = redis_client
    
    # ============== SOLUTION 1: LOCKING ==============
    
    async def get_with_lock(
        self,
        key: str,
        fetch_func: Callable,
        ttl: int = 3600,
        lock_timeout: int = 5
    ) -> Any:
        """
        Get from cache with lock to prevent stampede.
        Only one caller fetches on cache miss.
        """
        # Try cache first
        data = await self.redis.get(key)
        if data:
            return json.loads(data)
        
        # Cache miss - try to acquire lock
        lock_key = f"lock:{key}"
        lock_acquired = await self.redis.set(
            lock_key, 
            "1", 
            nx=True,  # Only if not exists
            ex=lock_timeout
        )
        
        if lock_acquired:
            try:
                # We got the lock - fetch data
                print(f"Lock acquired, fetching {key}")
                result = await fetch_func()
                
                # Store in cache
                if result is not None:
                    await self.redis.setex(
                        key, ttl, json.dumps(result, default=str)
                    )
                
                return result
            finally:
                # Release lock
                await self.redis.delete(lock_key)
        else:
            # Someone else is fetching - wait for cache
            print(f"Lock not acquired, waiting for {key}")
            for _ in range(lock_timeout * 10):  # Poll every 100ms
                await asyncio.sleep(0.1)
                data = await self.redis.get(key)
                if data:
                    return json.loads(data)
            
            # Timeout - fetch ourselves (fallback)
            return await fetch_func()
    
    # ============== SOLUTION 2: PROBABILISTIC EARLY EXPIRATION ==============
    
    async def get_with_early_expiration(
        self,
        key: str,
        fetch_func: Callable,
        ttl: int = 3600,
        beta: float = 1.0  # Higher = earlier refresh
    ) -> Any:
        """
        Probabilistically refresh before expiration.
        Based on XFetch algorithm.
        """
        data = await self.redis.get(key)
        
        if data:
            cached = json.loads(data)
            
            # Check if we should early-refresh
            expiry = await self.redis.ttl(key)
            
            if expiry > 0:
                # Calculate probability of refresh
                # Higher probability as we approach expiration
                delta = ttl - expiry  # Time since cached
                
                # XFetch formula
                random_val = random.random()
                threshold = delta * beta * random.random() / ttl
                
                if random_val < threshold:
                    # Probabilistically refresh in background
                    print(f"Early refresh triggered for {key}")
                    asyncio.create_task(
                        self._background_refresh(key, fetch_func, ttl)
                    )
            
            return cached['value']
        
        # Cache miss - fetch and store
        result = await fetch_func()
        if result is not None:
            await self.redis.setex(
                key,
                ttl,
                json.dumps({'value': result, 'fetched_at': time.time()}, default=str)
            )
        return result
    
    async def _background_refresh(
        self, 
        key: str, 
        fetch_func: Callable,
        ttl: int
    ):
        """Refresh cache in background"""
        try:
            result = await fetch_func()
            if result is not None:
                await self.redis.setex(
                    key,
                    ttl,
                    json.dumps({'value': result, 'fetched_at': time.time()}, default=str)
                )
        except Exception as e:
            print(f"Background refresh failed for {key}: {e}")
    
    # ============== SOLUTION 3: NEVER-EXPIRE WITH BACKGROUND REFRESH ==============
    
    async def get_with_background_refresh(
        self,
        key: str,
        fetch_func: Callable,
        soft_ttl: int = 300,  # Refresh after this
        hard_ttl: int = 3600  # Actually expire after this
    ) -> Any:
        """
        Cache never expires for readers.
        Background job refreshes periodically.
        """
        data = await self.redis.get(key)
        
        if data:
            cached = json.loads(data)
            age = time.time() - cached['fetched_at']
            
            if age > soft_ttl:
                # Data is stale - trigger background refresh
                # But still return stale data immediately!
                asyncio.create_task(
                    self._refresh_if_not_refreshing(key, fetch_func, hard_ttl)
                )
            
            return cached['value']
        
        # Cold start - must fetch synchronously
        result = await fetch_func()
        await self._store(key, result, hard_ttl)
        return result
    
    async def _refresh_if_not_refreshing(
        self,
        key: str,
        fetch_func: Callable,
        ttl: int
    ):
        """Refresh only if not already refreshing"""
        refresh_lock = f"refreshing:{key}"
        
        if await self.redis.set(refresh_lock, "1", nx=True, ex=30):
            try:
                result = await fetch_func()
                await self._store(key, result, ttl)
            finally:
                await self.redis.delete(refresh_lock)
    
    async def _store(self, key: str, value: Any, ttl: int):
        await self.redis.setex(
            key,
            ttl,
            json.dumps({'value': value, 'fetched_at': time.time()}, default=str)
        )

# ============== DEMO ==============

async def demo_stampede_prevention():
    redis_client = redis.Redis()
    cache = StampedeProtectedCache(redis_client)
    
    async def expensive_fetch():
        """Simulates expensive database query"""
        print("  → Fetching from database...")
        await asyncio.sleep(1)  # 1 second query
        return {"data": "expensive result", "timestamp": time.time()}
    
    # Simulate 10 concurrent requests
    print("Simulating stampede with locking:")
    
    async def make_request(i: int):
        result = await cache.get_with_lock(
            "popular_data",
            expensive_fetch,
            ttl=60
        )
        print(f"  Request {i} got result")
        return result
    
    # Clear cache to simulate expiration
    await redis_client.delete("popular_data")
    
    # All requests hit simultaneously
    results = await asyncio.gather(*[make_request(i) for i in range(10)])
    
    # Only ONE "Fetching from database" should print!
    print(f"All {len(results)} requests completed")

# asyncio.run(demo_stampede_prevention())
```

---

## 10.6 Distributed Caching with Redis

```python
# distributed_cache.py
import redis.asyncio as redis
from redis.asyncio.cluster import RedisCluster
import json
from typing import Any, List, Optional
import hashlib

class DistributedCache:
    """
    Production-ready distributed cache with Redis Cluster.
    """
    
    def __init__(self, nodes: List[dict] = None, single_node: str = None):
        if nodes:
            # Redis Cluster mode
            self.client = RedisCluster(
                startup_nodes=nodes,
                decode_responses=True
            )
            self.is_cluster = True
        else:
            # Single node mode
            self.client = redis.from_url(
                single_node or "redis://localhost:6379",
                decode_responses=True
            )
            self.is_cluster = False
    
    def _serialize(self, value: Any) -> str:
        return json.dumps(value, default=str)
    
    def _deserialize(self, data: str) -> Any:
        return json.loads(data) if data else None
    
    async def get(self, key: str) -> Optional[Any]:
        data = await self.client.get(key)
        return self._deserialize(data)
    
    async def set(
        self, 
        key: str, 
        value: Any, 
        ttl: int = 3600
    ) -> bool:
        return await self.client.setex(key, ttl, self._serialize(value))
    
    async def delete(self, key: str) -> int:
        return await self.client.delete(key)
    
    async def mget(self, keys: List[str]) -> List[Any]:
        """Get multiple keys at once (pipelining)"""
        if not keys:
            return []
        
        values = await self.client.mget(keys)
        return [self._deserialize(v) for v in values]
    
    async def mset(
        self, 
        mapping: dict[str, Any], 
        ttl: int = 3600
    ):
        """Set multiple keys at once"""
        pipe = self.client.pipeline()
        
        for key, value in mapping.items():
            pipe.setex(key, ttl, self._serialize(value))
        
        await pipe.execute()
    
    # ============== HASH OPERATIONS (for structured data) ==============
    
    async def hget(self, key: str, field: str) -> Optional[Any]:
        """Get field from hash"""
        data = await self.client.hget(key, field)
        return self._deserialize(data)
    
    async def hset(self, key: str, field: str, value: Any):
        """Set field in hash"""
        await self.client.hset(key, field, self._serialize(value))
    
    async def hgetall(self, key: str) -> dict:
        """Get all fields from hash"""
        data = await self.client.hgetall(key)
        return {k: self._deserialize(v) for k, v in data.items()}
    
    async def hmset(self, key: str, mapping: dict):
        """Set multiple fields in hash"""
        serialized = {k: self._serialize(v) for k, v in mapping.items()}
        await self.client.hset(key, mapping=serialized)
    
    # ============== SORTED SET (for leaderboards, rankings) ==============
    
    async def zadd(self, key: str, score: float, member: str):
        """Add to sorted set"""
        await self.client.zadd(key, {member: score})
    
    async def zrange(
        self, 
        key: str, 
        start: int = 0, 
        end: int = -1,
        with_scores: bool = False
    ) -> List:
        """Get range from sorted set (lowest to highest)"""
        return await self.client.zrange(
            key, start, end, withscores=with_scores
        )
    
    async def zrevrange(
        self, 
        key: str, 
        start: int = 0, 
        end: int = -1,
        with_scores: bool = False
    ) -> List:
        """Get range from sorted set (highest to lowest)"""
        return await self.client.zrevrange(
            key, start, end, withscores=with_scores
        )
    
    # ============== ATOMIC OPERATIONS ==============
    
    async def incr(self, key: str, amount: int = 1) -> int:
        """Atomic increment"""
        return await self.client.incrby(key, amount)
    
    async def decr(self, key: str, amount: int = 1) -> int:
        """Atomic decrement"""
        return await self.client.decrby(key, amount)

# ============== TWO-TIER CACHING ==============

class TwoTierCache:
    """
    Local (L1) + Distributed (L2) cache.
    Reduces network calls for hot data.
    """
    
    def __init__(self, distributed_cache: DistributedCache):
        self.l1_cache: dict = {}  # Local memory
        self.l1_max_size = 1000
        self.l2_cache = distributed_cache
    
    async def get(self, key: str) -> Optional[Any]:
        # L1: Check local cache first
        if key in self.l1_cache:
            print(f"L1 HIT: {key}")
            return self.l1_cache[key]
        
        # L2: Check distributed cache
        value = await self.l2_cache.get(key)
        if value is not None:
            print(f"L2 HIT: {key}")
            self._l1_set(key, value)
            return value
        
        print(f"MISS: {key}")
        return None
    
    async def set(self, key: str, value: Any, ttl: int = 3600):
        # Set in both layers
        self._l1_set(key, value)
        await self.l2_cache.set(key, value, ttl)
    
    def _l1_set(self, key: str, value: Any):
        """Set in L1 with simple eviction"""
        if len(self.l1_cache) >= self.l1_max_size:
            # Simple eviction: remove random item
            # In production: use LRU
            self.l1_cache.pop(next(iter(self.l1_cache)))
        
        self.l1_cache[key] = value
    
    async def invalidate(self, key: str):
        """Invalidate from both layers"""
        self.l1_cache.pop(key, None)
        await self.l2_cache.delete(key)
```

---

## 10.7 Cache Design Best Practices

```
┌─────────────────────────────────────────────────────────────┐
│           CACHE DESIGN BEST PRACTICES                        │
│                                                              │
│  1. KEY DESIGN                                              │
│  ─────────────                                              │
│  • Use namespaces: "user:123", "product:456"               │
│  • Include version: "user:123:v2"                          │
│  • Keep keys short but readable                             │
│  • Use consistent naming conventions                        │
│                                                              │
│  Good: "user:123:profile"                                   │
│  Bad:  "u123p" (unreadable)                                │
│  Bad:  "user_profile_for_user_id_123" (too long)           │
│                                                              │
│                                                              │
│  2. TTL STRATEGY                                            │
│  ────────────────                                           │
│  • Different TTLs for different data types                  │
│  • Shorter for frequently changing data                     │
│  • Add jitter to prevent synchronized expiration            │
│                                                              │
│  ttl = base_ttl + random.randint(0, jitter)                │
│                                                              │
│                                                              │
│  3. SERIALIZATION                                           │
│  ─────────────────                                          │
│  • JSON for readability and debugging                       │
│  • MessagePack for performance                              │
│  • Compress large values                                    │
│                                                              │
│                                                              │
│  4. ERROR HANDLING                                          │
│  ──────────────────                                         │
│  • Cache failures should not break the app                  │
│  • Fall back to database on cache errors                    │
│  • Log cache errors for monitoring                          │
│                                                              │
│                                                              │
│  5. MONITORING                                              │
│  ─────────────                                              │
│  • Track hit rate (target >95%)                             │
│  • Monitor memory usage                                     │
│  • Alert on high miss rates                                 │
│  • Track latency percentiles                                │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

---

## Summary

| Pattern | Use When | Pros | Cons |
|---------|----------|------|------|
| **Cache-Aside** | General purpose | Flexible, simple | App manages cache |
| **Write-Through** | Read-heavy, need consistency | Always fresh | Slower writes |
| **Write-Behind** | Write-heavy, speed critical | Fast writes | Data loss risk |
| **Read-Through** | Simple read caching | Clean app code | Less control |

### Key Takeaways

1. **Cache everything that's expensive** - DB queries, API calls, computations
2. **Use appropriate TTLs** - Balance freshness vs hit rate
3. **Invalidate proactively** - Don't rely only on TTL
4. **Prevent stampedes** - Use locking or probabilistic refresh
5. **Monitor hit rates** - Target >95%, investigate lower

---

## Practice Exercises

1. Implement cache-aside for a user service
2. Build a write-behind cache with batching
3. Create stampede protection with locking
4. Set up two-tier caching (local + Redis)
5. Implement cache invalidation with events

**Next Chapter**: Time and ordering in distributed systems!
