# Chapter 16: Scalability Patterns

> **Goal**: Master the patterns that allow systems to handle massive scale

---

## 16.1 Horizontal vs Vertical Scaling

### The Fundamental Choice

```
┌─────────────────────────────────────────────────────────────┐
│           SCALING: VERTICAL vs HORIZONTAL                    │
│                                                              │
│  VERTICAL SCALING (Scale Up)                                │
│  ───────────────────────────                                │
│  Add more power to existing machine                         │
│                                                              │
│  Before:          After:                                    │
│  ┌─────────┐      ┌─────────────┐                          │
│  │ 4 CPU   │      │ 32 CPU      │                          │
│  │ 16GB RAM│  →   │ 256GB RAM   │                          │
│  │ 500GB   │      │ 4TB SSD     │                          │
│  └─────────┘      └─────────────┘                          │
│                                                              │
│  Pros:                      Cons:                           │
│  ├── Simple                 ├── Hardware limits             │
│  ├── No code changes        ├── Expensive at high end       │
│  ├── ACID preserved         ├── Single point of failure     │
│  └── Low latency            └── Downtime during upgrade     │
│                                                              │
│                                                              │
│  HORIZONTAL SCALING (Scale Out)                             │
│  ─────────────────────────────                              │
│  Add more machines                                          │
│                                                              │
│  Before:          After:                                    │
│  ┌─────────┐      ┌─────────┐ ┌─────────┐ ┌─────────┐     │
│  │ Server  │      │ Server  │ │ Server  │ │ Server  │     │
│  │    1    │  →   │    1    │ │    2    │ │    3    │     │
│  └─────────┘      └─────────┘ └─────────┘ └─────────┘     │
│                                                              │
│  Pros:                      Cons:                           │
│  ├── Near infinite scale    ├── Complex architecture        │
│  ├── Fault tolerant         ├── Data consistency harder     │
│  ├── Cost effective         ├── Network overhead            │
│  └── No single point fail   └── Requires stateless design   │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

### Cost Comparison

```
┌─────────────────────────────────────────────────────────────┐
│              COST SCALING COMPARISON                         │
│                                                              │
│  Cost ($)                                                   │
│    │                                                        │
│    │                              ╱ Vertical                │
│    │                           ╱                            │
│    │                        ╱                               │
│    │                     ╱                                  │
│    │                  ╱                                     │
│    │               ╱                                        │
│    │            ╱   ─────────────── Horizontal              │
│    │         ╱  ────                                        │
│    │      ╱────                                             │
│    │   ╱──                                                  │
│    │╱──                                                     │
│    └────────────────────────────────────────▶ Capacity      │
│                                                              │
│  Vertical: Exponential cost growth                          │
│  Horizontal: Linear cost growth (after initial complexity)  │
│                                                              │
│  Example AWS Pricing (approximate):                         │
│  ├── 1x m5.xlarge (4 CPU, 16GB):   $140/month              │
│  ├── 1x m5.4xlarge (16 CPU, 64GB): $560/month (4x price)   │
│  ├── 4x m5.xlarge (16 CPU, 64GB):  $560/month (same!)      │
│  ├── 1x m5.16xlarge (64 CPU):      $2,240/month            │
│  └── 16x m5.xlarge (64 CPU):       $2,240/month + HA!      │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

### Designing for Horizontal Scale

```python
# horizontal_scaling.py
"""
Key principles for horizontal scalability
"""

from fastapi import FastAPI, Request, Depends
from typing import Optional
import redis.asyncio as redis
import httpx
import os

app = FastAPI()

# ============== PRINCIPLE 1: STATELESS SERVICES ==============

# BAD: State stored in memory (can't scale horizontally)
class BadUserSession:
    sessions = {}  # Dies when this instance dies!
    
    def create(self, user_id: str, data: dict):
        self.sessions[user_id] = data
    
    def get(self, user_id: str):
        return self.sessions.get(user_id)  # Only works on THIS server!

# GOOD: State stored externally (Redis)
class GoodUserSession:
    def __init__(self, redis_client: redis.Redis):
        self.redis = redis_client
    
    async def create(self, user_id: str, data: dict, ttl: int = 3600):
        import json
        await self.redis.setex(
            f"session:{user_id}",
            ttl,
            json.dumps(data)
        )
    
    async def get(self, user_id: str) -> Optional[dict]:
        import json
        data = await self.redis.get(f"session:{user_id}")
        return json.loads(data) if data else None

# ============== PRINCIPLE 2: SHARE NOTHING ==============

# Each instance should be completely independent
# No shared local storage, no shared memory

# Configuration from environment (same for all instances)
DATABASE_URL = os.getenv("DATABASE_URL")
REDIS_URL = os.getenv("REDIS_URL")
SERVICE_NAME = os.getenv("SERVICE_NAME", "api")

# ============== PRINCIPLE 3: DESIGN FOR FAILURE ==============

async def call_user_service_with_retry(user_id: int, max_retries: int = 3):
    """Any instance might fail - always handle it"""
    
    for attempt in range(max_retries):
        try:
            async with httpx.AsyncClient(timeout=5.0) as client:
                response = await client.get(
                    f"http://user-service/users/{user_id}"
                )
                return response.json()
        except (httpx.TimeoutException, httpx.ConnectError) as e:
            if attempt == max_retries - 1:
                raise
            await asyncio.sleep(0.5 * (attempt + 1))

# ============== PRINCIPLE 4: USE LOAD BALANCER ==============

"""
Load balancer distributes traffic across instances:

                    ┌─────────────────┐
    Requests ──────▶│  Load Balancer  │
                    └────────┬────────┘
                             │
              ┌──────────────┼──────────────┐
              ▼              ▼              ▼
        ┌──────────┐  ┌──────────┐  ┌──────────┐
        │Instance 1│  │Instance 2│  │Instance 3│
        └──────────┘  └──────────┘  └──────────┘

All instances are identical and interchangeable!
"""

# ============== PRINCIPLE 5: IDEMPOTENT OPERATIONS ==============

import uuid

@app.post("/orders")
async def create_order(
    request: Request,
    redis_client: redis.Redis = Depends(get_redis)
):
    """
    Idempotent order creation - safe to retry.
    Client provides idempotency key.
    """
    idempotency_key = request.headers.get("Idempotency-Key")
    if not idempotency_key:
        idempotency_key = str(uuid.uuid4())
    
    # Check if already processed
    cache_key = f"order:idempotency:{idempotency_key}"
    existing = await redis_client.get(cache_key)
    if existing:
        return json.loads(existing)  # Return same response
    
    # Process order
    order = await process_order(await request.json())
    
    # Cache response
    await redis_client.setex(cache_key, 86400, json.dumps(order))
    
    return order
```

---

## 16.2 Database Scaling Strategies

### Read Scaling with Replicas

```
┌─────────────────────────────────────────────────────────────┐
│              READ SCALING WITH REPLICAS                      │
│                                                              │
│  Problem: Database is bottleneck for read-heavy workload    │
│                                                              │
│  Solution: Add read replicas                                │
│                                                              │
│                    ┌───────────────┐                        │
│     Writes ───────▶│    PRIMARY    │                        │
│                    │   (Leader)    │                        │
│                    └───────┬───────┘                        │
│                            │                                 │
│               Replication  │  (async or sync)               │
│                            │                                 │
│            ┌───────────────┼───────────────┐                │
│            ▼               ▼               ▼                 │
│     ┌───────────┐   ┌───────────┐   ┌───────────┐          │
│     │  REPLICA  │   │  REPLICA  │   │  REPLICA  │          │
│     │     1     │   │     2     │   │     3     │          │
│     └───────────┘   └───────────┘   └───────────┘          │
│            ▲               ▲               ▲                 │
│            └───────────────┴───────────────┘                │
│                            │                                 │
│     Reads ◀────────────────┘                                │
│                                                              │
│  Read capacity: 4x (1 primary + 3 replicas)                 │
│  Write capacity: 1x (still limited to primary)              │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

```python
# read_write_splitting.py
import asyncpg
from typing import Optional, Any
import random

class ReadWritePool:
    """
    Database connection pool with read/write splitting.
    Writes go to primary, reads distributed across replicas.
    """
    
    def __init__(
        self,
        primary_url: str,
        replica_urls: list[str],
        min_connections: int = 5,
        max_connections: int = 20
    ):
        self.primary_url = primary_url
        self.replica_urls = replica_urls
        self.primary_pool: Optional[asyncpg.Pool] = None
        self.replica_pools: list[asyncpg.Pool] = []
    
    async def connect(self):
        """Initialize connection pools"""
        # Primary pool for writes
        self.primary_pool = await asyncpg.create_pool(
            self.primary_url,
            min_size=5,
            max_size=20
        )
        
        # Replica pools for reads
        for url in self.replica_urls:
            pool = await asyncpg.create_pool(
                url,
                min_size=5,
                max_size=20
            )
            self.replica_pools.append(pool)
    
    async def close(self):
        await self.primary_pool.close()
        for pool in self.replica_pools:
            await pool.close()
    
    def _get_read_pool(self) -> asyncpg.Pool:
        """Get a replica pool for reading (round-robin or random)"""
        if self.replica_pools:
            return random.choice(self.replica_pools)
        return self.primary_pool  # Fallback to primary
    
    async def execute_write(self, query: str, *args) -> str:
        """Execute write query on primary"""
        async with self.primary_pool.acquire() as conn:
            return await conn.execute(query, *args)
    
    async def fetch_read(
        self, 
        query: str, 
        *args,
        use_primary: bool = False
    ) -> list:
        """
        Execute read query on replica.
        use_primary=True for read-your-writes consistency.
        """
        pool = self.primary_pool if use_primary else self._get_read_pool()
        async with pool.acquire() as conn:
            return await conn.fetch(query, *args)
    
    async def fetchrow_read(
        self, 
        query: str, 
        *args,
        use_primary: bool = False
    ) -> Optional[asyncpg.Record]:
        """Fetch single row from replica"""
        pool = self.primary_pool if use_primary else self._get_read_pool()
        async with pool.acquire() as conn:
            return await conn.fetchrow(query, *args)

# Usage example
class UserRepository:
    def __init__(self, db: ReadWritePool):
        self.db = db
    
    async def create_user(self, username: str, email: str) -> int:
        """Write operation - goes to primary"""
        result = await self.db.fetchrow_read(
            """
            INSERT INTO users (username, email)
            VALUES ($1, $2)
            RETURNING id
            """,
            username, email,
            use_primary=True  # Write on primary, read result from primary
        )
        return result['id']
    
    async def get_user(self, user_id: int, ensure_fresh: bool = False):
        """Read operation - goes to replica by default"""
        return await self.db.fetchrow_read(
            "SELECT * FROM users WHERE id = $1",
            user_id,
            use_primary=ensure_fresh  # Use primary if need latest data
        )
    
    async def list_users(self, limit: int = 100):
        """List operation - always use replica"""
        return await self.db.fetch_read(
            "SELECT * FROM users LIMIT $1",
            limit
        )
```

### Write Scaling with Sharding

```
┌─────────────────────────────────────────────────────────────┐
│              WRITE SCALING WITH SHARDING                     │
│                                                              │
│  Problem: Single database can't handle write volume         │
│                                                              │
│  Solution: Partition data across multiple databases         │
│                                                              │
│                   ┌─────────────────┐                       │
│                   │  Shard Router   │                       │
│                   │ (determines     │                       │
│                   │  which shard)   │                       │
│                   └────────┬────────┘                       │
│                            │                                 │
│         ┌──────────────────┼──────────────────┐             │
│         │                  │                  │              │
│         ▼                  ▼                  ▼              │
│  ┌────────────┐    ┌────────────┐    ┌────────────┐        │
│  │  Shard 0   │    │  Shard 1   │    │  Shard 2   │        │
│  │ Users 0-33%│    │Users 34-66%│    │Users 67-100│        │
│  │            │    │            │    │            │        │
│  │  Primary   │    │  Primary   │    │  Primary   │        │
│  │  Replica   │    │  Replica   │    │  Replica   │        │
│  │  Replica   │    │  Replica   │    │  Replica   │        │
│  └────────────┘    └────────────┘    └────────────┘        │
│                                                              │
│  Each shard handles 1/3 of writes!                          │
│  Add more shards = more write capacity                      │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

```python
# database_sharding.py
import hashlib
from typing import Dict, Any, List, Optional
import asyncpg

class ShardedDatabase:
    """
    Sharded database with consistent hashing.
    """
    
    def __init__(self, shard_configs: List[dict]):
        """
        shard_configs: [
            {"id": 0, "url": "postgresql://shard0/db"},
            {"id": 1, "url": "postgresql://shard1/db"},
            {"id": 2, "url": "postgresql://shard2/db"},
        ]
        """
        self.shard_configs = shard_configs
        self.num_shards = len(shard_configs)
        self.pools: Dict[int, asyncpg.Pool] = {}
    
    async def connect(self):
        for config in self.shard_configs:
            self.pools[config["id"]] = await asyncpg.create_pool(
                config["url"],
                min_size=5,
                max_size=20
            )
    
    async def close(self):
        for pool in self.pools.values():
            await pool.close()
    
    def _get_shard_id(self, shard_key: str) -> int:
        """
        Determine shard based on key using consistent hashing.
        Same key always maps to same shard.
        """
        hash_value = int(hashlib.md5(str(shard_key).encode()).hexdigest(), 16)
        return hash_value % self.num_shards
    
    def _get_pool(self, shard_key: str) -> asyncpg.Pool:
        shard_id = self._get_shard_id(shard_key)
        return self.pools[shard_id]
    
    async def execute(self, shard_key: str, query: str, *args) -> str:
        """Execute query on appropriate shard"""
        pool = self._get_pool(shard_key)
        async with pool.acquire() as conn:
            return await conn.execute(query, *args)
    
    async def fetch(self, shard_key: str, query: str, *args) -> list:
        """Fetch from appropriate shard"""
        pool = self._get_pool(shard_key)
        async with pool.acquire() as conn:
            return await conn.fetch(query, *args)
    
    async def fetch_all_shards(self, query: str, *args) -> list:
        """
        Execute query on ALL shards and combine results.
        Used for cross-shard queries (expensive!).
        """
        results = []
        for pool in self.pools.values():
            async with pool.acquire() as conn:
                rows = await conn.fetch(query, *args)
                results.extend(rows)
        return results

# Sharded User Repository
class ShardedUserRepository:
    """
    User repository with sharding by user_id.
    """
    
    def __init__(self, db: ShardedDatabase):
        self.db = db
    
    async def create_user(self, user_id: str, data: dict) -> dict:
        """Create user on appropriate shard"""
        await self.db.execute(
            shard_key=user_id,  # Shard by user_id
            query="""
                INSERT INTO users (id, username, email, created_at)
                VALUES ($1, $2, $3, NOW())
            """,
            user_id, data['username'], data['email']
        )
        return {"id": user_id, **data}
    
    async def get_user(self, user_id: str) -> Optional[dict]:
        """Get user from appropriate shard"""
        rows = await self.db.fetch(
            shard_key=user_id,  # Same key = same shard
            query="SELECT * FROM users WHERE id = $1",
            user_id
        )
        return dict(rows[0]) if rows else None
    
    async def search_users(self, username_pattern: str) -> list:
        """
        Search across all shards (scatter-gather).
        This is expensive - avoid if possible!
        """
        return await self.db.fetch_all_shards(
            "SELECT * FROM users WHERE username LIKE $1 LIMIT 100",
            f"%{username_pattern}%"
        )

# ============== SHARD KEY SELECTION ==============

"""
GOOD shard keys:
├── User ID (for user-centric data)
├── Tenant ID (for multi-tenant apps)  
├── Order ID (for order data)
└── Region (for geo-distributed data)

BAD shard keys:
├── Timestamp (causes hot shards)
├── Sequential IDs (uneven distribution)
├── Foreign keys (causes cross-shard joins)
└── Frequently changing fields
"""
```

---

## 16.3 CQRS (Command Query Responsibility Segregation)

### The Pattern

```
┌─────────────────────────────────────────────────────────────┐
│                        CQRS                                  │
│                                                              │
│  Traditional:                                               │
│  ────────────                                               │
│  Same model for reads and writes                            │
│                                                              │
│    Reads ──┐                                                │
│            ├──▶ [Single Model] ──▶ [Single DB]             │
│   Writes ──┘                                                │
│                                                              │
│                                                              │
│  CQRS:                                                      │
│  ─────                                                      │
│  Separate models optimized for each                         │
│                                                              │
│   Commands ──▶ [Write Model] ──▶ [Write DB]                │
│                      │           (normalized)               │
│                      │ Events                               │
│                      ▼                                       │
│                 [Projector]                                 │
│                      │                                       │
│                      ▼                                       │
│    Queries ──▶ [Read Model] ◀── [Read DB]                  │
│                                 (denormalized)              │
│                                                              │
│                                                              │
│  Benefits:                                                  │
│  ├── Optimize read and write models independently           │
│  ├── Scale reads and writes separately                      │
│  ├── Different storage for different needs                  │
│  └── Simpler queries (pre-computed views)                   │
│                                                              │
│  Costs:                                                     │
│  ├── Eventual consistency between models                    │
│  ├── More infrastructure                                    │
│  └── More complex                                           │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

```python
# cqrs_pattern.py
from dataclasses import dataclass, field
from datetime import datetime
from typing import List, Optional, Dict, Any
from abc import ABC, abstractmethod
import asyncio
import json

# ============== DOMAIN EVENTS ==============

@dataclass
class DomainEvent:
    event_id: str
    event_type: str
    aggregate_id: str
    aggregate_type: str
    timestamp: datetime
    data: Dict[str, Any]

@dataclass
class OrderCreated(DomainEvent):
    pass

@dataclass  
class OrderItemAdded(DomainEvent):
    pass

@dataclass
class OrderShipped(DomainEvent):
    pass

# ============== WRITE SIDE (Commands) ==============

@dataclass
class CreateOrderCommand:
    order_id: str
    customer_id: str
    items: List[dict]

@dataclass
class AddItemCommand:
    order_id: str
    product_id: str
    quantity: int
    price: float

@dataclass
class ShipOrderCommand:
    order_id: str
    tracking_number: str

class OrderAggregate:
    """
    Write model - enforces business rules.
    Optimized for consistency, not queries.
    """
    
    def __init__(self, order_id: str):
        self.order_id = order_id
        self.customer_id: Optional[str] = None
        self.items: List[dict] = []
        self.status = "draft"
        self.total = 0.0
        self.events: List[DomainEvent] = []
    
    def create(self, customer_id: str, items: List[dict]):
        """Create order - validates business rules"""
        if self.status != "draft":
            raise ValueError("Order already created")
        
        if not items:
            raise ValueError("Order must have at least one item")
        
        self.customer_id = customer_id
        self.items = items
        self.status = "created"
        self.total = sum(item['price'] * item['quantity'] for item in items)
        
        # Emit event
        self.events.append(DomainEvent(
            event_id=f"evt-{self.order_id}-1",
            event_type="OrderCreated",
            aggregate_id=self.order_id,
            aggregate_type="Order",
            timestamp=datetime.utcnow(),
            data={
                "customer_id": customer_id,
                "items": items,
                "total": self.total
            }
        ))
    
    def add_item(self, product_id: str, quantity: int, price: float):
        """Add item - validates business rules"""
        if self.status not in ["created", "draft"]:
            raise ValueError("Cannot modify shipped order")
        
        item = {"product_id": product_id, "quantity": quantity, "price": price}
        self.items.append(item)
        self.total += price * quantity
        
        self.events.append(DomainEvent(
            event_id=f"evt-{self.order_id}-{len(self.events)+1}",
            event_type="OrderItemAdded",
            aggregate_id=self.order_id,
            aggregate_type="Order",
            timestamp=datetime.utcnow(),
            data={"item": item, "new_total": self.total}
        ))
    
    def ship(self, tracking_number: str):
        """Ship order - validates business rules"""
        if self.status != "created":
            raise ValueError(f"Cannot ship order in {self.status} status")
        
        if not self.items:
            raise ValueError("Cannot ship empty order")
        
        self.status = "shipped"
        
        self.events.append(DomainEvent(
            event_id=f"evt-{self.order_id}-{len(self.events)+1}",
            event_type="OrderShipped",
            aggregate_id=self.order_id,
            aggregate_type="Order", 
            timestamp=datetime.utcnow(),
            data={"tracking_number": tracking_number}
        ))

class CommandHandler:
    """Handles commands and produces events"""
    
    def __init__(self, event_store, event_publisher):
        self.event_store = event_store
        self.event_publisher = event_publisher
        self.aggregates: Dict[str, OrderAggregate] = {}
    
    async def handle_create_order(self, cmd: CreateOrderCommand):
        """Handle create order command"""
        aggregate = OrderAggregate(cmd.order_id)
        aggregate.create(cmd.customer_id, cmd.items)
        
        # Save events
        await self.event_store.save(aggregate.events)
        
        # Publish events for read side
        for event in aggregate.events:
            await self.event_publisher.publish(event)
        
        self.aggregates[cmd.order_id] = aggregate
        return aggregate.order_id
    
    async def handle_ship_order(self, cmd: ShipOrderCommand):
        """Handle ship order command"""
        aggregate = self.aggregates.get(cmd.order_id)
        if not aggregate:
            raise ValueError(f"Order {cmd.order_id} not found")
        
        aggregate.ship(cmd.tracking_number)
        
        await self.event_store.save(aggregate.events[-1:])  # Save new events
        
        for event in aggregate.events[-1:]:
            await self.event_publisher.publish(event)

# ============== READ SIDE (Queries) ==============

@dataclass
class OrderReadModel:
    """
    Read model - optimized for queries.
    Denormalized, pre-computed.
    """
    order_id: str
    customer_id: str
    customer_name: str  # Denormalized!
    items: List[dict]
    item_count: int  # Pre-computed!
    total: float
    status: str
    tracking_number: Optional[str]
    created_at: datetime
    updated_at: datetime

class OrderProjection:
    """
    Builds and maintains read models from events.
    """
    
    def __init__(self):
        self.orders: Dict[str, OrderReadModel] = {}
        self.customer_cache: Dict[str, str] = {}  # customer_id -> name
    
    async def handle_event(self, event: DomainEvent):
        """Update read model based on event"""
        
        if event.event_type == "OrderCreated":
            await self._handle_order_created(event)
        elif event.event_type == "OrderItemAdded":
            await self._handle_item_added(event)
        elif event.event_type == "OrderShipped":
            await self._handle_order_shipped(event)
    
    async def _handle_order_created(self, event: DomainEvent):
        """Build read model for new order"""
        data = event.data
        
        # Get customer name (would query customer service in real app)
        customer_name = self.customer_cache.get(
            data['customer_id'], 
            f"Customer {data['customer_id']}"
        )
        
        self.orders[event.aggregate_id] = OrderReadModel(
            order_id=event.aggregate_id,
            customer_id=data['customer_id'],
            customer_name=customer_name,
            items=data['items'],
            item_count=len(data['items']),
            total=data['total'],
            status="created",
            tracking_number=None,
            created_at=event.timestamp,
            updated_at=event.timestamp
        )
    
    async def _handle_item_added(self, event: DomainEvent):
        """Update read model when item added"""
        order = self.orders.get(event.aggregate_id)
        if order:
            order.items.append(event.data['item'])
            order.item_count = len(order.items)
            order.total = event.data['new_total']
            order.updated_at = event.timestamp
    
    async def _handle_order_shipped(self, event: DomainEvent):
        """Update read model when shipped"""
        order = self.orders.get(event.aggregate_id)
        if order:
            order.status = "shipped"
            order.tracking_number = event.data['tracking_number']
            order.updated_at = event.timestamp

class QueryHandler:
    """Handles read queries against read models"""
    
    def __init__(self, projection: OrderProjection):
        self.projection = projection
    
    async def get_order(self, order_id: str) -> Optional[OrderReadModel]:
        """Fast lookup - no joins needed!"""
        return self.projection.orders.get(order_id)
    
    async def get_customer_orders(self, customer_id: str) -> List[OrderReadModel]:
        """Pre-indexed by customer"""
        return [
            order for order in self.projection.orders.values()
            if order.customer_id == customer_id
        ]
    
    async def get_orders_by_status(self, status: str) -> List[OrderReadModel]:
        """Filter by status"""
        return [
            order for order in self.projection.orders.values()
            if order.status == status
        ]
    
    async def get_order_summary(self, order_id: str) -> dict:
        """Pre-computed summary - no aggregation needed"""
        order = self.projection.orders.get(order_id)
        if not order:
            return None
        
        return {
            "order_id": order.order_id,
            "customer": order.customer_name,
            "item_count": order.item_count,  # Pre-computed!
            "total": order.total,
            "status": order.status
        }

# ============== DEMO ==============

async def demo_cqrs():
    # Setup
    event_store = InMemoryEventStore()
    event_publisher = InMemoryEventPublisher()
    projection = OrderProjection()
    
    # Subscribe projection to events
    event_publisher.subscribe(projection.handle_event)
    
    command_handler = CommandHandler(event_store, event_publisher)
    query_handler = QueryHandler(projection)
    
    # Command: Create order
    print("=== Creating Order ===")
    order_id = await command_handler.handle_create_order(CreateOrderCommand(
        order_id="ORD-001",
        customer_id="CUST-123",
        items=[
            {"product_id": "PROD-A", "quantity": 2, "price": 29.99},
            {"product_id": "PROD-B", "quantity": 1, "price": 49.99}
        ]
    ))
    
    # Query: Get order (from read model)
    print("\n=== Querying Order ===")
    order = await query_handler.get_order(order_id)
    print(f"Order: {order}")
    
    summary = await query_handler.get_order_summary(order_id)
    print(f"Summary: {summary}")
    
    # Command: Ship order
    print("\n=== Shipping Order ===")
    await command_handler.handle_ship_order(ShipOrderCommand(
        order_id="ORD-001",
        tracking_number="TRACK-12345"
    ))
    
    # Query again
    order = await query_handler.get_order(order_id)
    print(f"After shipping: status={order.status}, tracking={order.tracking_number}")
```

---

## 16.4 Event Sourcing

```
┌─────────────────────────────────────────────────────────────┐
│                    EVENT SOURCING                            │
│                                                              │
│  Traditional: Store current state                           │
│  ─────────────────────────────────                          │
│  ┌─────────────────────────────────────────┐               │
│  │ Account: 123                             │               │
│  │ Balance: $150    ← Only current state!  │               │
│  │ Updated: 2024-01-15                      │               │
│  └─────────────────────────────────────────┘               │
│                                                              │
│  Event Sourcing: Store all events                           │
│  ────────────────────────────────                           │
│  ┌─────────────────────────────────────────┐               │
│  │ Event 1: AccountCreated($0)             │               │
│  │ Event 2: MoneyDeposited($200)           │               │
│  │ Event 3: MoneyWithdrawn($50)            │               │
│  │ Event 4: MoneyDeposited($100)           │               │
│  │ Event 5: MoneyWithdrawn($100)           │               │
│  │ ─────────────────────────────            │               │
│  │ Current State: $0+200-50+100-100 = $150 │               │
│  └─────────────────────────────────────────┘               │
│                                                              │
│  Benefits:                                                  │
│  ├── Complete audit trail                                   │
│  ├── Can rebuild state at any point in time                 │
│  ├── Debug by replaying events                              │
│  ├── Easy to add new projections                            │
│  └── Natural fit for CQRS                                   │
│                                                              │
│  Challenges:                                                │
│  ├── Storage grows forever                                  │
│  ├── Schema evolution is complex                            │
│  ├── Eventual consistency                                   │
│  └── Requires mindset shift                                 │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

```python
# event_sourcing.py
from dataclasses import dataclass, field
from datetime import datetime
from typing import List, Dict, Any, Optional, Type
from abc import ABC, abstractmethod
import json
import uuid

# ============== BASE CLASSES ==============

@dataclass
class Event:
    """Base class for all domain events"""
    event_id: str = field(default_factory=lambda: str(uuid.uuid4()))
    timestamp: datetime = field(default_factory=datetime.utcnow)
    version: int = 0

@dataclass
class AccountCreated(Event):
    account_id: str = ""
    owner_name: str = ""
    initial_balance: float = 0.0

@dataclass
class MoneyDeposited(Event):
    account_id: str = ""
    amount: float = 0.0
    description: str = ""

@dataclass
class MoneyWithdrawn(Event):
    account_id: str = ""
    amount: float = 0.0
    description: str = ""

@dataclass
class AccountClosed(Event):
    account_id: str = ""
    reason: str = ""

# ============== EVENT STORE ==============

class EventStore:
    """
    Append-only store for events.
    Events are immutable once stored.
    """
    
    def __init__(self):
        self.events: Dict[str, List[Event]] = {}  # aggregate_id -> events
        self.all_events: List[Event] = []  # Global event log
    
    def append(self, aggregate_id: str, events: List[Event], expected_version: int):
        """
        Append events with optimistic concurrency control.
        """
        if aggregate_id not in self.events:
            self.events[aggregate_id] = []
        
        current_version = len(self.events[aggregate_id])
        
        # Optimistic locking
        if current_version != expected_version:
            raise ConcurrencyError(
                f"Expected version {expected_version}, but current is {current_version}"
            )
        
        # Assign versions and append
        for i, event in enumerate(events):
            event.version = current_version + i + 1
            self.events[aggregate_id].append(event)
            self.all_events.append(event)
    
    def get_events(self, aggregate_id: str, from_version: int = 0) -> List[Event]:
        """Get events for an aggregate, optionally from a specific version"""
        events = self.events.get(aggregate_id, [])
        return [e for e in events if e.version > from_version]
    
    def get_all_events(self, from_position: int = 0) -> List[Event]:
        """Get all events (for rebuilding read models)"""
        return self.all_events[from_position:]

class ConcurrencyError(Exception):
    pass

# ============== AGGREGATE ==============

class BankAccount:
    """
    Event-sourced aggregate.
    State is derived from events.
    """
    
    def __init__(self, account_id: str):
        self.account_id = account_id
        self.owner_name = ""
        self.balance = 0.0
        self.is_open = False
        self.version = 0
        self.pending_events: List[Event] = []
    
    # ===== Command Methods (Business Logic) =====
    
    def create(self, owner_name: str, initial_balance: float = 0.0):
        """Command: Create account"""
        if self.is_open:
            raise ValueError("Account already exists")
        
        if initial_balance < 0:
            raise ValueError("Initial balance cannot be negative")
        
        self._apply(AccountCreated(
            account_id=self.account_id,
            owner_name=owner_name,
            initial_balance=initial_balance
        ))
    
    def deposit(self, amount: float, description: str = ""):
        """Command: Deposit money"""
        if not self.is_open:
            raise ValueError("Account is not open")
        
        if amount <= 0:
            raise ValueError("Deposit amount must be positive")
        
        self._apply(MoneyDeposited(
            account_id=self.account_id,
            amount=amount,
            description=description
        ))
    
    def withdraw(self, amount: float, description: str = ""):
        """Command: Withdraw money"""
        if not self.is_open:
            raise ValueError("Account is not open")
        
        if amount <= 0:
            raise ValueError("Withdrawal amount must be positive")
        
        if amount > self.balance:
            raise ValueError(f"Insufficient funds. Balance: {self.balance}")
        
        self._apply(MoneyWithdrawn(
            account_id=self.account_id,
            amount=amount,
            description=description
        ))
    
    def close(self, reason: str = ""):
        """Command: Close account"""
        if not self.is_open:
            raise ValueError("Account is not open")
        
        if self.balance != 0:
            raise ValueError(f"Cannot close account with balance {self.balance}")
        
        self._apply(AccountClosed(
            account_id=self.account_id,
            reason=reason
        ))
    
    # ===== Event Application (State Changes) =====
    
    def _apply(self, event: Event):
        """Apply event to update state"""
        self._mutate(event)
        self.pending_events.append(event)
    
    def _mutate(self, event: Event):
        """Actually mutate state based on event type"""
        if isinstance(event, AccountCreated):
            self.owner_name = event.owner_name
            self.balance = event.initial_balance
            self.is_open = True
        
        elif isinstance(event, MoneyDeposited):
            self.balance += event.amount
        
        elif isinstance(event, MoneyWithdrawn):
            self.balance -= event.amount
        
        elif isinstance(event, AccountClosed):
            self.is_open = False
        
        self.version = event.version if event.version else self.version + 1
    
    # ===== Reconstitution =====
    
    @classmethod
    def from_events(cls, account_id: str, events: List[Event]) -> 'BankAccount':
        """Rebuild aggregate state from events"""
        account = cls(account_id)
        
        for event in events:
            account._mutate(event)
        
        return account

# ============== REPOSITORY ==============

class BankAccountRepository:
    """Repository that saves and loads event-sourced aggregates"""
    
    def __init__(self, event_store: EventStore):
        self.event_store = event_store
    
    def save(self, account: BankAccount):
        """Save pending events to event store"""
        if not account.pending_events:
            return
        
        expected_version = account.version - len(account.pending_events)
        
        self.event_store.append(
            aggregate_id=account.account_id,
            events=account.pending_events,
            expected_version=expected_version
        )
        
        account.pending_events = []
    
    def get(self, account_id: str) -> Optional[BankAccount]:
        """Load aggregate by replaying events"""
        events = self.event_store.get_events(account_id)
        
        if not events:
            return None
        
        return BankAccount.from_events(account_id, events)

# ============== SNAPSHOTS (Performance Optimization) ==============

@dataclass
class Snapshot:
    """Snapshot of aggregate state at a point in time"""
    aggregate_id: str
    version: int
    state: Dict[str, Any]
    timestamp: datetime = field(default_factory=datetime.utcnow)

class SnapshotStore:
    """Store for aggregate snapshots"""
    
    def __init__(self):
        self.snapshots: Dict[str, Snapshot] = {}
    
    def save(self, aggregate_id: str, version: int, state: Dict[str, Any]):
        self.snapshots[aggregate_id] = Snapshot(
            aggregate_id=aggregate_id,
            version=version,
            state=state
        )
    
    def get(self, aggregate_id: str) -> Optional[Snapshot]:
        return self.snapshots.get(aggregate_id)

class OptimizedBankAccountRepository:
    """
    Repository with snapshot support.
    Snapshots avoid replaying all events from beginning.
    """
    
    def __init__(self, event_store: EventStore, snapshot_store: SnapshotStore):
        self.event_store = event_store
        self.snapshot_store = snapshot_store
        self.snapshot_frequency = 100  # Take snapshot every 100 events
    
    def get(self, account_id: str) -> Optional[BankAccount]:
        """Load from snapshot + recent events"""
        # Try to get snapshot
        snapshot = self.snapshot_store.get(account_id)
        
        if snapshot:
            # Restore from snapshot
            account = BankAccount(account_id)
            account.owner_name = snapshot.state['owner_name']
            account.balance = snapshot.state['balance']
            account.is_open = snapshot.state['is_open']
            account.version = snapshot.version
            
            # Apply events after snapshot
            events = self.event_store.get_events(account_id, from_version=snapshot.version)
            for event in events:
                account._mutate(event)
            
            return account
        
        # No snapshot - replay all events
        events = self.event_store.get_events(account_id)
        if not events:
            return None
        
        return BankAccount.from_events(account_id, events)
    
    def save(self, account: BankAccount):
        """Save events and possibly create snapshot"""
        if not account.pending_events:
            return
        
        expected_version = account.version - len(account.pending_events)
        
        self.event_store.append(
            aggregate_id=account.account_id,
            events=account.pending_events,
            expected_version=expected_version
        )
        
        # Create snapshot if needed
        if account.version % self.snapshot_frequency == 0:
            self.snapshot_store.save(
                aggregate_id=account.account_id,
                version=account.version,
                state={
                    'owner_name': account.owner_name,
                    'balance': account.balance,
                    'is_open': account.is_open
                }
            )
        
        account.pending_events = []

# ============== DEMO ==============

def demo_event_sourcing():
    event_store = EventStore()
    repo = BankAccountRepository(event_store)
    
    # Create account
    print("=== Creating Account ===")
    account = BankAccount("ACC-001")
    account.create("Alice Smith", initial_balance=100.0)
    repo.save(account)
    
    # Make transactions
    print("\n=== Making Transactions ===")
    account = repo.get("ACC-001")
    account.deposit(50.0, "Salary")
    account.withdraw(30.0, "Groceries")
    account.deposit(200.0, "Bonus")
    account.withdraw(100.0, "Rent")
    repo.save(account)
    
    # Check balance
    print(f"\nCurrent balance: ${account.balance}")
    
    # View event history
    print("\n=== Event History ===")
    events = event_store.get_events("ACC-001")
    for event in events:
        print(f"  v{event.version}: {event.__class__.__name__}")
        if hasattr(event, 'amount'):
            print(f"         Amount: ${event.amount}")
    
    # Rebuild state from events
    print("\n=== Rebuilding State ===")
    rebuilt = repo.get("ACC-001")
    print(f"Rebuilt balance: ${rebuilt.balance}")
    print(f"Matches: {account.balance == rebuilt.balance}")

# demo_event_sourcing()
```

---

## 16.5 Rate Limiting

```
┌─────────────────────────────────────────────────────────────┐
│                    RATE LIMITING                             │
│                                                              │
│  Why Rate Limit?                                            │
│  ├── Prevent abuse and DDoS attacks                         │
│  ├── Ensure fair usage among clients                        │
│  ├── Protect backend services from overload                 │
│  └── Control costs (API calls, compute)                     │
│                                                              │
│                                                              │
│  Common Algorithms:                                         │
│  ──────────────────                                         │
│                                                              │
│  1. TOKEN BUCKET                                            │
│     Bucket fills with tokens at fixed rate.                 │
│     Each request takes a token.                             │
│     No token = rejected.                                    │
│     Allows bursts up to bucket size.                        │
│                                                              │
│  2. LEAKY BUCKET                                            │
│     Requests enter a bucket (queue).                        │
│     Processed at fixed rate.                                │
│     Smooths out traffic.                                    │
│                                                              │
│  3. FIXED WINDOW                                            │
│     Count requests per time window (e.g., per minute).      │
│     Reset at window boundary.                               │
│     Simple but has edge case issues.                        │
│                                                              │
│  4. SLIDING WINDOW                                          │
│     Rolling window of time.                                 │
│     Most accurate but more complex.                         │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

```python
# rate_limiting.py
import time
import asyncio
from dataclasses import dataclass
from typing import Optional, Tuple
import redis.asyncio as redis
from abc import ABC, abstractmethod

# ============== TOKEN BUCKET ==============

class TokenBucket:
    """
    Token Bucket Algorithm.
    Allows bursts up to bucket capacity.
    """
    
    def __init__(
        self,
        capacity: int,        # Max tokens in bucket
        refill_rate: float,   # Tokens added per second
    ):
        self.capacity = capacity
        self.refill_rate = refill_rate
        self.tokens = capacity
        self.last_refill = time.time()
    
    def _refill(self):
        """Add tokens based on time passed"""
        now = time.time()
        elapsed = now - self.last_refill
        
        # Add tokens proportional to elapsed time
        tokens_to_add = elapsed * self.refill_rate
        self.tokens = min(self.capacity, self.tokens + tokens_to_add)
        self.last_refill = now
    
    def consume(self, tokens: int = 1) -> bool:
        """Try to consume tokens. Returns True if allowed."""
        self._refill()
        
        if self.tokens >= tokens:
            self.tokens -= tokens
            return True
        
        return False
    
    def get_wait_time(self, tokens: int = 1) -> float:
        """How long to wait for tokens to be available"""
        self._refill()
        
        if self.tokens >= tokens:
            return 0
        
        tokens_needed = tokens - self.tokens
        return tokens_needed / self.refill_rate

# ============== SLIDING WINDOW ==============

class SlidingWindowCounter:
    """
    Sliding Window Counter Algorithm.
    More accurate than fixed window.
    """
    
    def __init__(self, window_size: int, max_requests: int):
        self.window_size = window_size  # In seconds
        self.max_requests = max_requests
        self.previous_count = 0
        self.current_count = 0
        self.window_start = int(time.time())
    
    def _get_window_weight(self) -> float:
        """Calculate weight of previous window"""
        now = time.time()
        current_window = int(now) // self.window_size
        window_start = current_window * self.window_size
        
        # How far we are into current window (0.0 to 1.0)
        progress = (now - window_start) / self.window_size
        
        # Weight of previous window
        return 1 - progress
    
    def is_allowed(self) -> Tuple[bool, int]:
        """Check if request is allowed. Returns (allowed, current_count)"""
        now = time.time()
        current_window = int(now) // self.window_size
        
        # Check if we moved to a new window
        if current_window != self.window_start // self.window_size:
            self.previous_count = self.current_count
            self.current_count = 0
            self.window_start = current_window * self.window_size
        
        # Calculate weighted count
        weight = self._get_window_weight()
        estimated_count = (self.previous_count * weight) + self.current_count
        
        if estimated_count < self.max_requests:
            self.current_count += 1
            return True, int(estimated_count) + 1
        
        return False, int(estimated_count)

# ============== DISTRIBUTED RATE LIMITER (Redis) ==============

class DistributedRateLimiter:
    """
    Distributed rate limiter using Redis.
    Works across multiple application instances.
    """
    
    def __init__(self, redis_client: redis.Redis):
        self.redis = redis_client
    
    async def is_allowed(
        self,
        key: str,              # Identifier (user_id, IP, API key)
        max_requests: int,     # Max requests per window
        window_seconds: int,   # Window size
    ) -> Tuple[bool, dict]:
        """
        Check if request is allowed using sliding window.
        Returns (allowed, info_dict).
        """
        now = time.time()
        window_start = now - window_seconds
        
        # Use sorted set with timestamp as score
        redis_key = f"ratelimit:{key}"
        
        async with self.redis.pipeline(transaction=True) as pipe:
            # Remove old entries outside window
            pipe.zremrangebyscore(redis_key, 0, window_start)
            
            # Count current entries
            pipe.zcard(redis_key)
            
            # Add current request
            pipe.zadd(redis_key, {str(now): now})
            
            # Set expiry
            pipe.expire(redis_key, window_seconds)
            
            results = await pipe.execute()
        
        current_count = results[1]
        
        allowed = current_count < max_requests
        
        info = {
            "allowed": allowed,
            "current": current_count,
            "limit": max_requests,
            "remaining": max(0, max_requests - current_count - 1),
            "reset_at": int(now) + window_seconds
        }
        
        if not allowed:
            # Remove the request we just added
            await self.redis.zrem(redis_key, str(now))
        
        return allowed, info
    
    async def is_allowed_token_bucket(
        self,
        key: str,
        capacity: int,
        refill_rate: float,
        tokens: int = 1
    ) -> Tuple[bool, dict]:
        """
        Token bucket using Redis for distributed state.
        """
        redis_key = f"tokenbucket:{key}"
        now = time.time()
        
        # Lua script for atomic token bucket
        lua_script = """
        local key = KEYS[1]
        local capacity = tonumber(ARGV[1])
        local refill_rate = tonumber(ARGV[2])
        local tokens_requested = tonumber(ARGV[3])
        local now = tonumber(ARGV[4])
        
        -- Get current state
        local bucket = redis.call('HMGET', key, 'tokens', 'last_refill')
        local tokens = tonumber(bucket[1]) or capacity
        local last_refill = tonumber(bucket[2]) or now
        
        -- Refill tokens
        local elapsed = now - last_refill
        tokens = math.min(capacity, tokens + (elapsed * refill_rate))
        
        -- Try to consume
        local allowed = 0
        if tokens >= tokens_requested then
            tokens = tokens - tokens_requested
            allowed = 1
        end
        
        -- Save state
        redis.call('HMSET', key, 'tokens', tokens, 'last_refill', now)
        redis.call('EXPIRE', key, 3600)
        
        return {allowed, tokens}
        """
        
        result = await self.redis.eval(
            lua_script,
            1,
            redis_key,
            capacity,
            refill_rate,
            tokens,
            now
        )
        
        allowed = bool(result[0])
        remaining_tokens = float(result[1])
        
        return allowed, {
            "allowed": allowed,
            "remaining_tokens": remaining_tokens,
            "capacity": capacity
        }

# ============== FASTAPI MIDDLEWARE ==============

from fastapi import FastAPI, Request, HTTPException, Response
from starlette.middleware.base import BaseHTTPMiddleware

class RateLimitMiddleware(BaseHTTPMiddleware):
    """Rate limiting middleware for FastAPI"""
    
    def __init__(
        self,
        app,
        redis_client: redis.Redis,
        requests_per_minute: int = 60,
        key_func=None  # Function to extract key from request
    ):
        super().__init__(app)
        self.limiter = DistributedRateLimiter(redis_client)
        self.requests_per_minute = requests_per_minute
        self.key_func = key_func or self._default_key_func
    
    def _default_key_func(self, request: Request) -> str:
        """Default: rate limit by IP address"""
        forwarded = request.headers.get("X-Forwarded-For")
        if forwarded:
            return forwarded.split(",")[0].strip()
        return request.client.host or "unknown"
    
    async def dispatch(self, request: Request, call_next):
        # Skip rate limiting for health checks
        if request.url.path in ["/health", "/health/live", "/health/ready"]:
            return await call_next(request)
        
        # Get rate limit key
        key = self.key_func(request)
        
        # Check rate limit
        allowed, info = await self.limiter.is_allowed(
            key=key,
            max_requests=self.requests_per_minute,
            window_seconds=60
        )
        
        if not allowed:
            raise HTTPException(
                status_code=429,
                detail="Rate limit exceeded",
                headers={
                    "X-RateLimit-Limit": str(info["limit"]),
                    "X-RateLimit-Remaining": str(info["remaining"]),
                    "X-RateLimit-Reset": str(info["reset_at"]),
                    "Retry-After": str(info["reset_at"] - int(time.time()))
                }
            )
        
        # Add rate limit headers to response
        response = await call_next(request)
        response.headers["X-RateLimit-Limit"] = str(info["limit"])
        response.headers["X-RateLimit-Remaining"] = str(info["remaining"])
        response.headers["X-RateLimit-Reset"] = str(info["reset_at"])
        
        return response

# ============== USAGE ==============

app = FastAPI()

# Add rate limiting middleware
# redis_client = redis.Redis(...)
# app.add_middleware(RateLimitMiddleware, redis_client=redis_client, requests_per_minute=100)

@app.get("/api/resource")
async def get_resource():
    return {"data": "This endpoint is rate limited"}

# Decorator-based rate limiting for specific endpoints
def rate_limit(max_requests: int = 10, window_seconds: int = 60):
    """Decorator for endpoint-specific rate limits"""
    def decorator(func):
        async def wrapper(request: Request, *args, **kwargs):
            # Get limiter from app state
            limiter = request.app.state.rate_limiter
            key = f"{func.__name__}:{request.client.host}"
            
            allowed, info = await limiter.is_allowed(
                key=key,
                max_requests=max_requests,
                window_seconds=window_seconds
            )
            
            if not allowed:
                raise HTTPException(status_code=429, detail="Rate limit exceeded")
            
            return await func(request, *args, **kwargs)
        return wrapper
    return decorator
```

---

## Summary

| Pattern | Use Case | Trade-offs |
|---------|----------|------------|
| **Horizontal Scaling** | Handle more traffic | Complexity, stateless requirement |
| **Read Replicas** | Read-heavy workloads | Replication lag |
| **Sharding** | Write-heavy, large data | Cross-shard queries hard |
| **CQRS** | Different read/write needs | Eventual consistency |
| **Event Sourcing** | Audit trails, temporal queries | Storage, complexity |
| **Rate Limiting** | Protect services | User experience |

---

## Practice Exercises

1. Implement read/write splitting with automatic failover
2. Build a sharded user database with consistent hashing
3. Create a CQRS system with separate read/write models
4. Implement event sourcing for an order system
5. Build distributed rate limiting with Redis

**Next Chapter**: Data pipeline patterns - batch and stream processing!
