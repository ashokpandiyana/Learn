# Chapter 13: Distributed Coordination

> **Goal**: Master the techniques for coordinating actions across distributed nodes

---

## 13.1 Why Coordination is Hard

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           THE COORDINATION PROBLEM                           â”‚
â”‚                                                              â”‚
â”‚  In a single machine:                                       â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                       â”‚
â”‚  - Use mutex/lock                                           â”‚
â”‚  - Use atomic operations                                    â”‚
â”‚  - Simple and fast!                                         â”‚
â”‚                                                              â”‚
â”‚  In distributed systems:                                    â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                   â”‚
â”‚  - No shared memory                                         â”‚
â”‚  - Network can fail                                         â”‚
â”‚  - Clocks are not synchronized                              â”‚
â”‚  - Nodes can crash at any time                              â”‚
â”‚                                                              â”‚
â”‚                                                              â”‚
â”‚  Common Coordination Problems:                              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                              â”‚
â”‚                                                              â”‚
â”‚  1. LEADER ELECTION                                         â”‚
â”‚     "Who's in charge?" - Only ONE node should be leader     â”‚
â”‚                                                              â”‚
â”‚  2. DISTRIBUTED LOCKING                                     â”‚
â”‚     "Who can access this resource?" - Mutual exclusion      â”‚
â”‚                                                              â”‚
â”‚  3. GROUP MEMBERSHIP                                        â”‚
â”‚     "Who's in the cluster?" - Track alive nodes             â”‚
â”‚                                                              â”‚
â”‚  4. CONFIGURATION MANAGEMENT                                â”‚
â”‚     "What are the settings?" - Consistent config            â”‚
â”‚                                                              â”‚
â”‚  5. BARRIER SYNCHRONIZATION                                 â”‚
â”‚     "Wait for everyone" - Coordinate phases                 â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 13.2 Leader Election

### Why We Need Leaders

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 LEADER ELECTION                              â”‚
â”‚                                                              â”‚
â”‚  Problem: Some tasks should only be done by ONE node        â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€               â”‚
â”‚                                                              â”‚
â”‚  Examples:                                                  â”‚
â”‚  â”œâ”€â”€ Database writes (single-leader replication)            â”‚
â”‚  â”œâ”€â”€ Cron jobs (run once, not on every node)               â”‚
â”‚  â”œâ”€â”€ Coordination tasks (assign work to workers)            â”‚
â”‚  â””â”€â”€ Resource management (allocate partitions)              â”‚
â”‚                                                              â”‚
â”‚                                                              â”‚
â”‚  Without Leader Election:                                   â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                  â”‚
â”‚                                                              â”‚
â”‚  Node A: "I'll send the daily email!"                       â”‚
â”‚  Node B: "I'll send the daily email!"                       â”‚
â”‚  Node C: "I'll send the daily email!"                       â”‚
â”‚                                                              â”‚
â”‚  User receives 3 emails! ğŸ˜±                                 â”‚
â”‚                                                              â”‚
â”‚                                                              â”‚
â”‚  With Leader Election:                                      â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                      â”‚
â”‚                                                              â”‚
â”‚  Node A (Leader): "I'll send the daily email!"              â”‚
â”‚  Node B (Follower): "A is leader, I'll wait"               â”‚
â”‚  Node C (Follower): "A is leader, I'll wait"               â”‚
â”‚                                                              â”‚
â”‚  User receives 1 email! âœ“                                   â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Leader Election Requirements

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          LEADER ELECTION REQUIREMENTS                        â”‚
â”‚                                                              â”‚
â”‚  1. SAFETY: At most ONE leader at any time                  â”‚
â”‚     No "split brain" - two nodes thinking they're leader    â”‚
â”‚                                                              â”‚
â”‚  2. LIVENESS: Eventually, a leader is elected               â”‚
â”‚     System doesn't get stuck without a leader forever       â”‚
â”‚                                                              â”‚
â”‚  3. FAULT TOLERANCE: Survives node failures                 â”‚
â”‚     If leader crashes, new leader is elected                â”‚
â”‚                                                              â”‚
â”‚                                                              â”‚
â”‚  The Danger: SPLIT BRAIN                                    â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                   â”‚
â”‚                                                              â”‚
â”‚  Network Partition:                                         â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     X     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚  Partition A â”‚    X     â”‚  Partition B â”‚                 â”‚
â”‚  â”‚             â”‚     X     â”‚             â”‚                 â”‚
â”‚  â”‚  Node 1 â˜…   â”‚     X     â”‚  Node 3 â˜…   â”‚ â† Two Leaders! â”‚
â”‚  â”‚  Node 2     â”‚     X     â”‚  Node 4     â”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     X     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                                                              â”‚
â”‚  Both sides elect their own leader!                         â”‚
â”‚  Data corruption, inconsistency follows.                    â”‚
â”‚                                                              â”‚
â”‚  Solution: QUORUM - need majority to elect leader           â”‚
â”‚            3 of 5 nodes, 2 of 3 nodes, etc.                 â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Implementation: Redis-Based Leader Election

```python
# leader_election_redis.py
import asyncio
import redis.asyncio as redis
import uuid
import time
from typing import Optional, Callable
from dataclasses import dataclass

@dataclass
class LeaderElectionConfig:
    key: str = "leader:election"
    ttl_seconds: int = 30  # Leader lease duration
    renewal_interval: int = 10  # Renew lease every N seconds
    
class RedisLeaderElection:
    """
    Leader election using Redis with lease-based approach.
    Uses SET NX (set if not exists) for atomic leader claim.
    """
    
    def __init__(
        self,
        redis_url: str,
        node_id: str = None,
        config: LeaderElectionConfig = None
    ):
        self.redis = redis.from_url(redis_url, decode_responses=True)
        self.node_id = node_id or str(uuid.uuid4())[:8]
        self.config = config or LeaderElectionConfig()
        self.is_leader = False
        self._running = False
        self._on_become_leader: Optional[Callable] = None
        self._on_lose_leadership: Optional[Callable] = None
    
    def on_become_leader(self, callback: Callable):
        """Register callback for when this node becomes leader"""
        self._on_become_leader = callback
        return self
    
    def on_lose_leadership(self, callback: Callable):
        """Register callback for when this node loses leadership"""
        self._on_lose_leadership = callback
        return self
    
    async def start(self):
        """Start the leader election process"""
        self._running = True
        print(f"[{self.node_id}] Starting leader election...")
        
        while self._running:
            try:
                if self.is_leader:
                    # Try to renew leadership
                    renewed = await self._renew_leadership()
                    if not renewed:
                        await self._lose_leadership()
                else:
                    # Try to become leader
                    acquired = await self._try_become_leader()
                    if acquired:
                        await self._become_leader()
                
                await asyncio.sleep(self.config.renewal_interval)
                
            except Exception as e:
                print(f"[{self.node_id}] Election error: {e}")
                if self.is_leader:
                    await self._lose_leadership()
                await asyncio.sleep(1)
    
    async def stop(self):
        """Stop leader election and resign if leader"""
        self._running = False
        if self.is_leader:
            await self._resign()
    
    async def _try_become_leader(self) -> bool:
        """
        Attempt to become leader using SET NX (set if not exists).
        This is atomic - only one node can succeed.
        """
        # SET key value NX EX ttl
        # NX = only set if key doesn't exist
        # EX = expire after ttl seconds
        result = await self.redis.set(
            self.config.key,
            self.node_id,
            nx=True,  # Only if not exists
            ex=self.config.ttl_seconds  # Auto-expire
        )
        return result is True
    
    async def _renew_leadership(self) -> bool:
        """
        Renew leadership lease using Lua script for atomicity.
        Only renew if we're still the leader.
        """
        # Lua script: only extend if we're still the leader
        lua_script = """
        if redis.call('GET', KEYS[1]) == ARGV[1] then
            redis.call('EXPIRE', KEYS[1], ARGV[2])
            return 1
        else
            return 0
        end
        """
        result = await self.redis.eval(
            lua_script,
            1,  # number of keys
            self.config.key,  # KEYS[1]
            self.node_id,  # ARGV[1]
            self.config.ttl_seconds  # ARGV[2]
        )
        return result == 1
    
    async def _resign(self):
        """Voluntarily give up leadership"""
        lua_script = """
        if redis.call('GET', KEYS[1]) == ARGV[1] then
            redis.call('DEL', KEYS[1])
            return 1
        else
            return 0
        end
        """
        await self.redis.eval(lua_script, 1, self.config.key, self.node_id)
        await self._lose_leadership()
    
    async def _become_leader(self):
        """Handle becoming leader"""
        self.is_leader = True
        print(f"[{self.node_id}] ğŸ‘‘ Became LEADER")
        if self._on_become_leader:
            await self._on_become_leader()
    
    async def _lose_leadership(self):
        """Handle losing leadership"""
        if self.is_leader:
            self.is_leader = False
            print(f"[{self.node_id}] Lost leadership")
            if self._on_lose_leadership:
                await self._on_lose_leadership()
    
    async def get_current_leader(self) -> Optional[str]:
        """Get the current leader's node ID"""
        return await self.redis.get(self.config.key)

# ============== USAGE EXAMPLE ==============

class SchedulerService:
    """
    Example service that should only run on leader node.
    """
    
    def __init__(self, node_id: str):
        self.node_id = node_id
        self.election = RedisLeaderElection(
            redis_url="redis://localhost:6379",
            node_id=node_id
        )
        self._scheduler_task: Optional[asyncio.Task] = None
        
        self.election.on_become_leader(self._start_scheduling)
        self.election.on_lose_leadership(self._stop_scheduling)
    
    async def start(self):
        """Start the service"""
        await self.election.start()
    
    async def _start_scheduling(self):
        """Called when we become leader - start the scheduler"""
        print(f"[{self.node_id}] Starting scheduler...")
        self._scheduler_task = asyncio.create_task(self._run_scheduler())
    
    async def _stop_scheduling(self):
        """Called when we lose leadership - stop the scheduler"""
        print(f"[{self.node_id}] Stopping scheduler...")
        if self._scheduler_task:
            self._scheduler_task.cancel()
            try:
                await self._scheduler_task
            except asyncio.CancelledError:
                pass
    
    async def _run_scheduler(self):
        """The actual scheduler logic - only runs on leader"""
        while True:
            print(f"[{self.node_id}] ğŸ“§ Running scheduled task (only on leader!)")
            await asyncio.sleep(5)

async def demo_leader_election():
    """Demo with multiple nodes competing for leadership"""
    
    # Simulate 3 nodes
    nodes = [
        RedisLeaderElection("redis://localhost:6379", f"node-{i}")
        for i in range(3)
    ]
    
    # Start all nodes
    tasks = [asyncio.create_task(node.start()) for node in nodes]
    
    # Let them compete for a while
    await asyncio.sleep(15)
    
    # Check who's leader
    for node in nodes:
        status = "LEADER ğŸ‘‘" if node.is_leader else "follower"
        print(f"{node.node_id}: {status}")
    
    # Kill the leader
    leader = next(n for n in nodes if n.is_leader)
    print(f"\n--- Killing leader {leader.node_id} ---\n")
    await leader.stop()
    
    # Watch new leader emerge
    await asyncio.sleep(15)
    
    for node in nodes:
        if node._running:
            status = "LEADER ğŸ‘‘" if node.is_leader else "follower"
            print(f"{node.node_id}: {status}")

# asyncio.run(demo_leader_election())
```

---

## 13.3 Distributed Locks

### Why Distributed Locks

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               DISTRIBUTED LOCKS                              â”‚
â”‚                                                              â”‚
â”‚  Problem: Multiple nodes accessing shared resource          â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€            â”‚
â”‚                                                              â”‚
â”‚  Without Lock:                                              â”‚
â”‚  Node A: Read balance ($100) â†’ Add $50 â†’ Write ($150)      â”‚
â”‚  Node B: Read balance ($100) â†’ Add $30 â†’ Write ($130)      â”‚
â”‚                                                              â”‚
â”‚  Both read $100, final balance is $130 or $150!            â”‚
â”‚  Lost update: $50 or $30 disappeared!                       â”‚
â”‚                                                              â”‚
â”‚                                                              â”‚
â”‚  With Distributed Lock:                                     â”‚
â”‚  Node A: Lock â†’ Read ($100) â†’ Add $50 â†’ Write ($150) â†’ Unlock
â”‚  Node B: Wait... â†’ Lock â†’ Read ($150) â†’ Add $30 â†’ Write ($180)
â”‚                                                              â”‚
â”‚  Final balance: $180 âœ“                                      â”‚
â”‚                                                              â”‚
â”‚                                                              â”‚
â”‚  Lock Requirements:                                         â”‚
â”‚  â”œâ”€â”€ Mutual Exclusion: Only one holder at a time           â”‚
â”‚  â”œâ”€â”€ Deadlock-free: Locks eventually released              â”‚
â”‚  â”œâ”€â”€ Fault-tolerant: Survives failures                     â”‚
â”‚  â””â”€â”€ Fair (optional): Requests served in order             â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### The Dangers of Distributed Locks

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          DISTRIBUTED LOCK DANGERS                            â”‚
â”‚                                                              â”‚
â”‚  DANGER 1: Process Pause (GC, etc.)                        â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                       â”‚
â”‚                                                              â”‚
â”‚  Time:  T1        T2        T3        T4        T5         â”‚
â”‚                                                              â”‚
â”‚  Node A: [Lock acquired]                                    â”‚
â”‚          â”‚                                                   â”‚
â”‚          â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚
â”‚          â””â”€â”€â”‚   GC PAUSE       â”‚â”€â”€â”                         â”‚
â”‚             â”‚   (30 seconds)   â”‚  â”‚                         â”‚
â”‚             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                         â”‚
â”‚                                   â”‚ Lock expired!           â”‚
â”‚  Node B:              [Lock acquired]                       â”‚
â”‚                       â”‚                                      â”‚
â”‚                       â”‚ Writing to resource...              â”‚
â”‚                       â”‚                                      â”‚
â”‚  Node A:              â”‚         â”‚ GC ends                   â”‚
â”‚                       â”‚         â”‚ Still thinks has lock!    â”‚
â”‚                       â”‚         â”‚ Writing to resource! ğŸ’¥   â”‚
â”‚                                                              â”‚
â”‚  TWO nodes writing simultaneously!                          â”‚
â”‚                                                              â”‚
â”‚                                                              â”‚
â”‚  DANGER 2: Clock Skew                                       â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                      â”‚
â”‚                                                              â”‚
â”‚  Lock expires at "10:00:30" (server time)                   â”‚
â”‚  Node A's clock: 10:00:25 (thinks lock valid)              â”‚
â”‚  Actual server time: 10:00:35 (lock expired!)              â”‚
â”‚                                                              â”‚
â”‚  Node A operates on expired lock!                           â”‚
â”‚                                                              â”‚
â”‚                                                              â”‚
â”‚  Solution: FENCING TOKENS (see next section)               â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Redis Lock Implementation (Redlock-like)

```python
# distributed_lock.py
import asyncio
import redis.asyncio as redis
import uuid
import time
from typing import Optional
from contextlib import asynccontextmanager
from dataclasses import dataclass

@dataclass
class LockConfig:
    ttl_ms: int = 10000  # Lock TTL in milliseconds
    retry_delay_ms: int = 200  # Delay between retries
    retry_count: int = 3  # Number of retries

class DistributedLock:
    """
    Distributed lock using Redis.
    Based on the Redlock algorithm principles.
    """
    
    # Lua script for safe release (only release if we own it)
    RELEASE_SCRIPT = """
    if redis.call('GET', KEYS[1]) == ARGV[1] then
        return redis.call('DEL', KEYS[1])
    else
        return 0
    end
    """
    
    # Lua script for extending lock
    EXTEND_SCRIPT = """
    if redis.call('GET', KEYS[1]) == ARGV[1] then
        return redis.call('PEXPIRE', KEYS[1], ARGV[2])
    else
        return 0
    end
    """
    
    def __init__(
        self,
        redis_client: redis.Redis,
        resource: str,
        config: LockConfig = None
    ):
        self.redis = redis_client
        self.resource = f"lock:{resource}"
        self.config = config or LockConfig()
        self.token: Optional[str] = None
        self.acquired_at: Optional[float] = None
    
    async def acquire(self) -> bool:
        """
        Attempt to acquire the lock.
        Returns True if lock was acquired.
        """
        self.token = str(uuid.uuid4())
        
        for attempt in range(self.config.retry_count):
            start_time = time.time() * 1000  # ms
            
            # Try to set the lock
            acquired = await self.redis.set(
                self.resource,
                self.token,
                nx=True,  # Only if not exists
                px=self.config.ttl_ms  # Expire in ms
            )
            
            if acquired:
                elapsed = (time.time() * 1000) - start_time
                # Check if lock is still valid (didn't take too long)
                if elapsed < self.config.ttl_ms:
                    self.acquired_at = time.time()
                    return True
            
            # Wait before retry
            await asyncio.sleep(self.config.retry_delay_ms / 1000)
        
        self.token = None
        return False
    
    async def release(self) -> bool:
        """
        Release the lock.
        Uses Lua script to ensure we only release if we own it.
        """
        if not self.token:
            return False
        
        result = await self.redis.eval(
            self.RELEASE_SCRIPT,
            1,
            self.resource,
            self.token
        )
        
        released = result == 1
        if released:
            self.token = None
            self.acquired_at = None
        
        return released
    
    async def extend(self, additional_ms: int = None) -> bool:
        """
        Extend the lock TTL.
        Useful for long-running operations.
        """
        if not self.token:
            return False
        
        ttl = additional_ms or self.config.ttl_ms
        
        result = await self.redis.eval(
            self.EXTEND_SCRIPT,
            1,
            self.resource,
            self.token,
            ttl
        )
        
        return result == 1
    
    def is_held(self) -> bool:
        """Check if we believe we hold the lock"""
        if not self.token or not self.acquired_at:
            return False
        
        elapsed_ms = (time.time() - self.acquired_at) * 1000
        return elapsed_ms < self.config.ttl_ms
    
    @asynccontextmanager
    async def hold(self):
        """
        Context manager for holding the lock.
        Automatically releases on exit.
        """
        acquired = await self.acquire()
        if not acquired:
            raise LockNotAcquiredError(f"Could not acquire lock: {self.resource}")
        
        try:
            yield self
        finally:
            await self.release()

class LockNotAcquiredError(Exception):
    pass

# ============== LOCK WITH AUTO-EXTENSION ==============

class AutoExtendingLock(DistributedLock):
    """
    A lock that automatically extends itself while held.
    Prevents expiration during long operations.
    """
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self._extend_task: Optional[asyncio.Task] = None
    
    async def acquire(self) -> bool:
        acquired = await super().acquire()
        if acquired:
            self._start_auto_extend()
        return acquired
    
    async def release(self) -> bool:
        self._stop_auto_extend()
        return await super().release()
    
    def _start_auto_extend(self):
        """Start background task to extend lock"""
        self._extend_task = asyncio.create_task(self._auto_extend_loop())
    
    def _stop_auto_extend(self):
        """Stop the auto-extend task"""
        if self._extend_task:
            self._extend_task.cancel()
            self._extend_task = None
    
    async def _auto_extend_loop(self):
        """Periodically extend the lock"""
        extend_interval = self.config.ttl_ms / 3 / 1000  # Extend at 1/3 of TTL
        
        try:
            while True:
                await asyncio.sleep(extend_interval)
                extended = await self.extend()
                if not extended:
                    print(f"Warning: Failed to extend lock {self.resource}")
                    break
        except asyncio.CancelledError:
            pass

# ============== USAGE EXAMPLE ==============

async def demo_distributed_lock():
    redis_client = redis.from_url("redis://localhost:6379")
    
    async def worker(worker_id: str, resource: str):
        lock = DistributedLock(redis_client, resource)
        
        print(f"[{worker_id}] Trying to acquire lock...")
        
        async with lock.hold():
            print(f"[{worker_id}] ğŸ”’ Lock acquired!")
            # Simulate critical section
            for i in range(3):
                print(f"[{worker_id}] Working... {i+1}/3")
                await asyncio.sleep(1)
            print(f"[{worker_id}] ğŸ”“ Releasing lock")
    
    # Multiple workers competing for same lock
    await asyncio.gather(
        worker("Worker-A", "shared-resource"),
        worker("Worker-B", "shared-resource"),
        worker("Worker-C", "shared-resource"),
    )

# asyncio.run(demo_distributed_lock())
```

---

## 13.4 Fencing Tokens

### The Problem Fencing Solves

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  FENCING TOKENS                              â”‚
â”‚                                                              â”‚
â”‚  Problem: Lock expiration during GC pause                   â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                  â”‚
â”‚                                                              â”‚
â”‚  Node A: Acquires lock (token=34)                           â”‚
â”‚          â”‚                                                   â”‚
â”‚          â”‚ [LONG GC PAUSE - lock expires]                   â”‚
â”‚          â”‚                                                   â”‚
â”‚  Node B: Acquires lock (token=35)                           â”‚
â”‚          â”‚                                                   â”‚
â”‚          â”‚ Writes to storage                                â”‚
â”‚          â”‚                                                   â”‚
â”‚  Node A: â”‚ GC ends, thinks it has lock                      â”‚
â”‚          â”‚ Writes to storage! ğŸ’¥                            â”‚
â”‚          â”‚                                                   â”‚
â”‚  DATA CORRUPTION - Node A overwrites Node B's work!         â”‚
â”‚                                                              â”‚
â”‚                                                              â”‚
â”‚  Solution: FENCING TOKEN                                    â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                  â”‚
â”‚                                                              â”‚
â”‚  Lock includes monotonically increasing token.              â”‚
â”‚  Storage server rejects writes with old tokens.             â”‚
â”‚                                                              â”‚
â”‚  Node A: Acquires lock (token=34)                           â”‚
â”‚          â”‚                                                   â”‚
â”‚          â”‚ [LONG GC PAUSE - lock expires]                   â”‚
â”‚          â”‚                                                   â”‚
â”‚  Node B: Acquires lock (token=35)                           â”‚
â”‚          â”‚                                                   â”‚
â”‚          â”‚ Writes to storage (token=35) âœ“                   â”‚
â”‚          â”‚                                                   â”‚
â”‚  Node A: â”‚ GC ends                                          â”‚
â”‚          â”‚ Writes to storage (token=34)                     â”‚
â”‚          â”‚ Storage: "Token 34 < 35, REJECTED!" âœ“            â”‚
â”‚          â”‚                                                   â”‚
â”‚  Data protected by fencing!                                 â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Implementation with Fencing

```python
# fencing_token.py
import asyncio
import redis.asyncio as redis
from typing import Optional, Any
from dataclasses import dataclass
import time

@dataclass
class FencedLock:
    """Lock with fencing token"""
    resource: str
    token: str  # Unique token for this lock holder
    fencing_token: int  # Monotonically increasing number
    acquired_at: float
    ttl_ms: int

class FencingLockManager:
    """
    Distributed lock manager with fencing tokens.
    Each lock acquisition gets a unique, increasing fencing token.
    """
    
    ACQUIRE_SCRIPT = """
    local resource = KEYS[1]
    local token_counter_key = KEYS[2]
    local lock_token = ARGV[1]
    local ttl_ms = tonumber(ARGV[2])
    
    -- Try to acquire lock
    local acquired = redis.call('SET', resource, lock_token, 'NX', 'PX', ttl_ms)
    
    if acquired then
        -- Increment and get fencing token
        local fencing_token = redis.call('INCR', token_counter_key)
        -- Store fencing token with lock
        redis.call('HSET', resource .. ':meta', 'fencing_token', fencing_token)
        redis.call('PEXPIRE', resource .. ':meta', ttl_ms)
        return fencing_token
    else
        return nil
    end
    """
    
    def __init__(self, redis_client: redis.Redis):
        self.redis = redis_client
    
    async def acquire(
        self, 
        resource: str, 
        ttl_ms: int = 10000
    ) -> Optional[FencedLock]:
        """Acquire lock with fencing token"""
        import uuid
        lock_token = str(uuid.uuid4())
        
        fencing_token = await self.redis.eval(
            self.ACQUIRE_SCRIPT,
            2,  # Number of keys
            f"lock:{resource}",  # KEYS[1]
            f"lock:{resource}:counter",  # KEYS[2]
            lock_token,  # ARGV[1]
            ttl_ms  # ARGV[2]
        )
        
        if fencing_token is not None:
            return FencedLock(
                resource=resource,
                token=lock_token,
                fencing_token=int(fencing_token),
                acquired_at=time.time(),
                ttl_ms=ttl_ms
            )
        
        return None
    
    async def release(self, lock: FencedLock) -> bool:
        """Release the lock"""
        script = """
        if redis.call('GET', KEYS[1]) == ARGV[1] then
            redis.call('DEL', KEYS[1])
            redis.call('DEL', KEYS[1] .. ':meta')
            return 1
        end
        return 0
        """
        result = await self.redis.eval(
            script, 1, f"lock:{lock.resource}", lock.token
        )
        return result == 1

class FencedStorage:
    """
    Storage that validates fencing tokens.
    Rejects writes with stale tokens.
    """
    
    def __init__(self):
        self.data = {}
        self.last_token = {}  # resource -> last fencing token
    
    async def write(
        self, 
        resource: str, 
        value: Any, 
        fencing_token: int
    ) -> bool:
        """
        Write to storage with fencing token validation.
        Rejects writes with tokens older than previously seen.
        """
        last = self.last_token.get(resource, 0)
        
        if fencing_token < last:
            print(f"  âš ï¸ REJECTED: Token {fencing_token} < {last} (stale)")
            return False
        
        self.last_token[resource] = fencing_token
        self.data[resource] = value
        print(f"  âœ“ ACCEPTED: Token {fencing_token}, value={value}")
        return True
    
    def read(self, resource: str) -> Any:
        return self.data.get(resource)

# ============== DEMO ==============

async def demo_fencing():
    redis_client = redis.from_url("redis://localhost:6379")
    lock_manager = FencingLockManager(redis_client)
    storage = FencedStorage()
    
    print("=== Fencing Token Demo ===\n")
    
    # Node A acquires lock
    print("Node A: Acquiring lock...")
    lock_a = await lock_manager.acquire("account:123")
    print(f"Node A: Got lock with fencing token {lock_a.fencing_token}")
    
    # Simulate Node A's GC pause - lock expires
    print("\nNode A: [SIMULATING GC PAUSE - LOCK EXPIRES]")
    await asyncio.sleep(0.1)
    
    # Manually expire the lock (simulating timeout)
    await redis_client.delete(f"lock:account:123")
    
    # Node B acquires lock (Node A's lock expired)
    print("\nNode B: Acquiring lock...")
    lock_b = await lock_manager.acquire("account:123")
    print(f"Node B: Got lock with fencing token {lock_b.fencing_token}")
    
    # Node B writes successfully
    print("\nNode B: Writing to storage...")
    await storage.write("account:123", {"balance": 200}, lock_b.fencing_token)
    
    # Node A wakes up and tries to write with old token
    print("\nNode A: [GC PAUSE ENDS] Trying to write...")
    await storage.write("account:123", {"balance": 100}, lock_a.fencing_token)
    
    # Check final value
    print(f"\nFinal value: {storage.read('account:123')}")
    print("Node A's stale write was rejected! Data protected by fencing.")

# asyncio.run(demo_fencing())
```

---

## 13.5 ZooKeeper

### What is ZooKeeper?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     ZOOKEEPER                                â”‚
â”‚                                                              â”‚
â”‚  A centralized service for distributed coordination.        â”‚
â”‚  Like a "distributed file system for small files."          â”‚
â”‚                                                              â”‚
â”‚                                                              â”‚
â”‚  Architecture:                                              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                              â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              ZooKeeper Ensemble                      â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚   â”‚
â”‚  â”‚  â”‚ZK Serverâ”‚â—€â–¶â”‚ZK Serverâ”‚â—€â–¶â”‚ZK Serverâ”‚ (Quorum)    â”‚   â”‚
â”‚  â”‚  â”‚(Leader) â”‚  â”‚(Follower)â”‚ â”‚(Follower)â”‚             â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚          â–²                 â–²                 â–²               â”‚
â”‚          â”‚                 â”‚                 â”‚               â”‚
â”‚     â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”         â”‚
â”‚     â”‚Client A â”‚      â”‚Client B â”‚      â”‚Client C â”‚         â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                                                              â”‚
â”‚                                                              â”‚
â”‚  Data Model: Hierarchical Namespace (like filesystem)       â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â”‚
â”‚                                                              â”‚
â”‚  /                                                          â”‚
â”‚  â”œâ”€â”€ /services                                              â”‚
â”‚  â”‚   â”œâ”€â”€ /services/user-service                             â”‚
â”‚  â”‚   â”‚   â”œâ”€â”€ /services/user-service/instance-1              â”‚
â”‚  â”‚   â”‚   â””â”€â”€ /services/user-service/instance-2              â”‚
â”‚  â”‚   â””â”€â”€ /services/order-service                            â”‚
â”‚  â”œâ”€â”€ /locks                                                 â”‚
â”‚  â”‚   â””â”€â”€ /locks/resource-1                                  â”‚
â”‚  â””â”€â”€ /config                                                â”‚
â”‚      â””â”€â”€ /config/database                                   â”‚
â”‚                                                              â”‚
â”‚  Each node (znode) can store data (up to 1MB).             â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ZooKeeper Node Types

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 ZNODE TYPES                                  â”‚
â”‚                                                              â”‚
â”‚  1. PERSISTENT                                              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                              â”‚
â”‚  Survives client disconnection.                             â”‚
â”‚  Must be explicitly deleted.                                â”‚
â”‚  Use for: Configuration, metadata                           â”‚
â”‚                                                              â”‚
â”‚                                                              â”‚
â”‚  2. EPHEMERAL                                               â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                               â”‚
â”‚  Deleted when client session ends.                          â”‚
â”‚  Cannot have children.                                      â”‚
â”‚  Use for: Service discovery, leader election                â”‚
â”‚                                                              â”‚
â”‚  Client connects â†’ Creates /services/my-service             â”‚
â”‚  Client crashes â†’ /services/my-service auto-deleted!        â”‚
â”‚                                                              â”‚
â”‚                                                              â”‚
â”‚  3. SEQUENTIAL                                              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                              â”‚
â”‚  ZK appends monotonic number to name.                       â”‚
â”‚  /locks/lock- â†’ /locks/lock-0000000001                     â”‚
â”‚  Use for: Distributed locks, queues                         â”‚
â”‚                                                              â”‚
â”‚                                                              â”‚
â”‚  4. EPHEMERAL + SEQUENTIAL                                  â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                 â”‚
â”‚  Combination of both.                                       â”‚
â”‚  Auto-deleted + auto-numbered.                              â”‚
â”‚  Use for: Leader election with automatic failover           â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ZooKeeper Recipes

```python
# zookeeper_recipes.py
from kazoo.client import KazooClient
from kazoo.recipe.lock import Lock
from kazoo.recipe.election import Election
from kazoo.recipe.watchers import DataWatch
import time
import threading

class ZooKeeperRecipes:
    """
    Common distributed coordination patterns using ZooKeeper.
    Uses the Kazoo library for Python.
    """
    
    def __init__(self, hosts: str = "localhost:2181"):
        self.zk = KazooClient(hosts=hosts)
        self.zk.start()
    
    def close(self):
        self.zk.stop()
    
    # ==================== SERVICE DISCOVERY ====================
    
    def register_service(self, service_name: str, instance_id: str, data: dict):
        """
        Register a service instance.
        Uses ephemeral node - auto-removes if service dies!
        """
        import json
        path = f"/services/{service_name}/{instance_id}"
        
        self.zk.ensure_path(f"/services/{service_name}")
        self.zk.create(
            path,
            json.dumps(data).encode(),
            ephemeral=True,  # Auto-delete on disconnect!
            makepath=True
        )
        print(f"Registered: {path}")
    
    def discover_services(self, service_name: str) -> list:
        """Get all instances of a service"""
        import json
        path = f"/services/{service_name}"
        
        if not self.zk.exists(path):
            return []
        
        instances = []
        for instance_id in self.zk.get_children(path):
            data, stat = self.zk.get(f"{path}/{instance_id}")
            instances.append({
                "instance_id": instance_id,
                "data": json.loads(data.decode()) if data else {}
            })
        
        return instances
    
    def watch_services(self, service_name: str, callback):
        """Watch for service changes"""
        path = f"/services/{service_name}"
        
        @self.zk.ChildrenWatch(path)
        def watch_children(children):
            instances = self.discover_services(service_name)
            callback(instances)
            return True  # Keep watching
    
    # ==================== DISTRIBUTED LOCK ====================
    
    def get_lock(self, resource: str) -> Lock:
        """
        Get a distributed lock.
        Uses sequential ephemeral nodes for fairness.
        """
        return Lock(self.zk, f"/locks/{resource}")
    
    def with_lock(self, resource: str, func, *args, **kwargs):
        """Execute function while holding lock"""
        lock = self.get_lock(resource)
        
        with lock:
            print(f"Acquired lock: {resource}")
            result = func(*args, **kwargs)
            print(f"Releasing lock: {resource}")
            return result
    
    # ==================== LEADER ELECTION ====================
    
    def run_election(self, election_path: str, node_id: str, leader_func):
        """
        Participate in leader election.
        leader_func is called when this node becomes leader.
        """
        election = Election(self.zk, election_path, node_id)
        
        print(f"[{node_id}] Joining election...")
        election.run(leader_func)  # Blocks until leader, then calls func
    
    # ==================== DISTRIBUTED QUEUE ====================
    
    def enqueue(self, queue_name: str, item: str):
        """Add item to distributed queue"""
        path = f"/queues/{queue_name}/item-"
        
        self.zk.ensure_path(f"/queues/{queue_name}")
        created_path = self.zk.create(
            path,
            item.encode(),
            sequence=True  # Auto-append number
        )
        print(f"Enqueued: {created_path}")
        return created_path
    
    def dequeue(self, queue_name: str) -> str:
        """Remove and return first item from queue"""
        path = f"/queues/{queue_name}"
        
        children = sorted(self.zk.get_children(path))
        if not children:
            return None
        
        first = children[0]
        item_path = f"{path}/{first}"
        
        data, stat = self.zk.get(item_path)
        self.zk.delete(item_path)
        
        return data.decode()
    
    # ==================== CONFIGURATION MANAGEMENT ====================
    
    def set_config(self, key: str, value: str):
        """Set configuration value"""
        path = f"/config/{key}"
        
        if self.zk.exists(path):
            self.zk.set(path, value.encode())
        else:
            self.zk.create(path, value.encode(), makepath=True)
    
    def get_config(self, key: str) -> str:
        """Get configuration value"""
        path = f"/config/{key}"
        
        if not self.zk.exists(path):
            return None
        
        data, stat = self.zk.get(path)
        return data.decode()
    
    def watch_config(self, key: str, callback):
        """Watch for configuration changes"""
        path = f"/config/{key}"
        
        @DataWatch(self.zk, path)
        def watch_config_change(data, stat):
            if data:
                callback(data.decode())
            return True

# ============== USAGE EXAMPLES ==============

def demo_service_discovery():
    """Demo service discovery with ZooKeeper"""
    zk = ZooKeeperRecipes()
    
    # Register services
    zk.register_service("user-service", "instance-1", {
        "host": "192.168.1.10",
        "port": 8080
    })
    zk.register_service("user-service", "instance-2", {
        "host": "192.168.1.11",
        "port": 8080
    })
    
    # Discover services
    instances = zk.discover_services("user-service")
    print(f"Found instances: {instances}")
    
    # Watch for changes
    def on_service_change(instances):
        print(f"Services changed: {instances}")
    
    zk.watch_services("user-service", on_service_change)
    
    time.sleep(5)
    zk.close()

def demo_distributed_lock():
    """Demo distributed lock with ZooKeeper"""
    zk = ZooKeeperRecipes()
    
    def critical_section():
        print("In critical section...")
        time.sleep(2)
        return "done"
    
    result = zk.with_lock("my-resource", critical_section)
    print(f"Result: {result}")
    
    zk.close()

# Run demos
# demo_service_discovery()
# demo_distributed_lock()
```

---

## 13.6 Comparison: Redis vs ZooKeeper vs etcd

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         COORDINATION SERVICE COMPARISON                      â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚   Feature    â”‚    Redis     â”‚  ZooKeeper   â”‚   etcd    â”‚â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚
â”‚  â”‚ Consensus    â”‚ No (single   â”‚ Zab          â”‚ Raft      â”‚â”‚
â”‚  â”‚              â”‚ leader)      â”‚              â”‚           â”‚â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚
â”‚  â”‚ Data Model   â”‚ Key-Value    â”‚ Hierarchical â”‚ Key-Value â”‚â”‚
â”‚  â”‚              â”‚              â”‚ (tree)       â”‚ (flat)    â”‚â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚
â”‚  â”‚ Watches      â”‚ Pub/Sub      â”‚ Native       â”‚ Native    â”‚â”‚
â”‚  â”‚              â”‚ (separate)   â”‚ (built-in)   â”‚ (built-in)â”‚â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚
â”‚  â”‚ Ephemeral    â”‚ TTL-based    â”‚ Native       â”‚ Lease-    â”‚â”‚
â”‚  â”‚ Nodes        â”‚              â”‚              â”‚ based     â”‚â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚
â”‚  â”‚ Performance  â”‚ Fastest      â”‚ Good         â”‚ Good      â”‚â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚
â”‚  â”‚ Consistency  â”‚ Eventual*    â”‚ Strong       â”‚ Strong    â”‚â”‚
â”‚  â”‚              â”‚ (configurable)â”‚ (lineariz.) â”‚ (lineariz)â”‚â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚
â”‚  â”‚ Best For     â”‚ Caching,     â”‚ Coordination â”‚ K8s,      â”‚â”‚
â”‚  â”‚              â”‚ simple locks â”‚ heavy apps   â”‚ service   â”‚â”‚
â”‚  â”‚              â”‚              â”‚              â”‚ discovery â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                              â”‚
â”‚  Recommendations:                                           â”‚
â”‚  â”œâ”€â”€ Simple locks/caching: Redis                            â”‚
â”‚  â”œâ”€â”€ Complex coordination: ZooKeeper                        â”‚
â”‚  â”œâ”€â”€ Kubernetes ecosystem: etcd                             â”‚
â”‚  â””â”€â”€ New projects: etcd (simpler than ZK)                   â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Summary

| Concept | Purpose | Key Insight |
|---------|---------|-------------|
| **Leader Election** | Single coordinator | Need quorum to prevent split-brain |
| **Distributed Locks** | Mutual exclusion | Beware of GC pauses and clock skew |
| **Fencing Tokens** | Prevent stale operations | Monotonic tokens protect storage |
| **ZooKeeper** | Coordination service | Ephemeral nodes for auto-cleanup |

---

## Practice Exercises

1. Implement leader election with automatic failover
2. Build a distributed lock with fencing tokens
3. Create a service registry using ZooKeeper/etcd
4. Implement a distributed queue with ordering guarantees
5. Test lock behavior during simulated network partitions

**Next Chapter**: Microservices Architecture patterns!
