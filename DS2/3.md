# Chapter 3: Core Concepts and Terminology

> **Goal**: Master the vocabulary and fundamental concepts used in distributed systems design

---

## 3.1 Infrastructure Terminology

### Nodes, Clusters, and Data Centers

Understanding the physical and logical hierarchy is crucial:

```
┌─────────────────────────────────────────────────────────────────────────┐
│                              GLOBAL VIEW                                 │
│                                                                          │
│  ┌─────────────────────────────┐    ┌─────────────────────────────┐    │
│  │       REGION: US-EAST       │    │       REGION: EU-WEST       │    │
│  │                             │    │                             │    │
│  │  ┌───────────────────────┐  │    │  ┌───────────────────────┐  │    │
│  │  │  AVAILABILITY ZONE A  │  │    │  │  AVAILABILITY ZONE A  │  │    │
│  │  │  ┌─────────────────┐  │  │    │  │  ┌─────────────────┐  │  │    │
│  │  │  │  DATA CENTER 1  │  │  │    │  │  │  DATA CENTER 1  │  │  │    │
│  │  │  │  ┌───────────┐  │  │  │    │  │  │  ┌───────────┐  │  │  │    │
│  │  │  │  │ CLUSTER   │  │  │  │    │  │  │  │ CLUSTER   │  │  │  │    │
│  │  │  │  │┌─┐┌─┐┌─┐  │  │  │  │    │  │  │  │┌─┐┌─┐┌─┐  │  │  │  │    │
│  │  │  │  ││N││N││N│  │  │  │  │    │  │  │  ││N││N││N│  │  │  │  │    │
│  │  │  │  │└─┘└─┘└─┘  │  │  │  │    │  │  │  │└─┘└─┘└─┘  │  │  │  │    │
│  │  │  │  │  NODES    │  │  │  │    │  │  │  │  NODES    │  │  │  │    │
│  │  │  │  └───────────┘  │  │  │    │  │  │  └───────────┘  │  │  │    │
│  │  │  └─────────────────┘  │  │    │  │  └─────────────────┘  │  │    │
│  │  └───────────────────────┘  │    │  └───────────────────────┘  │    │
│  │                             │    │                             │    │
│  │  ┌───────────────────────┐  │    │  ┌───────────────────────┐  │    │
│  │  │  AVAILABILITY ZONE B  │  │    │  │  AVAILABILITY ZONE B  │  │    │
│  │  │  (separate building,  │  │    │  │  (separate power,     │  │    │
│  │  │   power, network)     │  │    │  │   cooling, network)   │  │    │
│  │  └───────────────────────┘  │    │  └───────────────────────┘  │    │
│  └─────────────────────────────┘    └─────────────────────────────┘    │
│                                                                          │
│  Latency:                                                               │
│  ├── Within Node: ~0.1ms                                                │
│  ├── Within Cluster: ~0.5ms                                             │
│  ├── Within AZ: ~1ms                                                    │
│  ├── Cross-AZ (same region): ~2-5ms                                     │
│  └── Cross-Region: ~50-200ms                                            │
└─────────────────────────────────────────────────────────────────────────┘
```

**Definitions**:

| Term | Definition | Example |
|------|------------|---------|
| **Node** | Single machine (physical or virtual) running your software | EC2 instance, Kubernetes pod |
| **Cluster** | Group of nodes working together as one unit | Kubernetes cluster, Redis cluster |
| **Data Center** | Physical building with servers, power, cooling | AWS us-east-1a facility |
| **Availability Zone (AZ)** | Isolated location within a region (separate power, network) | us-east-1a, us-east-1b |
| **Region** | Geographic area containing multiple AZs | us-east-1 (N. Virginia), eu-west-1 (Ireland) |

**Python Example - Region-Aware Service**:

```python
from enum import Enum
from dataclasses import dataclass
import os

class Region(Enum):
    US_EAST = "us-east-1"
    US_WEST = "us-west-2"
    EU_WEST = "eu-west-1"
    AP_SOUTH = "ap-south-1"

@dataclass
class NodeInfo:
    node_id: str
    region: Region
    availability_zone: str
    cluster: str
    
    @classmethod
    def from_environment(cls):
        """Read node info from environment (set by orchestrator)"""
        return cls(
            node_id=os.getenv("NODE_ID", "unknown"),
            region=Region(os.getenv("AWS_REGION", "us-east-1")),
            availability_zone=os.getenv("AWS_AZ", "us-east-1a"),
            cluster=os.getenv("CLUSTER_NAME", "default")
        )

# Use in your service
node_info = NodeInfo.from_environment()

@app.get("/health")
async def health():
    return {
        "status": "healthy",
        "node": node_info.node_id,
        "region": node_info.region.value,
        "az": node_info.availability_zone
    }
```

### Replicas and Instances

```
┌─────────────────────────────────────────────────────────────┐
│                    SERVICE DEPLOYMENT                        │
│                                                              │
│    User Service (3 replicas for high availability)          │
│                                                              │
│    ┌──────────────┐  ┌──────────────┐  ┌──────────────┐    │
│    │   Replica 1  │  │   Replica 2  │  │   Replica 3  │    │
│    │   (Primary)  │  │   (Standby)  │  │   (Standby)  │    │
│    │              │  │              │  │              │    │
│    │   AZ: 1a     │  │   AZ: 1b     │  │   AZ: 1c     │    │
│    └──────────────┘  └──────────────┘  └──────────────┘    │
│           │                  │                  │            │
│           └──────────────────┴──────────────────┘            │
│                              │                               │
│                    ┌─────────▼─────────┐                    │
│                    │   Load Balancer   │                    │
│                    │   (distributes    │                    │
│                    │    traffic)       │                    │
│                    └───────────────────┘                    │
│                                                              │
│  Why 3 replicas across AZs?                                 │
│  - One AZ fails → 2 still running (66% capacity)           │
│  - Can handle rolling deployments                           │
│  - Odd number for leader election quorum                    │
└─────────────────────────────────────────────────────────────┘
```

---

## 3.2 Client-Server vs Peer-to-Peer

### Client-Server Architecture

The most common pattern you'll work with:

```
┌─────────────────────────────────────────────────────────────┐
│                    CLIENT-SERVER MODEL                       │
│                                                              │
│   CLIENTS (many)              SERVER (few)                  │
│                                                              │
│   ┌────────┐                 ┌──────────────┐              │
│   │ Browser│ ──── Request ──▶│              │              │
│   └────────┘ ◀── Response ───│              │              │
│                              │    Server    │              │
│   ┌────────┐                 │              │              │
│   │ Mobile │ ──── Request ──▶│  (Central    │              │
│   │  App   │ ◀── Response ───│   authority) │              │
│   └────────┘                 │              │              │
│                              │              │              │
│   ┌────────┐                 │              │              │
│   │ Another│ ──── Request ──▶│              │              │
│   │ Service│ ◀── Response ───│              │              │
│   └────────┘                 └──────────────┘              │
│                                                              │
│   Characteristics:                                          │
│   ├── Clients initiate communication                        │
│   ├── Server is always available (ideally)                  │
│   ├── Centralized control and data                          │
│   └── Easy to secure, manage, update                        │
└─────────────────────────────────────────────────────────────┘
```

```python
# Classic Client-Server Example

# SERVER (FastAPI)
from fastapi import FastAPI

app = FastAPI()

# Server maintains state/data
database = {"users": {}, "posts": {}}

@app.post("/users")
async def create_user(user: dict):
    user_id = len(database["users"]) + 1
    database["users"][user_id] = user
    return {"id": user_id, **user}

@app.get("/users/{user_id}")
async def get_user(user_id: int):
    return database["users"].get(user_id)

# CLIENT
import httpx

async def client_example():
    async with httpx.AsyncClient(base_url="http://server:8000") as client:
        # Client requests, server responds
        response = await client.post("/users", json={"name": "Alice"})
        user = response.json()
        
        response = await client.get(f"/users/{user['id']}")
        print(response.json())
```

### Peer-to-Peer Architecture

Each node is both client AND server:

```
┌─────────────────────────────────────────────────────────────┐
│                    PEER-TO-PEER MODEL                        │
│                                                              │
│         ┌────────┐                ┌────────┐               │
│         │ Peer A │◀──────────────▶│ Peer B │               │
│         │(client │                │(client │               │
│         │+server)│                │+server)│               │
│         └────┬───┘                └───┬────┘               │
│              │                        │                     │
│              │      ┌────────┐        │                     │
│              └─────▶│ Peer C │◀───────┘                     │
│                     │(client │                              │
│              ┌─────▶│+server)│◀───────┐                     │
│              │      └────────┘        │                     │
│              │                        │                     │
│         ┌────┴───┐                ┌───┴────┐               │
│         │ Peer D │◀──────────────▶│ Peer E │               │
│         └────────┘                └────────┘               │
│                                                              │
│   Characteristics:                                          │
│   ├── No central authority                                  │
│   ├── Each peer can serve data it has                       │
│   ├── Highly resilient (no single point of failure)        │
│   ├── Scales naturally as more peers join                   │
│   └── Hard to coordinate, eventual consistency              │
│                                                              │
│   Examples: BitTorrent, Bitcoin, IPFS                       │
└─────────────────────────────────────────────────────────────┘
```

```python
# Simple P2P Node Example
import asyncio
from typing import Dict, Set
import httpx

class P2PNode:
    def __init__(self, node_id: str, port: int):
        self.node_id = node_id
        self.port = port
        self.peers: Set[str] = set()  # Known peer addresses
        self.data: Dict[str, str] = {}  # Data this node holds
    
    async def join_network(self, bootstrap_peer: str):
        """Join network through a known peer"""
        async with httpx.AsyncClient() as client:
            # Get list of peers from bootstrap
            response = await client.get(f"{bootstrap_peer}/peers")
            existing_peers = response.json()["peers"]
            
            # Add all known peers
            self.peers.update(existing_peers)
            self.peers.add(bootstrap_peer)
            
            # Announce ourselves to all peers
            for peer in self.peers:
                await client.post(
                    f"{peer}/announce",
                    json={"peer": f"http://localhost:{self.port}"}
                )
    
    async def store(self, key: str, value: str):
        """Store data and replicate to peers"""
        self.data[key] = value
        
        # Replicate to all peers (simple broadcast)
        async with httpx.AsyncClient() as client:
            for peer in self.peers:
                try:
                    await client.post(
                        f"{peer}/replicate",
                        json={"key": key, "value": value}
                    )
                except httpx.RequestError:
                    pass  # Peer might be offline
    
    async def get(self, key: str) -> str:
        """Get data - check locally first, then ask peers"""
        if key in self.data:
            return self.data[key]
        
        # Ask peers
        async with httpx.AsyncClient() as client:
            for peer in self.peers:
                try:
                    response = await client.get(f"{peer}/data/{key}")
                    if response.status_code == 200:
                        return response.json()["value"]
                except httpx.RequestError:
                    continue
        
        raise KeyError(f"Key {key} not found in network")
```

### Comparison

| Aspect | Client-Server | Peer-to-Peer |
|--------|---------------|--------------|
| **Control** | Centralized | Decentralized |
| **Single Point of Failure** | Yes (server) | No |
| **Scalability** | Limited by server | Infinite (theoretically) |
| **Consistency** | Easier (one source of truth) | Harder (eventual) |
| **Security** | Easier to secure | Complex |
| **Management** | Simple | Complex |
| **Cost** | Server infrastructure | Distributed across peers |
| **Use Cases** | Web apps, APIs, databases | File sharing, crypto, CDN |

---

## 3.3 Synchronous vs Asynchronous Communication

### Synchronous (Blocking)

```
┌─────────────────────────────────────────────────────────────┐
│                SYNCHRONOUS COMMUNICATION                     │
│                                                              │
│   Service A                          Service B               │
│       │                                  │                   │
│       │────── Request ──────────────────▶│                   │
│       │                                  │                   │
│       │         (A is BLOCKED,           │                   │
│       │          waiting...)             │ Processing...     │
│       │                                  │                   │
│       │◀───── Response ─────────────────│                   │
│       │                                  │                   │
│       │ (Now A can continue)             │                   │
│       ▼                                  │                   │
│                                                              │
│   Characteristics:                                          │
│   ├── Caller waits for response                             │
│   ├── Simple to reason about                                │
│   ├── Tight coupling between services                       │
│   ├── Cascading failures possible                           │
│   └── Total latency = sum of all call latencies            │
└─────────────────────────────────────────────────────────────┘
```

**Python Example**:

```python
import httpx
import time

# SYNCHRONOUS - Each call blocks until complete
def get_user_profile_sync(user_id: int) -> dict:
    """Total time = time(user) + time(orders) + time(recommendations)"""
    
    start = time.time()
    
    # Call 1: Get user (blocks for ~100ms)
    user_response = httpx.get(f"http://user-service/users/{user_id}")
    user = user_response.json()
    print(f"User fetched: {time.time() - start:.0f}ms")
    
    # Call 2: Get orders (blocks for ~150ms) 
    orders_response = httpx.get(f"http://order-service/users/{user_id}/orders")
    orders = orders_response.json()
    print(f"Orders fetched: {time.time() - start:.0f}ms")
    
    # Call 3: Get recommendations (blocks for ~200ms)
    rec_response = httpx.get(f"http://rec-service/users/{user_id}/recommendations")
    recommendations = rec_response.json()
    print(f"Recommendations fetched: {time.time() - start:.0f}ms")
    
    # Total time: 100 + 150 + 200 = 450ms!
    print(f"Total time: {time.time() - start:.0f}ms")
    
    return {
        "user": user,
        "orders": orders,
        "recommendations": recommendations
    }
```

### Asynchronous (Non-Blocking)

```
┌─────────────────────────────────────────────────────────────┐
│               ASYNCHRONOUS COMMUNICATION                     │
│                                                              │
│   Pattern 1: Async/Await (concurrent requests)              │
│   ─────────────────────────────────────────────             │
│                                                              │
│   Service A          Service B        Service C             │
│       │                  │                │                  │
│       │─── Request 1 ───▶│                │                  │
│       │─── Request 2 ────────────────────▶│                  │
│       │                  │                │                  │
│       │  (A continues    │ Processing     │ Processing       │
│       │   other work)    │                │                  │
│       │                  │                │                  │
│       │◀── Response 1 ───│                │                  │
│       │◀── Response 2 ────────────────────│                  │
│       │                                                      │
│   Total time = max(request times), not sum!                 │
│                                                              │
│   Pattern 2: Message Queue (fire and forget)                │
│   ─────────────────────────────────────────                  │
│                                                              │
│   Producer              Queue              Consumer          │
│       │                  │                    │              │
│       │─── Message ─────▶│                    │              │
│       │    (returns      │                    │              │
│       │    immediately)  │                    │              │
│       │                  │─── Message ───────▶│              │
│       │                  │                    │ Processing   │
│       │                  │                    │   (later)    │
│       │                  │                    │              │
│   Producer doesn't wait for consumer!                       │
└─────────────────────────────────────────────────────────────┘
```

**Python Example - Concurrent Requests**:

```python
import asyncio
import httpx
import time

# ASYNCHRONOUS - All calls happen concurrently
async def get_user_profile_async(user_id: int) -> dict:
    """Total time = max(time(user), time(orders), time(recommendations))"""
    
    start = time.time()
    
    async with httpx.AsyncClient() as client:
        # Launch all requests concurrently (like Promise.all)
        user_task = client.get(f"http://user-service/users/{user_id}")
        orders_task = client.get(f"http://order-service/users/{user_id}/orders")
        rec_task = client.get(f"http://rec-service/users/{user_id}/recommendations")
        
        # Wait for all to complete
        user_response, orders_response, rec_response = await asyncio.gather(
            user_task, orders_task, rec_task
        )
    
    # Total time: max(100, 150, 200) = 200ms! (vs 450ms sync)
    print(f"Total time: {time.time() - start:.0f}ms")
    
    return {
        "user": user_response.json(),
        "orders": orders_response.json(),
        "recommendations": rec_response.json()
    }

# Run it
asyncio.run(get_user_profile_async(123))
```

**Python Example - Message Queue**:

```python
import asyncio
from dataclasses import dataclass
from typing import Callable, Any
import json

# Simple in-memory message queue (use Redis/Kafka in production)
class MessageQueue:
    def __init__(self):
        self.queues: dict[str, asyncio.Queue] = {}
    
    def get_queue(self, name: str) -> asyncio.Queue:
        if name not in self.queues:
            self.queues[name] = asyncio.Queue()
        return self.queues[name]
    
    async def publish(self, queue_name: str, message: dict):
        """Fire and forget - returns immediately"""
        queue = self.get_queue(queue_name)
        await queue.put(json.dumps(message))
        # Returns immediately! Doesn't wait for processing
    
    async def subscribe(self, queue_name: str, handler: Callable):
        """Process messages as they arrive"""
        queue = self.get_queue(queue_name)
        while True:
            message = await queue.get()
            await handler(json.loads(message))

# Usage
mq = MessageQueue()

# Producer - sends order and continues immediately
async def create_order(order_data: dict):
    # Save order to database
    order_id = await save_to_db(order_data)
    
    # Publish event - doesn't wait for email to be sent!
    await mq.publish("order_created", {
        "order_id": order_id,
        "user_email": order_data["email"],
        "total": order_data["total"]
    })
    
    # Return immediately - user gets fast response
    return {"order_id": order_id, "status": "created"}

# Consumer - processes emails in background
async def email_handler(message: dict):
    """Runs separately, whenever it can"""
    await send_email(
        to=message["user_email"],
        subject=f"Order {message['order_id']} confirmed",
        body=f"Your order total: ${message['total']}"
    )

# Start consumer in background
asyncio.create_task(mq.subscribe("order_created", email_handler))
```

### Comparison

| Aspect | Synchronous | Asynchronous |
|--------|-------------|--------------|
| **Response Time** | Sum of all calls | Max of concurrent calls |
| **Coupling** | Tight | Loose |
| **Error Handling** | Immediate | Delayed/complex |
| **Debugging** | Easier | Harder |
| **Scalability** | Limited | Better |
| **Use Cases** | Simple CRUD, critical paths | Background jobs, notifications |

---

## 3.4 Stateful vs Stateless Services

### Stateless Services

```
┌─────────────────────────────────────────────────────────────┐
│                    STATELESS SERVICE                         │
│                                                              │
│   Request 1 ──▶ ┌──────────┐                                │
│   (user: 123)   │ Server A │  Each request is independent   │
│                 └──────────┘  Server has NO memory of        │
│                               previous requests              │
│   Request 2 ──▶ ┌──────────┐                                │
│   (user: 123)   │ Server B │  Can go to ANY server          │
│                 └──────────┘  All state in external store    │
│                                                              │
│   Request 3 ──▶ ┌──────────┐                                │
│   (user: 123)   │ Server C │                                │
│                 └──────────┘                                │
│                      │                                       │
│                      ▼                                       │
│               ┌──────────────┐                              │
│               │   External   │  State stored externally:    │
│               │    State     │  - Database                  │
│               │    Store     │  - Redis                     │
│               └──────────────┘  - JWT tokens                │
│                                                              │
│   Benefits:                                                 │
│   ├── Easy to scale (just add more servers)                 │
│   ├── Easy to deploy (replace any server)                   │
│   ├── No sticky sessions needed                             │
│   └── Resilient to server failures                          │
└─────────────────────────────────────────────────────────────┘
```

**Python Example**:

```python
from fastapi import FastAPI, Depends, HTTPException
from fastapi.security import HTTPBearer
import jwt
import redis.asyncio as redis

app = FastAPI()
security = HTTPBearer()

# External state stores
redis_client = redis.Redis(host='redis', decode_responses=True)

# Stateless authentication using JWT
async def get_current_user(token = Depends(security)):
    """
    State is in the TOKEN, not the server!
    Any server can validate this token.
    """
    try:
        payload = jwt.decode(token.credentials, SECRET_KEY, algorithms=["HS256"])
        return payload
    except jwt.InvalidTokenError:
        raise HTTPException(status_code=401)

# Stateless endpoint - can run on any server
@app.get("/profile")
async def get_profile(user = Depends(get_current_user)):
    # Get data from external store (database)
    profile = await redis_client.hgetall(f"user:{user['sub']}")
    return profile

# Stateless shopping cart - stored in Redis, not server memory
@app.post("/cart/items")
async def add_to_cart(user = Depends(get_current_user), item: dict = None):
    cart_key = f"cart:{user['sub']}"
    
    # State in Redis, not server!
    await redis_client.hset(cart_key, item['product_id'], item['quantity'])
    await redis_client.expire(cart_key, 86400)  # Expire in 24h
    
    return {"status": "added"}

@app.get("/cart")
async def get_cart(user = Depends(get_current_user)):
    cart_key = f"cart:{user['sub']}"
    cart = await redis_client.hgetall(cart_key)
    return {"items": cart}
```

### Stateful Services

```
┌─────────────────────────────────────────────────────────────┐
│                    STATEFUL SERVICE                          │
│                                                              │
│   Request 1 ──▶ ┌──────────┐                                │
│   (user: 123)   │ Server A │  Server maintains state        │
│             ┌──▶│ [cache]  │  in memory                     │
│             │   │ [session]│                                │
│   Request 2 ─┘  │ [ws conn]│  Requests from same user       │
│   (user: 123)   └──────────┘  MUST go to same server        │
│                      │                                       │
│                      │ Sticky Session / Affinity            │
│                                                              │
│   When to use stateful:                                     │
│   ├── WebSocket connections (connection state)              │
│   ├── In-memory caching (hot data)                          │
│   ├── Gaming servers (real-time state)                      │
│   ├── Database servers (data locality)                      │
│   └── ML inference (loaded models)                          │
│                                                              │
│   Challenges:                                               │
│   ├── Scaling is harder (state must move with server)       │
│   ├── Failover is complex (state must be replicated)        │
│   ├── Load balancing needs sticky sessions                  │
│   └── Deployment requires careful orchestration             │
└─────────────────────────────────────────────────────────────┘
```

**Python Example - Stateful WebSocket**:

```python
from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from typing import Dict, Set
import json

app = FastAPI()

# STATEFUL: Server maintains active connections in memory
class ConnectionManager:
    def __init__(self):
        # State lives in THIS server's memory
        self.active_connections: Dict[str, WebSocket] = {}
        self.rooms: Dict[str, Set[str]] = {}  # room -> user_ids
    
    async def connect(self, user_id: str, websocket: WebSocket):
        await websocket.accept()
        self.active_connections[user_id] = websocket
    
    def disconnect(self, user_id: str):
        if user_id in self.active_connections:
            del self.active_connections[user_id]
    
    async def send_to_user(self, user_id: str, message: dict):
        # Can only send if user is connected to THIS server!
        if user_id in self.active_connections:
            await self.active_connections[user_id].send_json(message)
    
    async def broadcast_to_room(self, room_id: str, message: dict):
        if room_id in self.rooms:
            for user_id in self.rooms[room_id]:
                await self.send_to_user(user_id, message)

manager = ConnectionManager()

@app.websocket("/ws/{user_id}")
async def websocket_endpoint(websocket: WebSocket, user_id: str):
    await manager.connect(user_id, websocket)
    try:
        while True:
            data = await websocket.receive_json()
            
            if data["type"] == "join_room":
                room_id = data["room_id"]
                if room_id not in manager.rooms:
                    manager.rooms[room_id] = set()
                manager.rooms[room_id].add(user_id)
                
            elif data["type"] == "message":
                await manager.broadcast_to_room(
                    data["room_id"],
                    {"from": user_id, "text": data["text"]}
                )
                
    except WebSocketDisconnect:
        manager.disconnect(user_id)
```

### Scaling Stateful Services

When you must use stateful services, here's how to scale:

```python
# Solution 1: Consistent Hashing for routing
# Same user always goes to same server

import hashlib

class ConsistentRouter:
    def __init__(self, servers: list[str]):
        self.servers = sorted(servers)
        self.ring = {}
        
        for server in servers:
            # Each server gets multiple points on the ring
            for i in range(100):  # Virtual nodes
                key = self._hash(f"{server}:{i}")
                self.ring[key] = server
        
        self.sorted_keys = sorted(self.ring.keys())
    
    def _hash(self, key: str) -> int:
        return int(hashlib.md5(key.encode()).hexdigest(), 16)
    
    def get_server(self, user_id: str) -> str:
        """Always returns same server for same user_id"""
        if not self.ring:
            return None
            
        hash_key = self._hash(user_id)
        
        for key in self.sorted_keys:
            if key >= hash_key:
                return self.ring[key]
        
        return self.ring[self.sorted_keys[0]]

# Usage
router = ConsistentRouter(["server1", "server2", "server3"])
server = router.get_server("user_123")  # Always returns same server


# Solution 2: State replication with Redis Pub/Sub
# Broadcast events to all servers

import redis.asyncio as redis

class DistributedConnectionManager:
    def __init__(self, server_id: str):
        self.server_id = server_id
        self.local_connections: Dict[str, WebSocket] = {}
        self.redis = redis.Redis()
        self.pubsub = self.redis.pubsub()
    
    async def start(self):
        """Subscribe to cross-server messages"""
        await self.pubsub.subscribe("messages")
        asyncio.create_task(self._listen_messages())
    
    async def _listen_messages(self):
        async for message in self.pubsub.listen():
            if message["type"] == "message":
                data = json.loads(message["data"])
                # Check if target user is connected to THIS server
                if data["target_user"] in self.local_connections:
                    await self.local_connections[data["target_user"]].send_json(
                        data["payload"]
                    )
    
    async def send_to_user(self, user_id: str, message: dict):
        # Broadcast to all servers via Redis
        # The server with the connection will deliver it
        await self.redis.publish("messages", json.dumps({
            "target_user": user_id,
            "payload": message
        }))
```

### Decision Guide

```
┌──────────────────────────────────────────────────────────────┐
│          STATEFUL vs STATELESS DECISION TREE                 │
│                                                              │
│  Start Here: Does your service need to remember              │
│              something between requests?                     │
│                     │                                         │
│         ┌───── NO ──┴── YES ─────┐                           │
│         │                        │                            │
│         ▼                        ▼                            │
│    STATELESS                Can this state                   │
│    (preferred)              be stored externally?            │
│                                   │                           │
│                       ┌─── YES ───┴─── NO ────┐              │
│                       │                        │              │
│                       ▼                        ▼              │
│                  STATELESS              Is real-time          │
│                  with external          critical (<10ms)?     │
│                  store (Redis)                │               │
│                                    ┌── YES ──┴── NO ─┐       │
│                                    │                  │       │
│                                    ▼                  ▼       │
│                               STATEFUL           STATELESS    │
│                               (with care!)       (async)      │
│                                                               │
│  Examples:                                                   │
│  ├── User profiles → STATELESS (use database)               │
│  ├── Shopping cart → STATELESS (use Redis)                  │
│  ├── WebSocket chat → STATEFUL (connection in memory)       │
│  ├── Game server → STATEFUL (real-time state)               │
│  └── API server → STATELESS (JWT + database)                │
└──────────────────────────────────────────────────────────────┘
```

---

## 3.5 Throughput, Latency, and Availability

### Throughput

**Definition**: Number of operations completed per unit time

```
┌─────────────────────────────────────────────────────────────┐
│                     THROUGHPUT                               │
│                                                              │
│   Measured in:                                              │
│   ├── Requests per second (RPS)                             │
│   ├── Transactions per second (TPS)                         │
│   ├── Queries per second (QPS)                              │
│   └── Messages per second                                   │
│                                                              │
│   Example:                                                  │
│   ┌────────────────────────────────────────────┐           │
│   │     1 second                                │           │
│   │  ┌──┬──┬──┬──┬──┬──┬──┬──┬──┬──┐          │           │
│   │  │R1│R2│R3│R4│R5│R6│R7│R8│R9│10│          │           │
│   │  └──┴──┴──┴──┴──┴──┴──┴──┴──┴──┘          │           │
│   │  Throughput = 10 RPS                       │           │
│   └────────────────────────────────────────────┘           │
│                                                              │
│   Bottlenecks:                                              │
│   ├── CPU (processing)                                      │
│   ├── Memory (data structures)                              │
│   ├── Network (bandwidth)                                   │
│   ├── Disk I/O (database)                                   │
│   └── External dependencies (other services)                │
└─────────────────────────────────────────────────────────────┘
```

### Latency

**Definition**: Time from request start to response received

```
┌─────────────────────────────────────────────────────────────┐
│                      LATENCY                                 │
│                                                              │
│   Request Lifecycle:                                        │
│                                                              │
│   Client ──────────────────────────────────────▶ Server     │
│     │                                              │         │
│     │ Network latency (1)                          │         │
│     │ ─────────────────────────────────────────▶   │         │
│     │                                              │         │
│     │                          Processing time (2) │         │
│     │                          ┌─────────────────┐ │         │
│     │                          │ Parse request   │ │         │
│     │                          │ Query database  │ │         │
│     │                          │ Build response  │ │         │
│     │                          └─────────────────┘ │         │
│     │                                              │         │
│     │ Network latency (3)                          │         │
│     │ ◀─────────────────────────────────────────   │         │
│     │                                              │         │
│                                                              │
│   Total Latency = (1) + (2) + (3)                           │
│                                                              │
│   Key Metrics:                                              │
│   ├── Average (mean): Misleading! Hides outliers            │
│   ├── Median (p50): 50% of requests faster than this        │
│   ├── p95: 95% of requests faster (typical SLA)             │
│   ├── p99: 99% of requests faster (tail latency)            │
│   └── p99.9: 99.9% faster (worst case for users)            │
└─────────────────────────────────────────────────────────────┘
```

**Python Example - Measuring Latency**:

```python
import time
import statistics
from collections import defaultdict
from contextlib import contextmanager
import asyncio

class LatencyTracker:
    def __init__(self):
        self.latencies = defaultdict(list)
    
    @contextmanager
    def track(self, operation: str):
        start = time.perf_counter()
        try:
            yield
        finally:
            elapsed_ms = (time.perf_counter() - start) * 1000
            self.latencies[operation].append(elapsed_ms)
    
    def get_stats(self, operation: str) -> dict:
        data = self.latencies[operation]
        if not data:
            return {}
        
        sorted_data = sorted(data)
        n = len(sorted_data)
        
        return {
            "count": n,
            "mean": statistics.mean(data),
            "median": statistics.median(data),
            "p95": sorted_data[int(n * 0.95)] if n >= 20 else None,
            "p99": sorted_data[int(n * 0.99)] if n >= 100 else None,
            "min": min(data),
            "max": max(data),
        }

# Usage in FastAPI
from fastapi import FastAPI, Request

app = FastAPI()
tracker = LatencyTracker()

@app.middleware("http")
async def track_latency(request: Request, call_next):
    with tracker.track(f"{request.method} {request.url.path}"):
        response = await call_next(request)
    return response

@app.get("/metrics/latency")
async def get_latency_metrics():
    return {
        operation: tracker.get_stats(operation)
        for operation in tracker.latencies
    }
```

### Availability

**Definition**: Percentage of time the system is operational

```
┌─────────────────────────────────────────────────────────────┐
│                    AVAILABILITY                              │
│                                                              │
│   Formula:                                                  │
│   Availability = Uptime / (Uptime + Downtime) × 100%        │
│                                                              │
│   "Nines" of Availability:                                  │
│   ┌────────────────┬──────────────┬────────────────────┐   │
│   │ Availability   │ Downtime/Year│ Downtime/Month     │   │
│   ├────────────────┼──────────────┼────────────────────┤   │
│   │ 99% (2 nines)  │ 3.65 days    │ 7.31 hours         │   │
│   │ 99.9% (3 nines)│ 8.76 hours   │ 43.8 minutes       │   │
│   │ 99.95%         │ 4.38 hours   │ 21.9 minutes       │   │
│   │ 99.99% (4 9s)  │ 52.6 minutes │ 4.38 minutes       │   │
│   │ 99.999% (5 9s) │ 5.26 minutes │ 26.3 seconds       │   │
│   └────────────────┴──────────────┴────────────────────┘   │
│                                                              │
│   Calculating System Availability:                          │
│                                                              │
│   Components in SERIES (all must work):                     │
│   A ──▶ B ──▶ C                                             │
│   System = A × B × C                                        │
│   Example: 99.9% × 99.9% × 99.9% = 99.7%                   │
│                                                              │
│   Components in PARALLEL (any can work):                    │
│   ┌─ A ─┐                                                   │
│   │     │                                                    │
│   ├─ B ─┤                                                   │
│   │     │                                                    │
│   └─ C ─┘                                                   │
│   System = 1 - (1-A) × (1-B) × (1-C)                       │
│   Example: 1 - (0.001)³ = 99.9999999%                      │
└─────────────────────────────────────────────────────────────┘
```

**Python Example - Calculating Availability**:

```python
from dataclasses import dataclass
from typing import List

@dataclass
class Component:
    name: str
    availability: float  # 0.0 to 1.0

def series_availability(components: List[Component]) -> float:
    """All components must work (multiply availabilities)"""
    result = 1.0
    for c in components:
        result *= c.availability
    return result

def parallel_availability(components: List[Component]) -> float:
    """Any component can work (1 - all fail probability)"""
    all_fail_probability = 1.0
    for c in components:
        all_fail_probability *= (1 - c.availability)
    return 1 - all_fail_probability

# Real-world example: Web application
def calculate_system_availability():
    # Series: Request must go through all these
    load_balancer = Component("LB", 0.9999)
    api_server = Component("API", 0.999)
    database = Component("DB", 0.999)
    
    # But we have 3 replicas of each (parallel)
    api_cluster = parallel_availability([
        Component("API-1", 0.999),
        Component("API-2", 0.999),
        Component("API-3", 0.999),
    ])
    
    db_cluster = parallel_availability([
        Component("DB-Primary", 0.999),
        Component("DB-Replica-1", 0.999),
        Component("DB-Replica-2", 0.999),
    ])
    
    # Overall system (series of clusters)
    system_availability = (
        load_balancer.availability * 
        api_cluster * 
        db_cluster
    )
    
    print(f"Single API server: {0.999:.4%}")
    print(f"API cluster (3 replicas): {api_cluster:.6%}")
    print(f"DB cluster (3 replicas): {db_cluster:.6%}")
    print(f"System availability: {system_availability:.4%}")
    
    # Calculate downtime
    yearly_downtime_minutes = (1 - system_availability) * 365 * 24 * 60
    print(f"Expected downtime: {yearly_downtime_minutes:.1f} minutes/year")

calculate_system_availability()
# Output:
# Single API server: 99.9000%
# API cluster (3 replicas): 99.999900%
# DB cluster (3 replicas): 99.999900%
# System availability: 99.9897%
# Expected downtime: 54.2 minutes/year
```

### The Relationship

```
┌─────────────────────────────────────────────────────────────┐
│      THROUGHPUT vs LATENCY vs AVAILABILITY                   │
│                                                              │
│   These often conflict! You must make trade-offs:           │
│                                                              │
│   Higher Throughput ←→ Higher Latency                       │
│   ─────────────────────────────────────                     │
│   Processing more requests can increase queue wait time     │
│                                                              │
│   ┌─────────────────────────────────────────────────┐      │
│   │ Throughput                                       │      │
│   │     ▲                                            │      │
│   │     │            ┌────────────────┐             │      │
│   │     │           /                  \            │      │
│   │     │         /                      \          │      │
│   │     │       /                          \        │      │
│   │     │     /                              \      │      │
│   │     └────────────────────────────────────────▶  │      │
│   │                Concurrent Requests      Latency │      │
│   └─────────────────────────────────────────────────┘      │
│   Sweet spot is in the middle!                              │
│                                                              │
│   Higher Availability ←→ Higher Cost                        │
│   ─────────────────────────────────────                     │
│   More replicas = more hardware = more money                │
│                                                              │
│   Lower Latency ←→ Lower Availability                       │
│   ──────────────────────────────────                        │
│   Synchronous replication = lower latency but single point  │
│   Async replication = higher availability but data lag      │
└─────────────────────────────────────────────────────────────┘
```

---

## 3.6 Idempotency

**Definition**: An operation is idempotent if performing it multiple times has the same effect as performing it once.

```
┌─────────────────────────────────────────────────────────────┐
│                     IDEMPOTENCY                              │
│                                                              │
│   IDEMPOTENT (safe to retry):                               │
│   ├── GET /users/123        → Always returns same user      │
│   ├── PUT /users/123        → Sets user to specific state   │
│   ├── DELETE /users/123     → User ends up deleted          │
│   └── Set balance to $100   → Balance is always $100        │
│                                                              │
│   NOT IDEMPOTENT (dangerous to retry):                      │
│   ├── POST /users           → Creates duplicate users!      │
│   ├── POST /orders          → Multiple orders created!      │
│   └── Add $100 to balance   → Keeps adding $100!            │
│                                                              │
│   Why it matters in distributed systems:                    │
│   ─────────────────────────────────────                     │
│                                                              │
│   Client ─── Request ───▶ Server                            │
│      │                        │                              │
│      │    (server processes)  │                              │
│      │                        │                              │
│      │ ◀─── Response ─── LOST IN NETWORK!                   │
│      │                                                       │
│      │    (client doesn't know if it worked)                │
│      │                                                       │
│      │ ─── Retry Request ───▶ Server                        │
│      │                        │                              │
│      │    If NOT idempotent: DUPLICATE OPERATION!           │
│      │    If idempotent: Same result, no problem!           │
└─────────────────────────────────────────────────────────────┘
```

**Python Implementation**:

```python
from fastapi import FastAPI, Header, HTTPException
from pydantic import BaseModel
import redis.asyncio as redis
import json
import hashlib
from typing import Optional

app = FastAPI()
redis_client = redis.Redis(host='redis', decode_responses=True)

class PaymentRequest(BaseModel):
    user_id: int
    amount: float
    currency: str

class PaymentResponse(BaseModel):
    payment_id: str
    status: str
    amount: float

# IDEMPOTENT payment endpoint
@app.post("/payments", response_model=PaymentResponse)
async def create_payment(
    payment: PaymentRequest,
    idempotency_key: str = Header(..., alias="Idempotency-Key")
):
    """
    Idempotency-Key ensures same request returns same response.
    Client generates unique key (usually UUID) for each logical operation.
    """
    
    cache_key = f"payment:idempotency:{idempotency_key}"
    
    # Step 1: Check if we've seen this request before
    cached_response = await redis_client.get(cache_key)
    if cached_response:
        # Return exact same response as before
        return PaymentResponse(**json.loads(cached_response))
    
    # Step 2: Check if request is in progress (prevent concurrent duplicates)
    lock_key = f"payment:lock:{idempotency_key}"
    lock_acquired = await redis_client.set(lock_key, "1", nx=True, ex=30)
    
    if not lock_acquired:
        raise HTTPException(
            status_code=409,
            detail="Request already in progress"
        )
    
    try:
        # Step 3: Process the payment
        payment_id = await process_payment(payment)
        
        response = PaymentResponse(
            payment_id=payment_id,
            status="completed",
            amount=payment.amount
        )
        
        # Step 4: Cache the response (24 hour TTL)
        await redis_client.setex(
            cache_key,
            86400,
            json.dumps(response.model_dump())
        )
        
        return response
        
    finally:
        # Always release the lock
        await redis_client.delete(lock_key)

async def process_payment(payment: PaymentRequest) -> str:
    """Actually process the payment (charge card, etc.)"""
    # In real implementation: call payment gateway
    payment_id = hashlib.sha256(
        f"{payment.user_id}:{payment.amount}:{payment.currency}".encode()
    ).hexdigest()[:16]
    return payment_id


# Usage from client:
"""
import httpx
import uuid

async def make_payment(user_id: int, amount: float):
    # Generate idempotency key once per logical payment
    idempotency_key = str(uuid.uuid4())
    
    async with httpx.AsyncClient() as client:
        for attempt in range(3):  # Retry up to 3 times
            try:
                response = await client.post(
                    "http://api/payments",
                    json={"user_id": user_id, "amount": amount, "currency": "USD"},
                    headers={"Idempotency-Key": idempotency_key},
                    timeout=5.0
                )
                return response.json()
            except httpx.TimeoutException:
                continue  # Safe to retry with same idempotency key!
    
    raise Exception("Payment failed after 3 attempts")
"""
```

---

## Summary: Key Terminology Cheat Sheet

```
┌─────────────────────────────────────────────────────────────┐
│                   QUICK REFERENCE                            │
│                                                              │
│   INFRASTRUCTURE                                            │
│   ├── Node: Single machine                                  │
│   ├── Cluster: Group of nodes                               │
│   ├── Region: Geographic area                               │
│   └── AZ: Isolated location within region                   │
│                                                              │
│   ARCHITECTURE                                              │
│   ├── Client-Server: Centralized (most common)              │
│   └── Peer-to-Peer: Decentralized (BitTorrent)             │
│                                                              │
│   COMMUNICATION                                             │
│   ├── Synchronous: Wait for response                        │
│   └── Asynchronous: Fire and forget                         │
│                                                              │
│   STATE                                                     │
│   ├── Stateless: No server memory (preferred)               │
│   └── Stateful: Server maintains state                      │
│                                                              │
│   METRICS                                                   │
│   ├── Throughput: Operations per second                     │
│   ├── Latency: Time per operation (focus on p99!)           │
│   └── Availability: Uptime percentage (count the 9s)        │
│                                                              │
│   PATTERNS                                                  │
│   └── Idempotency: Safe to retry                            │
│                                                              │
│   RULES OF THUMB                                            │
│   ├── Prefer stateless services                             │
│   ├── Make non-GET operations idempotent                    │
│   ├── Measure p99 latency, not average                      │
│   ├── Design for 99.9% availability minimum                 │
│   └── Trade-offs are everywhere!                            │
└─────────────────────────────────────────────────────────────┘
```

---

## Practice Exercises

1. **Build a stateless service** that stores session data in Redis instead of memory
2. **Implement idempotency** for a POST endpoint using Redis
3. **Add latency tracking** middleware to your FastAPI app
4. **Calculate the availability** of a system with 3 API servers (99.9% each) and 2 databases (99.95% each)
5. **Convert a synchronous** endpoint to asynchronous using `asyncio.gather`

**Next Chapter**: Communication patterns - REST, gRPC, message queues, and more!
