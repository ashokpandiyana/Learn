# Chapter 12: Consensus Algorithms

> **Goal**: Understand how distributed systems agree on values despite failures

---

## 12.1 The Consensus Problem

```
┌─────────────────────────────────────────────────────────────┐
│              THE CONSENSUS PROBLEM                           │
│                                                              │
│  Question: How can multiple nodes agree on a single value   │
│            when some nodes might fail?                       │
│                                                              │
│                                                              │
│  Example: Leader Election                                   │
│  ────────────────────────                                   │
│                                                              │
│  Node A: "I should be leader!"                              │
│  Node B: "No, I should be leader!"                          │
│  Node C: "Wait, who's the leader?"                          │
│                                                              │
│  They must ALL agree on ONE leader.                         │
│                                                              │
│                                                              │
│  Requirements for Consensus:                                │
│  ───────────────────────────                                │
│                                                              │
│  1. AGREEMENT                                               │
│     All non-faulty nodes decide the same value              │
│                                                              │
│  2. VALIDITY                                                │
│     The decided value was proposed by some node             │
│     (can't decide on arbitrary values)                      │
│                                                              │
│  3. TERMINATION                                             │
│     All non-faulty nodes eventually decide                  │
│     (can't get stuck forever)                               │
│                                                              │
│                                                              │
│  Use Cases:                                                 │
│  ──────────                                                 │
│  ├── Leader election                                        │
│  ├── Distributed locks                                      │
│  ├── Atomic broadcast                                       │
│  ├── State machine replication                              │
│  └── Distributed transactions                               │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

---

## 12.2 FLP Impossibility

```
┌─────────────────────────────────────────────────────────────┐
│              FLP IMPOSSIBILITY THEOREM                       │
│                                                              │
│  Fischer, Lynch, and Patterson (1985) proved:               │
│                                                              │
│  "In a purely asynchronous system where at least one        │
│   process can fail, there is NO algorithm that guarantees   │
│   consensus will be reached."                               │
│                                                              │
│                                                              │
│  Why?                                                       │
│  ─────                                                      │
│  In a purely async system, you can't tell the difference   │
│  between:                                                   │
│  • A very slow node                                         │
│  • A crashed node                                           │
│                                                              │
│  So you can't safely proceed without a node that might     │
│  just be slow!                                              │
│                                                              │
│                                                              │
│  How Real Systems Work Around This:                         │
│  ───────────────────────────────────                        │
│                                                              │
│  1. TIMEOUTS (Partial Synchrony)                           │
│     Assume messages arrive within some bound                │
│     "Eventually synchronous" model                          │
│                                                              │
│  2. RANDOMIZATION                                           │
│     Use random delays to break symmetry                     │
│     Consensus reached with high probability                 │
│                                                              │
│  3. FAILURE DETECTORS                                       │
│     Imperfect but useful crash detection                    │
│                                                              │
│  Raft and Paxos use timeouts (partial synchrony)           │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

---

## 12.3 Paxos (Brief Overview)

```
┌─────────────────────────────────────────────────────────────┐
│                     PAXOS                                    │
│                                                              │
│  Invented by Leslie Lamport (1989).                        │
│  Correct but notoriously difficult to understand.          │
│                                                              │
│  "The Paxos algorithm, when presented in plain English,    │
│   is very simple." - Lamport                                │
│                                                              │
│  "I have spent a significant fraction of my research       │
│   life studying Paxos. I still find it very confusing."    │
│   - Various researchers                                     │
│                                                              │
│                                                              │
│  Roles:                                                     │
│  ──────                                                     │
│  • Proposers: Propose values                                │
│  • Acceptors: Vote on proposals                             │
│  • Learners: Learn the decided value                        │
│                                                              │
│  (A node can play multiple roles)                          │
│                                                              │
│                                                              │
│  Two Phases:                                                │
│  ───────────                                                │
│                                                              │
│  Phase 1 (Prepare):                                         │
│  Proposer: "I want to propose with number N"               │
│  Acceptors: "OK, I promise not to accept anything < N"     │
│                                                              │
│  Phase 2 (Accept):                                          │
│  Proposer: "Accept value V with number N"                  │
│  Acceptors: "Accepted" (if promise still valid)            │
│                                                              │
│                                                              │
│  We'll focus on RAFT instead - easier to understand!       │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

---

## 12.4 Raft Consensus Algorithm

### 12.4.1 Raft Overview

```
┌─────────────────────────────────────────────────────────────┐
│                      RAFT                                    │
│                                                              │
│  Designed for UNDERSTANDABILITY (2014).                    │
│  Equivalent to Paxos but much easier to implement.         │
│                                                              │
│  Used by: etcd, Consul, CockroachDB, TiKV                  │
│                                                              │
│                                                              │
│  Key Idea: Decompose consensus into subproblems            │
│  ───────────────────────────────────────────────            │
│                                                              │
│  1. LEADER ELECTION                                         │
│     Elect one leader among nodes                            │
│                                                              │
│  2. LOG REPLICATION                                         │
│     Leader replicates log entries to followers              │
│                                                              │
│  3. SAFETY                                                  │
│     Ensure consistency despite failures                     │
│                                                              │
│                                                              │
│  Node States:                                               │
│  ────────────                                               │
│                                                              │
│  ┌──────────┐    timeout    ┌───────────┐    majority     │
│  │ FOLLOWER │──────────────▶│ CANDIDATE │────────────────▶ │
│  └──────────┘               └───────────┘                  │
│       ▲                          │                          │
│       │         loses election   │                          │
│       │          or higher term  │     ┌──────────┐        │
│       └──────────────────────────┘     │  LEADER  │        │
│       │                                └────┬─────┘        │
│       │         discovers leader            │               │
│       └─────────────────────────────────────┘               │
│                  or higher term                             │
│                                                              │
│                                                              │
│  Terms:                                                     │
│  ──────                                                     │
│  Time is divided into TERMS (like epochs)                  │
│  Each term has at most ONE leader                          │
│  Terms are numbered: 1, 2, 3, ...                          │
│                                                              │
│  Term 1        Term 2        Term 3                        │
│  ┌────────────┬────────────┬────────────                   │
│  │  Leader A  │  Leader B  │  Leader C  ...               │
│  └────────────┴────────────┴────────────                   │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

### 12.4.2 Leader Election

```
┌─────────────────────────────────────────────────────────────┐
│              RAFT LEADER ELECTION                            │
│                                                              │
│  Step 1: Election Timeout                                   │
│  ────────────────────────                                   │
│  Each follower has random timeout (150-300ms)              │
│  If no heartbeat from leader → start election              │
│                                                              │
│                                                              │
│  Step 2: Become Candidate                                   │
│  ────────────────────────                                   │
│  • Increment current term                                   │
│  • Vote for self                                            │
│  • Send RequestVote to all other nodes                      │
│                                                              │
│                                                              │
│  Step 3: Collect Votes                                      │
│  ─────────────────────                                      │
│  Other nodes vote YES if:                                   │
│  • Candidate's term ≥ their term                           │
│  • Haven't voted for someone else this term                │
│  • Candidate's log is at least as up-to-date               │
│                                                              │
│                                                              │
│  Step 4: Become Leader (or not)                            │
│  ──────────────────────────────                             │
│  • Majority votes → Become leader, send heartbeats         │
│  • Discover higher term → Become follower                  │
│  • Timeout without majority → New election                  │
│                                                              │
│                                                              │
│  Example Election:                                          │
│  ─────────────────                                          │
│                                                              │
│  Time    Node A          Node B          Node C             │
│  ────────────────────────────────────────────────           │
│    0     Follower        Follower        Follower           │
│          (timeout=200)   (timeout=150)   (timeout=180)      │
│                                                              │
│  150     Follower        Candidate!      Follower           │
│                          term=1                              │
│                          vote for self                       │
│                                                              │
│  151     Receives        Sends           Receives           │
│          RequestVote     RequestVote     RequestVote        │
│                                                              │
│  152     Votes YES       Waiting...      Votes YES          │
│                                                              │
│  153     Follower        LEADER!         Follower           │
│          (term=1)        (got 3 votes)   (term=1)           │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

### 12.4.3 Log Replication

```
┌─────────────────────────────────────────────────────────────┐
│              RAFT LOG REPLICATION                            │
│                                                              │
│  The log is an ordered sequence of commands.                │
│  Leader receives commands, replicates to followers.         │
│                                                              │
│                                                              │
│  Log Structure:                                             │
│  ──────────────                                             │
│                                                              │
│  Index:   1        2        3        4        5             │
│         ┌────┐   ┌────┐   ┌────┐   ┌────┐   ┌────┐        │
│  Leader │x=1 │   │y=2 │   │x=3 │   │z=5 │   │y=1 │        │
│         │t=1 │   │t=1 │   │t=2 │   │t=2 │   │t=2 │        │
│         └────┘   └────┘   └────┘   └────┘   └────┘        │
│                                        ▲                    │
│                                   commitIndex               │
│                                                              │
│  Each entry has:                                            │
│  • Command (e.g., "x=1")                                    │
│  • Term when created                                        │
│  • Index position                                           │
│                                                              │
│                                                              │
│  Replication Flow:                                          │
│  ─────────────────                                          │
│                                                              │
│  1. Client sends command to leader                          │
│  2. Leader appends to its log                               │
│  3. Leader sends AppendEntries to followers                 │
│  4. Followers append to their logs, respond                 │
│  5. Leader commits when MAJORITY have entry                 │
│  6. Leader responds to client                               │
│  7. Leader notifies followers of commit                     │
│                                                              │
│                                                              │
│  Commit Rule:                                               │
│  ────────────                                               │
│  Entry is COMMITTED when:                                   │
│  • Stored on majority of servers                            │
│  • Leader has committed entries up to current term          │
│                                                              │
│  Committed entries are DURABLE and will never be lost!     │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

### 12.4.4 Raft Implementation

```python
# raft.py
import asyncio
import random
from enum import Enum
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Any, Callable
import time

class NodeState(Enum):
    FOLLOWER = "follower"
    CANDIDATE = "candidate"
    LEADER = "leader"

@dataclass
class LogEntry:
    term: int
    index: int
    command: Any

@dataclass 
class RequestVoteRequest:
    term: int
    candidate_id: str
    last_log_index: int
    last_log_term: int

@dataclass
class RequestVoteResponse:
    term: int
    vote_granted: bool

@dataclass
class AppendEntriesRequest:
    term: int
    leader_id: str
    prev_log_index: int
    prev_log_term: int
    entries: List[LogEntry]
    leader_commit: int

@dataclass
class AppendEntriesResponse:
    term: int
    success: bool
    match_index: int = 0

class RaftNode:
    """
    Raft consensus node implementation.
    """
    
    def __init__(
        self,
        node_id: str,
        peers: List[str],
        apply_callback: Callable[[Any], None] = None
    ):
        self.node_id = node_id
        self.peers = peers
        self.apply_callback = apply_callback or (lambda x: None)
        
        # Persistent state
        self.current_term = 0
        self.voted_for: Optional[str] = None
        self.log: List[LogEntry] = []
        
        # Volatile state
        self.state = NodeState.FOLLOWER
        self.commit_index = 0
        self.last_applied = 0
        
        # Leader state
        self.next_index: Dict[str, int] = {}
        self.match_index: Dict[str, int] = {}
        
        # Timing
        self.election_timeout = self._random_election_timeout()
        self.last_heartbeat = time.time()
        
        # Network simulation (in real system, this would be RPC)
        self.network: Dict[str, 'RaftNode'] = {}
        
        # Running state
        self.running = False
    
    def _random_election_timeout(self) -> float:
        """Random timeout between 150-300ms"""
        return random.uniform(0.15, 0.30)
    
    def _last_log_index(self) -> int:
        return len(self.log)
    
    def _last_log_term(self) -> int:
        if self.log:
            return self.log[-1].term
        return 0
    
    def _get_log_entry(self, index: int) -> Optional[LogEntry]:
        if 0 < index <= len(self.log):
            return self.log[index - 1]
        return None
    
    async def start(self):
        """Start the Raft node"""
        self.running = True
        asyncio.create_task(self._election_timer())
        asyncio.create_task(self._apply_committed())
        print(f"[{self.node_id}] Started as {self.state.value}")
    
    async def stop(self):
        """Stop the Raft node"""
        self.running = False
    
    async def _election_timer(self):
        """Check for election timeout"""
        while self.running:
            await asyncio.sleep(0.05)  # Check every 50ms
            
            if self.state == NodeState.LEADER:
                # Leaders send heartbeats
                await self._send_heartbeats()
            else:
                # Check if election timeout elapsed
                elapsed = time.time() - self.last_heartbeat
                if elapsed > self.election_timeout:
                    await self._start_election()
    
    async def _start_election(self):
        """Start a new election"""
        self.state = NodeState.CANDIDATE
        self.current_term += 1
        self.voted_for = self.node_id
        self.election_timeout = self._random_election_timeout()
        self.last_heartbeat = time.time()
        
        print(f"[{self.node_id}] Starting election for term {self.current_term}")
        
        votes_received = 1  # Vote for self
        votes_needed = (len(self.peers) + 1) // 2 + 1
        
        # Request votes from all peers
        vote_tasks = []
        for peer_id in self.peers:
            task = self._request_vote(peer_id)
            vote_tasks.append(task)
        
        responses = await asyncio.gather(*vote_tasks, return_exceptions=True)
        
        for response in responses:
            if isinstance(response, RequestVoteResponse):
                if response.term > self.current_term:
                    # Discovered higher term
                    self._become_follower(response.term)
                    return
                
                if response.vote_granted:
                    votes_received += 1
        
        # Check if won election
        if self.state == NodeState.CANDIDATE and votes_received >= votes_needed:
            self._become_leader()
    
    async def _request_vote(self, peer_id: str) -> Optional[RequestVoteResponse]:
        """Send RequestVote RPC to peer"""
        if peer_id not in self.network:
            return None
        
        request = RequestVoteRequest(
            term=self.current_term,
            candidate_id=self.node_id,
            last_log_index=self._last_log_index(),
            last_log_term=self._last_log_term()
        )
        
        peer = self.network[peer_id]
        return await peer.handle_request_vote(request)
    
    async def handle_request_vote(
        self, 
        request: RequestVoteRequest
    ) -> RequestVoteResponse:
        """Handle incoming RequestVote RPC"""
        
        # Check term
        if request.term > self.current_term:
            self._become_follower(request.term)
        
        vote_granted = False
        
        if request.term >= self.current_term:
            # Check if we can vote for this candidate
            if self.voted_for is None or self.voted_for == request.candidate_id:
                # Check if candidate's log is up-to-date
                if self._is_log_up_to_date(
                    request.last_log_index, 
                    request.last_log_term
                ):
                    vote_granted = True
                    self.voted_for = request.candidate_id
                    self.last_heartbeat = time.time()
                    print(f"[{self.node_id}] Voted for {request.candidate_id} "
                          f"in term {request.term}")
        
        return RequestVoteResponse(
            term=self.current_term,
            vote_granted=vote_granted
        )
    
    def _is_log_up_to_date(self, last_index: int, last_term: int) -> bool:
        """Check if candidate's log is at least as up-to-date as ours"""
        our_last_term = self._last_log_term()
        our_last_index = self._last_log_index()
        
        if last_term != our_last_term:
            return last_term > our_last_term
        return last_index >= our_last_index
    
    def _become_follower(self, term: int):
        """Become a follower"""
        print(f"[{self.node_id}] Becoming follower in term {term}")
        self.state = NodeState.FOLLOWER
        self.current_term = term
        self.voted_for = None
        self.last_heartbeat = time.time()
    
    def _become_leader(self):
        """Become the leader"""
        print(f"[{self.node_id}] Becoming LEADER in term {self.current_term}")
        self.state = NodeState.LEADER
        
        # Initialize leader state
        for peer_id in self.peers:
            self.next_index[peer_id] = self._last_log_index() + 1
            self.match_index[peer_id] = 0
    
    async def _send_heartbeats(self):
        """Send heartbeat (empty AppendEntries) to all followers"""
        if self.state != NodeState.LEADER:
            return
        
        tasks = []
        for peer_id in self.peers:
            task = self._send_append_entries(peer_id)
            tasks.append(task)
        
        await asyncio.gather(*tasks, return_exceptions=True)
        
        # Update commit index based on match indices
        self._update_commit_index()
    
    async def _send_append_entries(self, peer_id: str) -> bool:
        """Send AppendEntries RPC to a peer"""
        if peer_id not in self.network or self.state != NodeState.LEADER:
            return False
        
        next_idx = self.next_index.get(peer_id, 1)
        prev_log_index = next_idx - 1
        
        prev_log_term = 0
        if prev_log_index > 0:
            prev_entry = self._get_log_entry(prev_log_index)
            if prev_entry:
                prev_log_term = prev_entry.term
        
        # Get entries to send
        entries = self.log[next_idx - 1:] if next_idx <= len(self.log) else []
        
        request = AppendEntriesRequest(
            term=self.current_term,
            leader_id=self.node_id,
            prev_log_index=prev_log_index,
            prev_log_term=prev_log_term,
            entries=entries,
            leader_commit=self.commit_index
        )
        
        peer = self.network[peer_id]
        response = await peer.handle_append_entries(request)
        
        if response.term > self.current_term:
            self._become_follower(response.term)
            return False
        
        if response.success:
            if entries:
                self.next_index[peer_id] = next_idx + len(entries)
                self.match_index[peer_id] = self.next_index[peer_id] - 1
        else:
            # Decrement next_index and retry
            self.next_index[peer_id] = max(1, self.next_index[peer_id] - 1)
        
        return response.success
    
    async def handle_append_entries(
        self, 
        request: AppendEntriesRequest
    ) -> AppendEntriesResponse:
        """Handle incoming AppendEntries RPC"""
        
        # Check term
        if request.term < self.current_term:
            return AppendEntriesResponse(
                term=self.current_term,
                success=False
            )
        
        # Valid leader - reset election timer
        self.last_heartbeat = time.time()
        
        if request.term > self.current_term:
            self._become_follower(request.term)
        
        if self.state == NodeState.CANDIDATE:
            self._become_follower(request.term)
        
        # Check log consistency
        if request.prev_log_index > 0:
            prev_entry = self._get_log_entry(request.prev_log_index)
            if prev_entry is None or prev_entry.term != request.prev_log_term:
                return AppendEntriesResponse(
                    term=self.current_term,
                    success=False
                )
        
        # Append new entries
        for i, entry in enumerate(request.entries):
            index = request.prev_log_index + 1 + i
            
            if index <= len(self.log):
                # Check for conflicts
                existing = self.log[index - 1]
                if existing.term != entry.term:
                    # Delete conflicting entries
                    self.log = self.log[:index - 1]
                    self.log.append(entry)
                # else: already have this entry
            else:
                self.log.append(entry)
        
        # Update commit index
        if request.leader_commit > self.commit_index:
            self.commit_index = min(
                request.leader_commit,
                self._last_log_index()
            )
        
        return AppendEntriesResponse(
            term=self.current_term,
            success=True,
            match_index=self._last_log_index()
        )
    
    def _update_commit_index(self):
        """Update commit index based on majority replication"""
        if self.state != NodeState.LEADER:
            return
        
        # Find highest index replicated on majority
        for n in range(self._last_log_index(), self.commit_index, -1):
            entry = self._get_log_entry(n)
            if entry and entry.term == self.current_term:
                # Count replications
                count = 1  # Leader has it
                for peer_id in self.peers:
                    if self.match_index.get(peer_id, 0) >= n:
                        count += 1
                
                if count > (len(self.peers) + 1) // 2:
                    self.commit_index = n
                    print(f"[{self.node_id}] Committed index {n}")
                    break
    
    async def _apply_committed(self):
        """Apply committed entries to state machine"""
        while self.running:
            while self.last_applied < self.commit_index:
                self.last_applied += 1
                entry = self._get_log_entry(self.last_applied)
                if entry:
                    self.apply_callback(entry.command)
                    print(f"[{self.node_id}] Applied: {entry.command}")
            
            await asyncio.sleep(0.01)
    
    async def submit_command(self, command: Any) -> bool:
        """
        Submit a command to the cluster.
        Only leader can accept commands.
        """
        if self.state != NodeState.LEADER:
            print(f"[{self.node_id}] Not leader, cannot accept command")
            return False
        
        entry = LogEntry(
            term=self.current_term,
            index=self._last_log_index() + 1,
            command=command
        )
        
        self.log.append(entry)
        print(f"[{self.node_id}] Appended: {command} at index {entry.index}")
        
        # Replicate to followers
        await self._send_heartbeats()
        
        return True

# ============== DEMO ==============

async def demo_raft():
    """Demonstrate Raft consensus"""
    
    # Create a 3-node cluster
    nodes = {}
    peer_ids = ["node1", "node2", "node3"]
    
    for node_id in peer_ids:
        peers = [p for p in peer_ids if p != node_id]
        nodes[node_id] = RaftNode(node_id, peers)
    
    # Connect nodes (simulate network)
    for node in nodes.values():
        node.network = nodes
    
    print("=== Starting Raft Cluster ===\n")
    
    # Start all nodes
    for node in nodes.values():
        await node.start()
    
    # Wait for leader election
    print("\nWaiting for leader election...")
    await asyncio.sleep(1)
    
    # Find the leader
    leader = None
    for node in nodes.values():
        if node.state == NodeState.LEADER:
            leader = node
            break
    
    if leader:
        print(f"\n{leader.node_id} is the LEADER")
        
        # Submit some commands
        print("\n=== Submitting Commands ===\n")
        
        await leader.submit_command({"set": "x", "value": 1})
        await asyncio.sleep(0.2)
        
        await leader.submit_command({"set": "y", "value": 2})
        await asyncio.sleep(0.2)
        
        await leader.submit_command({"set": "z", "value": 3})
        await asyncio.sleep(0.5)
        
        # Check logs
        print("\n=== Final Logs ===")
        for node_id, node in nodes.items():
            log_cmds = [e.command for e in node.log]
            print(f"{node_id} ({node.state.value}): {log_cmds}")
    
    # Stop all nodes
    for node in nodes.values():
        await node.stop()

# asyncio.run(demo_raft())
```

---

## 12.5 Byzantine Fault Tolerance (BFT)

```
┌─────────────────────────────────────────────────────────────┐
│           BYZANTINE FAULT TOLERANCE                          │
│                                                              │
│  Normal consensus (Raft, Paxos) assumes:                   │
│  • Nodes can crash (fail-stop)                              │
│  • Nodes don't LIE                                          │
│                                                              │
│  Byzantine faults:                                          │
│  • Nodes can be MALICIOUS                                   │
│  • Nodes can send conflicting messages                      │
│  • Nodes can collude                                        │
│                                                              │
│                                                              │
│  Byzantine Generals Problem:                                │
│  ───────────────────────────                                │
│                                                              │
│  Generals surround a city.                                  │
│  Must all attack or all retreat (agreement).                │
│  Some generals might be TRAITORS.                          │
│                                                              │
│  General A: "Attack!"                                       │
│  General B: "Attack!"  (traitor, tells C: "Retreat!")      │
│  General C: ???                                             │
│                                                              │
│                                                              │
│  BFT Requirement:                                           │
│  ─────────────────                                          │
│  To tolerate f Byzantine nodes, need 3f+1 total nodes.     │
│                                                              │
│  f=1 → need 4 nodes                                        │
│  f=2 → need 7 nodes                                        │
│                                                              │
│                                                              │
│  PBFT (Practical BFT):                                      │
│  ─────────────────────                                      │
│  • O(n²) message complexity                                 │
│  • Used in permissioned blockchains                        │
│  • Too expensive for most applications                      │
│                                                              │
│                                                              │
│  When to use BFT:                                           │
│  ─────────────────                                          │
│  • Public blockchains                                       │
│  • Multi-party computation                                  │
│  • High-security systems                                    │
│                                                              │
│  For most systems: Raft/Paxos is sufficient!               │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

---

## 12.6 Consensus in Practice

### When to Use What

```
┌─────────────────────────────────────────────────────────────┐
│           CONSENSUS ALGORITHM SELECTION                      │
│                                                              │
│  Question 1: Do you need Byzantine fault tolerance?         │
│  ──────────────────────────────────────────────             │
│                                                              │
│  YES (untrusted participants) → PBFT or similar            │
│  NO  (trusted participants)   → Raft or Paxos              │
│                                                              │
│                                                              │
│  Question 2: How important is understandability?            │
│  ───────────────────────────────────────────────            │
│                                                              │
│  Very important → Raft                                      │
│  Less important → Paxos (more optimizations available)     │
│                                                              │
│                                                              │
│  Question 3: Read vs Write heavy?                           │
│  ────────────────────────────────                           │
│                                                              │
│  Read heavy  → Can read from followers (stale reads OK)    │
│  Write heavy → Consider sharding, not just consensus       │
│                                                              │
│                                                              │
│  Practical Recommendations:                                 │
│  ──────────────────────────                                 │
│                                                              │
│  • Most systems: Use etcd, Consul, or ZooKeeper            │
│    (Don't implement consensus yourself!)                    │
│                                                              │
│  • Key-value store: etcd (uses Raft)                       │
│  • Service discovery: Consul (uses Raft)                   │
│  • Coordination: ZooKeeper (uses Zab)                      │
│  • Database: CockroachDB, TiDB (use Raft)                  │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

---

## Summary

| Algorithm | Fault Tolerance | Message Complexity | Understandability |
|-----------|-----------------|-------------------|-------------------|
| **Paxos** | f of 2f+1 crash | O(n) | Hard |
| **Raft** | f of 2f+1 crash | O(n) | Easy |
| **PBFT** | f of 3f+1 Byzantine | O(n²) | Medium |

### Key Takeaways

1. **Consensus is necessary** for coordination in distributed systems
2. **FLP impossibility** means we use timeouts (partial synchrony)
3. **Raft is preferred** for understandability and correctness
4. **Don't implement consensus yourself** - use existing systems
5. **Byzantine tolerance** is expensive and usually unnecessary

---

## Practice Exercises

1. Implement Raft leader election
2. Add log replication to Raft implementation  
3. Simulate network partitions and observe behavior
4. Use etcd for distributed coordination
5. Compare Raft and Paxos performance

**Next Chapter**: Distributed coordination patterns (locks, barriers, queues)!
