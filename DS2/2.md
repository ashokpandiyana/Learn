# Chapter 2: What is a Distributed System?

> **Goal**: Understand what distributed systems are, why we need them, and why they're inherently difficult

---

## 2.1 Definition and Characteristics

### What IS a Distributed System?

**Simple Definition**:
> A distributed system is a collection of independent computers that appears to its users as a single coherent system.

**Technical Definition**:
> A system where components located on networked computers communicate and coordinate their actions by passing messages.

**Frontend Analogy**: 
Think of a React app with multiple microservices. The user sees one app, but behind the scenes, data comes from:
- User service (authentication)
- Product service (catalog)
- Order service (transactions)
- Notification service (emails)
- CDN (static assets)

```
                    What User Sees
                    ┌─────────────┐
                    │   Single    │
                    │    App      │
                    └──────┬──────┘
                           │
        ┌──────────────────┼──────────────────┐
        │                  │                   │
        ▼                  ▼                   ▼
  ┌───────────┐     ┌───────────┐      ┌───────────┐
  │  Service  │     │  Service  │      │  Service  │
  │     A     │────▶│     B     │────▶│     C     │
  │ (Users)   │     │ (Orders)  │      │ (Payments)│
  └─────┬─────┘     └─────┬─────┘      └─────┬─────┘
        │                  │                   │
        ▼                  ▼                   ▼
  ┌───────────┐     ┌───────────┐      ┌───────────┐
  │    DB     │     │    DB     │      │    DB     │
  │  Replica  │     │  Replica  │      │  Replica  │
  └───────────┘     └───────────┘      └───────────┘
  
        Reality: Many computers working together
```

### Key Characteristics

#### 1. **Concurrency**
Multiple components execute simultaneously.

```python
# In a distributed system, these happen AT THE SAME TIME
# on different machines:

# Machine 1 (User Service)
async def get_user(user_id):
    return await db.query("SELECT * FROM users WHERE id = ?", user_id)

# Machine 2 (Order Service) - executing simultaneously!
async def get_orders(user_id):
    return await db.query("SELECT * FROM orders WHERE user_id = ?", user_id)

# Machine 3 (Recommendation Service) - also at the same time!
async def get_recommendations(user_id):
    return await ml_model.predict(user_id)
```

#### 2. **No Global Clock**
Each machine has its own clock, and they're never perfectly synchronized.

```
Machine A Clock: 10:00:00.000
Machine B Clock: 10:00:00.003  ← 3ms ahead
Machine C Clock: 09:59:59.998  ← 2ms behind

Question: If A sends message at "10:00:00.000" and B receives it,
what time did it arrive?

B might think it arrived BEFORE it was sent!
This is called CLOCK SKEW and causes real problems.
```

#### 3. **Independent Failures**
Components can fail independently and partially.

```
Monolith:          Works │ Completely Dead
                   ──────┼────────────────▶

Distributed:       All OK │ A failed │ B failed │ A+C failed │ All dead
                   ───────┼──────────┼──────────┼────────────┼─────────▶
                   
More failure modes = More complexity!
```

#### 4. **Heterogeneity**
Different hardware, operating systems, programming languages.

```
┌─────────────────────────────────────────────────────────────┐
│                    Your Distributed System                   │
├─────────────────┬─────────────────┬─────────────────────────┤
│ User Service    │ ML Service      │ Payment Service         │
│ Python/FastAPI  │ Python/PyTorch  │ Go                      │
│ PostgreSQL      │ Redis           │ PostgreSQL              │
│ Linux/x86       │ Linux/GPU       │ Linux/ARM               │
│ AWS us-east-1   │ GCP us-west1    │ On-premise              │
└─────────────────┴─────────────────┴─────────────────────────┘

All need to work together seamlessly!
```

---

## 2.2 Why Distributed Systems?

### The Four Drivers

#### 1. **Scalability** - Handle More Load

```
Single Server Limits:
┌─────────────────────────────────┐
│ CPU:    Limited cores           │
│ Memory: Max ~12TB (expensive!)  │
│ Disk:   Limited I/O bandwidth   │
│ Network: Single NIC bottleneck  │
└─────────────────────────────────┘

Distributed Solution:
┌─────────┐ ┌─────────┐ ┌─────────┐ ┌─────────┐
│ Server1 │ │ Server2 │ │ Server3 │ │ Server4 │  Add more as needed!
└─────────┘ └─────────┘ └─────────┘ └─────────┘
```

**Real Example**: Google Search
- Processes 8.5 billion searches per day
- That's ~100,000 searches per second
- No single server can handle this
- Uses thousands of machines working together

#### 2. **Reliability** - No Single Point of Failure

```
Single Server:
┌─────────┐
│ Server  │ ←── If this dies, EVERYTHING is down
└─────────┘

Distributed:
┌─────────┐ ┌─────────┐ ┌─────────┐
│ Server1 │ │ Server2 │ │ Server3 │
│   ✗     │ │   ✓     │ │   ✓     │
└─────────┘ └─────────┘ └─────────┘
    ↓
  One fails, others continue serving users!
```

**Availability Calculation**:
```python
# Single server with 99.9% uptime
single_server_availability = 0.999
annual_downtime_hours = (1 - 0.999) * 24 * 365  # = 8.76 hours/year

# Three servers, any one can serve (with replication)
# System fails only if ALL fail simultaneously
triple_server_availability = 1 - (1 - 0.999) ** 3  # = 0.999999999
annual_downtime_seconds = (1 - 0.999999999) * 365 * 24 * 3600  # = 0.03 seconds/year!

print(f"Single server: {annual_downtime_hours:.2f} hours downtime/year")
print(f"Triple server: {annual_downtime_seconds:.2f} seconds downtime/year")
```

#### 3. **Performance** - Geographic Distribution

```
User in Tokyo accessing server in New York:
┌─────────────────────────────────────────────────────────┐
│  Tokyo ───────── 11,000 km ───────── New York          │
│                                                          │
│  Round trip time: ~200ms (speed of light limit!)        │
│  Every click feels sluggish                             │
└─────────────────────────────────────────────────────────┘

With geographic distribution:
┌─────────────────────────────────────────────────────────┐
│  Tokyo User ─── 50ms ─── Tokyo Server (replica)        │
│  NYC User ───── 10ms ─── NYC Server (replica)          │
│  London User ── 20ms ─── London Server (replica)       │
│                                                          │
│  Everyone gets fast response!                           │
└─────────────────────────────────────────────────────────┘
```

**CDN Example**:
```python
# Without CDN: Every image request goes to origin server
image_latency_ms = 200  # User in Australia, server in US

# With CDN: Images cached at edge locations
cdn_latency_ms = 20  # Served from nearby edge server

# For a page with 50 images:
without_cdn = 50 * 200  # 10,000ms = 10 seconds!
with_cdn = 50 * 20       # 1,000ms = 1 second
```

#### 4. **Cost Efficiency** - Horizontal Scaling

```
Vertical Scaling (Bigger Machine):
┌────────────────────────────────────────┐
│ 4 CPU  → 8 CPU  → 16 CPU → 32 CPU     │
│ $100   → $300   → $1000  → $4000      │
│                                        │
│ Cost grows EXPONENTIALLY               │
│ Eventually hits hardware limits        │
└────────────────────────────────────────┘

Horizontal Scaling (More Machines):
┌────────────────────────────────────────┐
│ 1 server → 2 servers → 4 servers      │
│ $100     → $200      → $400           │
│                                        │
│ Cost grows LINEARLY                   │
│ Can scale infinitely (theoretically)  │
└────────────────────────────────────────┘
```

---

## 2.3 The Eight Fallacies of Distributed Computing

> **These are assumptions developers make that WILL burn you**

### Fallacy 1: The Network is Reliable

**Reality**: Networks fail ALL the time.

```
Ways networks fail:
─────────────────────────────────────
├── Cable gets cut (construction, animals)
├── Switch/router fails
├── Network congestion (packets dropped)
├── DNS server unavailable
├── Firewall misconfiguration
├── Cloud provider outage
└── Software bugs in network stack
```

**Code that assumes reliable network**:
```python
# WRONG - Assumes network always works
def get_user_data(user_id):
    response = requests.get(f"http://user-service/users/{user_id}")
    return response.json()

# RIGHT - Handle network failures
import httpx
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=1, max=10)
)
async def get_user_data(user_id):
    async with httpx.AsyncClient(timeout=5.0) as client:
        try:
            response = await client.get(
                f"http://user-service/users/{user_id}"
            )
            response.raise_for_status()
            return response.json()
        except httpx.TimeoutException:
            raise NetworkError("User service timeout")
        except httpx.HTTPStatusError as e:
            if e.response.status_code >= 500:
                raise NetworkError("User service unavailable")
            raise
```

### Fallacy 2: Latency is Zero

**Reality**: Every network call takes time.

```
Latency Reality Check:
──────────────────────────────────────────
Local function call:     ~1 nanosecond
Same datacenter:         ~500 microseconds (500,000x slower!)
Cross-country:           ~50 milliseconds (50,000,000x slower!)
Cross-continent:         ~150 milliseconds

One network call that takes 1ms doesn't seem bad.
But chain 10 services together = 10ms minimum.
Add database calls, that's 50ms+.
Under load with retries? Could be seconds!
```

**The N+1 Query Problem in Distributed Systems**:
```python
# TERRIBLE - Makes N+1 network calls
async def get_users_with_orders(user_ids: list[int]):
    users = []
    for user_id in user_ids:  # If 100 users...
        user = await user_service.get(user_id)      # 100 calls to user service
        orders = await order_service.get(user_id)    # 100 calls to order service
        users.append({**user, 'orders': orders})
    return users
    # Total: 200 network calls! At 5ms each = 1 second!

# BETTER - Batch requests
async def get_users_with_orders(user_ids: list[int]):
    # Parallel batch calls
    users, orders = await asyncio.gather(
        user_service.get_batch(user_ids),      # 1 call
        order_service.get_batch(user_ids)      # 1 call
    )
    # Total: 2 network calls! Maybe 20ms total.
    
    orders_by_user = group_by(orders, 'user_id')
    return [
        {**user, 'orders': orders_by_user.get(user['id'], [])}
        for user in users
    ]
```

### Fallacy 3: Bandwidth is Infinite

**Reality**: Network capacity is limited and expensive.

```
Bandwidth Costs:
──────────────────────────────────────────
AWS data transfer: $0.09/GB (out to internet)
1 TB/day = $2,700/month just in bandwidth!

Bandwidth Limits:
──────────────────────────────────────────
Home internet: 100-1000 Mbps
Server NIC:    1-25 Gbps
Datacenter:    100 Gbps shared

A 10MB API response × 1000 requests/sec = 10 GB/sec = 80 Gbps!
You'll hit limits fast.
```

**Optimize Data Transfer**:
```python
from fastapi import FastAPI
from fastapi.responses import ORJSONResponse  # Faster JSON serialization
import gzip

app = FastAPI(default_response_class=ORJSONResponse)

# Use pagination - don't return everything
@app.get("/users")
async def list_users(skip: int = 0, limit: int = 20):  # Max 20 at a time
    return await db.fetch_users(skip=skip, limit=limit)

# Use compression for large responses
from starlette.middleware.gzip import GZipMiddleware
app.add_middleware(GZipMiddleware, minimum_size=1000)

# Return only needed fields
@app.get("/users/{user_id}")
async def get_user(user_id: int, fields: str = None):
    user = await db.fetch_user(user_id)
    if fields:
        requested_fields = fields.split(',')
        return {k: v for k, v in user.items() if k in requested_fields}
    return user
```

### Fallacy 4: The Network is Secure

**Reality**: Every network should be treated as hostile.

```
Attack Vectors:
──────────────────────────────────────────
├── Man-in-the-middle attacks
├── DNS spoofing
├── Packet sniffing
├── DDoS attacks
├── Compromised internal services
└── Cloud provider vulnerabilities

Even internal networks aren't safe!
```

**Security Best Practices**:
```python
from fastapi import FastAPI, Depends, HTTPException, Security
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
import jwt
import ssl

app = FastAPI()
security = HTTPBearer()

# 1. Always use HTTPS (TLS)
# Configure in your reverse proxy (nginx, traefik)

# 2. Validate all input
from pydantic import BaseModel, validator

class UserCreate(BaseModel):
    username: str
    email: str
    
    @validator('username')
    def username_alphanumeric(cls, v):
        if not v.isalnum():
            raise ValueError('must be alphanumeric')
        return v

# 3. Authenticate service-to-service calls
async def verify_token(
    credentials: HTTPAuthorizationCredentials = Security(security)
):
    try:
        payload = jwt.decode(
            credentials.credentials, 
            SECRET_KEY, 
            algorithms=["HS256"]
        )
        return payload
    except jwt.InvalidTokenError:
        raise HTTPException(status_code=401, detail="Invalid token")

@app.get("/internal/users")
async def internal_endpoint(token_data: dict = Depends(verify_token)):
    # Only authenticated services can call this
    return await get_users()

# 4. Use mTLS for service mesh (mutual TLS)
# Both client and server verify each other's certificates
```

### Fallacy 5: Topology Doesn't Change

**Reality**: Network topology changes constantly.

```
Changes that happen:
──────────────────────────────────────────
├── Servers added/removed (autoscaling)
├── Services redeployed (new IP addresses)
├── Load balancers reconfigured
├── DNS records updated
├── Network routes changed
├── Failover to backup datacenter
└── Cloud provider infrastructure updates
```

**Solution: Service Discovery**:
```python
# WRONG - Hardcoded addresses
USER_SERVICE_URL = "http://192.168.1.10:8080"

# RIGHT - Use service discovery
import consul

class ServiceDiscovery:
    def __init__(self):
        self.consul = consul.Consul()
        self._cache = {}
        self._cache_ttl = 30  # seconds
    
    async def get_service_url(self, service_name: str) -> str:
        # Check cache first
        cached = self._cache.get(service_name)
        if cached and time.time() < cached['expires']:
            return cached['url']
        
        # Query service registry
        _, services = self.consul.health.service(service_name, passing=True)
        if not services:
            raise ServiceNotFoundError(f"No healthy {service_name} instances")
        
        # Pick one (could use load balancing here)
        service = random.choice(services)
        url = f"http://{service['Service']['Address']}:{service['Service']['Port']}"
        
        # Cache it
        self._cache[service_name] = {
            'url': url,
            'expires': time.time() + self._cache_ttl
        }
        return url

# Usage
discovery = ServiceDiscovery()
user_service_url = await discovery.get_service_url("user-service")
```

### Fallacy 6: There is One Administrator

**Reality**: Multiple teams, multiple responsibilities, multiple points of failure.

```
Who's responsible when things break?
──────────────────────────────────────────
├── Your team: Application code
├── Platform team: Kubernetes, service mesh
├── Database team: PostgreSQL, Redis
├── Network team: Load balancers, firewalls
├── Security team: Certificates, access control
├── Cloud provider: VMs, storage, networking
└── Third-party services: Payment gateway, email service

A bug could be in ANY of these!
```

**Solution: Observability**:
```python
# Add context to every request for tracing across teams
import uuid
from fastapi import Request, FastAPI
from starlette.middleware.base import BaseHTTPMiddleware

class RequestContextMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request: Request, call_next):
        # Get or create request ID
        request_id = request.headers.get('X-Request-ID', str(uuid.uuid4()))
        
        # Add to all logs
        with structlog.contextvars.bound_contextvars(
            request_id=request_id,
            service="user-service",
            user_agent=request.headers.get('user-agent'),
        ):
            response = await call_next(request)
            response.headers['X-Request-ID'] = request_id
            return response

# Now when debugging:
# "Request abc-123 failed at order-service"
# Every team can search their logs for abc-123
```

### Fallacy 7: Transport Cost is Zero

**Reality**: Serialization, network I/O, and infrastructure cost money and time.

```
Hidden Costs:
──────────────────────────────────────────
├── Serialization/deserialization CPU time
├── Memory for network buffers
├── Cloud egress charges ($0.09/GB)
├── Load balancer costs
├── Service mesh overhead
└── TLS encryption/decryption
```

**Efficient Serialization**:
```python
import json
import pickle
import msgpack
from time import perf_counter

data = {"users": [{"id": i, "name": f"user_{i}"} for i in range(10000)]}

# Compare serialization performance
def benchmark_serialization():
    results = {}
    
    # JSON (human readable, slow)
    start = perf_counter()
    json_data = json.dumps(data)
    results['json'] = {
        'time_ms': (perf_counter() - start) * 1000,
        'size_kb': len(json_data.encode()) / 1024
    }
    
    # MessagePack (binary, fast)
    start = perf_counter()
    msgpack_data = msgpack.packb(data)
    results['msgpack'] = {
        'time_ms': (perf_counter() - start) * 1000,
        'size_kb': len(msgpack_data) / 1024
    }
    
    return results

# Typical results:
# JSON:     time=15ms, size=380KB
# MsgPack:  time=3ms,  size=250KB
# 5x faster, 35% smaller!
```

### Fallacy 8: The Network is Homogeneous

**Reality**: Different protocols, versions, and systems must interoperate.

```
Your system might talk to:
──────────────────────────────────────────
├── REST APIs (JSON over HTTP)
├── gRPC services (protobuf over HTTP/2)
├── Legacy SOAP services (XML)
├── Message queues (Kafka, RabbitMQ)
├── Databases (PostgreSQL, MongoDB, Redis)
├── Third-party APIs (various formats)
└── WebSockets for real-time
```

**Handle Multiple Protocols**:
```python
from abc import ABC, abstractmethod
from typing import Any

# Abstract interface for service communication
class ServiceClient(ABC):
    @abstractmethod
    async def call(self, method: str, params: dict) -> Any:
        pass

# REST implementation
class RESTClient(ServiceClient):
    async def call(self, method: str, params: dict) -> Any:
        async with httpx.AsyncClient() as client:
            response = await client.post(
                f"{self.base_url}/{method}",
                json=params
            )
            return response.json()

# gRPC implementation
class GRPCClient(ServiceClient):
    async def call(self, method: str, params: dict) -> Any:
        # Convert to protobuf and call gRPC
        stub = self.get_stub()
        grpc_method = getattr(stub, method)
        request = self.dict_to_proto(params)
        response = await grpc_method(request)
        return self.proto_to_dict(response)

# Your code uses the interface, not the implementation
async def get_user(client: ServiceClient, user_id: int):
    return await client.call("GetUser", {"user_id": user_id})
```

---

## 2.4 Types of Distributed Systems

### 2.4.1 Distributed Computing Systems

**Purpose**: Raw computational power

```
┌─────────────────────────────────────────────────────────────┐
│                    CLUSTER COMPUTING                         │
│                                                              │
│  ┌────────┐ ┌────────┐ ┌────────┐ ┌────────┐               │
│  │ Node 1 │ │ Node 2 │ │ Node 3 │ │ Node N │               │
│  │ (same  │ │ (same  │ │ (same  │ │ (same  │               │
│  │ hardware│ │hardware│ │hardware│ │hardware│              │
│  └────┬───┘ └────┬───┘ └────┬───┘ └────┬───┘               │
│       │          │          │          │                     │
│       └──────────┴──────────┴──────────┘                     │
│                         │                                     │
│               High-speed local network                        │
│                                                              │
│  Examples: HPC clusters, Hadoop, Spark                      │
└─────────────────────────────────────────────────────────────┘
```

```python
# Example: Distributed computing with Ray
import ray

ray.init()

@ray.remote
def process_chunk(data_chunk):
    """Process data on any available node"""
    return sum(x ** 2 for x in data_chunk)

# Distribute work across cluster
data = list(range(10_000_000))
chunk_size = 100_000
chunks = [data[i:i+chunk_size] for i in range(0, len(data), chunk_size)]

# Execute in parallel across nodes
futures = [process_chunk.remote(chunk) for chunk in chunks]
results = ray.get(futures)
total = sum(results)
```

### 2.4.2 Distributed Information Systems

**Purpose**: Handle data and transactions at scale

```
┌─────────────────────────────────────────────────────────────┐
│              DISTRIBUTED DATABASE SYSTEM                     │
│                                                              │
│     Write ──▶ ┌────────────┐                                │
│               │   Primary  │                                 │
│               │   (Leader) │                                 │
│               └──────┬─────┘                                 │
│                      │ Replication                           │
│            ┌─────────┼─────────┐                            │
│            ▼         ▼         ▼                             │
│       ┌────────┐ ┌────────┐ ┌────────┐                      │
│       │Replica1│ │Replica2│ │Replica3│                      │
│       └────────┘ └────────┘ └────────┘                      │
│            ▲         ▲         ▲                             │
│            └─────────┴─────────┘                             │
│                      │                                       │
│     Read ────────────┘                                       │
│                                                              │
│  Examples: PostgreSQL replication, MySQL cluster            │
└─────────────────────────────────────────────────────────────┘
```

```
┌─────────────────────────────────────────────────────────────┐
│                 MICROSERVICES ARCHITECTURE                   │
│                                                              │
│  ┌──────────┐    ┌──────────┐    ┌──────────┐              │
│  │   API    │───▶│  User    │───▶│  Order   │              │
│  │ Gateway  │    │ Service  │    │ Service  │              │
│  └──────────┘    └────┬─────┘    └────┬─────┘              │
│                       │               │                      │
│                       ▼               ▼                      │
│                  ┌─────────┐    ┌─────────┐                 │
│                  │  User   │    │  Order  │                 │
│                  │   DB    │    │   DB    │                 │
│                  └─────────┘    └─────────┘                 │
│                                                              │
│  Each service owns its data!                                │
└─────────────────────────────────────────────────────────────┘
```

### 2.4.3 Distributed Pervasive Systems

**Purpose**: Mobile and IoT environments

```
┌─────────────────────────────────────────────────────────────┐
│                    IoT ARCHITECTURE                          │
│                                                              │
│  ┌────────┐ ┌────────┐ ┌────────┐ ┌────────┐              │
│  │Sensor 1│ │Sensor 2│ │Sensor N│ │Phone   │              │
│  └────┬───┘ └────┬───┘ └────┬───┘ └────┬───┘              │
│       │          │          │          │                     │
│       └──────────┴──────────┴──────────┘                     │
│                         │                                     │
│                    ┌────▼────┐                               │
│                    │  Edge   │  (Process locally)            │
│                    │ Gateway │                               │
│                    └────┬────┘                               │
│                         │                                     │
│                    ┌────▼────┐                               │
│                    │  Cloud  │  (Aggregate, ML, Storage)    │
│                    │ Backend │                               │
│                    └─────────┘                               │
│                                                              │
│  Challenges: Connectivity, battery, security                │
└─────────────────────────────────────────────────────────────┘
```

---

## 2.5 A Simple Distributed System in Python

Let's build a minimal distributed system to understand the concepts:

```python
# service_a.py - First service
from fastapi import FastAPI
import httpx
import os

app = FastAPI()
SERVICE_B_URL = os.getenv("SERVICE_B_URL", "http://localhost:8001")

@app.get("/")
async def root():
    return {"service": "A", "status": "running"}

@app.get("/aggregate")
async def aggregate_data():
    """Call Service B and combine responses"""
    async with httpx.AsyncClient(timeout=5.0) as client:
        try:
            # This is distributed communication!
            response = await client.get(f"{SERVICE_B_URL}/data")
            service_b_data = response.json()
        except httpx.RequestError as e:
            # Network failure handling
            return {"error": f"Service B unreachable: {e}"}
    
    return {
        "service_a_data": {"message": "Hello from A"},
        "service_b_data": service_b_data,
        "combined": True
    }

# Run: uvicorn service_a:app --port 8000
```

```python
# service_b.py - Second service
from fastapi import FastAPI
import random

app = FastAPI()

@app.get("/")
async def root():
    return {"service": "B", "status": "running"}

@app.get("/data")
async def get_data():
    # Simulate occasional failures (distributed systems reality!)
    if random.random() < 0.1:  # 10% failure rate
        raise Exception("Random failure!")
    
    return {
        "message": "Hello from B",
        "random_number": random.randint(1, 100)
    }

# Run: uvicorn service_b:app --port 8001
```

```python
# client.py - Demonstrate distributed system behavior
import asyncio
import httpx

async def test_distributed_system():
    async with httpx.AsyncClient() as client:
        results = {"success": 0, "failure": 0}
        
        for i in range(20):
            try:
                response = await client.get(
                    "http://localhost:8000/aggregate",
                    timeout=5.0
                )
                data = response.json()
                
                if "error" in data:
                    results["failure"] += 1
                    print(f"Request {i}: PARTIAL FAILURE - {data['error']}")
                else:
                    results["success"] += 1
                    print(f"Request {i}: SUCCESS - {data}")
                    
            except httpx.RequestError as e:
                results["failure"] += 1
                print(f"Request {i}: COMPLETE FAILURE - {e}")
        
        print(f"\nResults: {results}")
        print(f"Success rate: {results['success']/20*100}%")

asyncio.run(test_distributed_system())
```

---

## Summary: Key Takeaways

1. **Distributed systems look simple but are fundamentally complex**
   - No global clock
   - Partial failures
   - Network is unreliable

2. **We use distributed systems for**:
   - Scalability (handle more load)
   - Reliability (survive failures)
   - Performance (geographic distribution)
   - Cost (horizontal scaling)

3. **The 8 Fallacies will bite you**:
   - Network fails
   - Latency is real
   - Bandwidth costs money
   - Security is never guaranteed
   - Topology changes
   - Multiple administrators
   - Transport has cost
   - Heterogeneous systems

4. **Always design for failure**:
   - Retry with backoff
   - Set timeouts
   - Add circuit breakers
   - Make operations idempotent

---

## Practice Exercises

1. **Build two services** that communicate via HTTP and observe what happens when one fails
2. **Add artificial latency** (sleep) and see how it affects the calling service
3. **Implement retry logic** with exponential backoff
4. **Add timeout handling** and observe timeout behavior
5. **Simulate network partition** (stop one service) and handle gracefully

**Next Chapter**: Core concepts and terminology you'll use every day in distributed systems!
