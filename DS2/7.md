# Chapter 7: Distributed Data Storage

> **Goal**: Understand how data is replicated and partitioned across multiple machines

---

## 7.1 Why Distribute Data?

```
┌─────────────────────────────────────────────────────────────┐
│           WHY WE DISTRIBUTE DATA                             │
│                                                              │
│  Single Database Limits:                                    │
│  ───────────────────────                                    │
│  ├── Storage: One disk can only hold so much                │
│  ├── Throughput: One CPU can only handle so many queries    │
│  ├── Availability: One machine = single point of failure    │
│  └── Latency: Users far from server experience delays       │
│                                                              │
│                                                              │
│  Two Solutions:                                             │
│  ───────────────                                            │
│                                                              │
│  1. REPLICATION (Copies)                                    │
│     Same data on multiple machines                          │
│     ┌─────┐  ┌─────┐  ┌─────┐                              │
│     │ DB  │  │ DB  │  │ DB  │   (Same data everywhere)     │
│     │Copy1│  │Copy2│  │Copy3│                              │
│     └─────┘  └─────┘  └─────┘                              │
│     Why: Fault tolerance, read scalability, low latency     │
│                                                              │
│  2. PARTITIONING (Sharding)                                 │
│     Different data on different machines                    │
│     ┌─────┐  ┌─────┐  ┌─────┐                              │
│     │Users│  │Users│  │Users│   (Different data)           │
│     │ A-H │  │ I-P │  │ Q-Z │                              │
│     └─────┘  └─────┘  └─────┘                              │
│     Why: Handle more data, write scalability                │
│                                                              │
│  Usually you need BOTH!                                     │
└─────────────────────────────────────────────────────────────┘
```

---

## 7.2 Replication

### 7.2.1 Single-Leader (Master-Slave) Replication

```
┌─────────────────────────────────────────────────────────────┐
│              SINGLE-LEADER REPLICATION                       │
│                                                              │
│                    ┌──────────────┐                         │
│    All Writes ───▶ │    LEADER    │                         │
│                    │   (Primary)  │                         │
│                    └──────┬───────┘                         │
│                           │                                  │
│              Replication  │  (async or sync)                │
│                           │                                  │
│            ┌──────────────┼──────────────┐                  │
│            ▼              ▼              ▼                   │
│     ┌──────────┐   ┌──────────┐   ┌──────────┐             │
│     │ FOLLOWER │   │ FOLLOWER │   │ FOLLOWER │             │
│     │(Replica 1)│   │(Replica 2)│   │(Replica 3)│            │
│     └──────────┘   └──────────┘   └──────────┘             │
│            ▲              ▲              ▲                   │
│            └──────────────┴──────────────┘                  │
│                           │                                  │
│    All Reads ◀────────────┘                                 │
│                                                              │
│  Characteristics:                                           │
│  ├── All writes go to leader                                │
│  ├── Leader replicates to followers                         │
│  ├── Reads can go to any replica                            │
│  └── Followers are read-only                                │
│                                                              │
│  Pros:                      Cons:                           │
│  ├── Simple to understand   ├── Leader is bottleneck        │
│  ├── No write conflicts     ├── Leader failure = downtime   │
│  └── Easy consistency       └── Async = possible data loss  │
└─────────────────────────────────────────────────────────────┘
```

**Python Implementation - Single Leader Replication**:

```python
# single_leader_replication.py
import asyncio
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Any
from datetime import datetime
import json
import hashlib

@dataclass
class LogEntry:
    """Write-ahead log entry for replication"""
    sequence: int
    operation: str  # 'SET', 'DELETE'
    key: str
    value: Any
    timestamp: datetime = field(default_factory=datetime.utcnow)
    
    def to_dict(self) -> dict:
        return {
            'sequence': self.sequence,
            'operation': self.operation,
            'key': self.key,
            'value': self.value,
            'timestamp': self.timestamp.isoformat()
        }

class ReplicationLog:
    """Write-ahead log for replication"""
    
    def __init__(self):
        self.entries: List[LogEntry] = []
        self.sequence = 0
    
    def append(self, operation: str, key: str, value: Any) -> LogEntry:
        self.sequence += 1
        entry = LogEntry(
            sequence=self.sequence,
            operation=operation,
            key=key,
            value=value
        )
        self.entries.append(entry)
        return entry
    
    def get_entries_after(self, sequence: int) -> List[LogEntry]:
        """Get all entries after given sequence number"""
        return [e for e in self.entries if e.sequence > sequence]

class LeaderNode:
    """Leader (Primary) node that accepts writes"""
    
    def __init__(self, node_id: str):
        self.node_id = node_id
        self.data: Dict[str, Any] = {}
        self.log = ReplicationLog()
        self.followers: List['FollowerNode'] = []
        self.replication_mode = 'async'  # 'sync' or 'async'
    
    def add_follower(self, follower: 'FollowerNode'):
        self.followers.append(follower)
        # Send full snapshot to new follower
        asyncio.create_task(self._send_snapshot(follower))
    
    async def _send_snapshot(self, follower: 'FollowerNode'):
        """Send full data snapshot to new follower"""
        await follower.receive_snapshot(self.data, self.log.sequence)
    
    async def write(self, key: str, value: Any) -> bool:
        """Write data and replicate to followers"""
        # 1. Write to local storage
        self.data[key] = value
        
        # 2. Append to write-ahead log
        entry = self.log.append('SET', key, value)
        
        # 3. Replicate to followers
        if self.replication_mode == 'sync':
            # Synchronous: Wait for all followers
            await self._replicate_sync(entry)
        else:
            # Asynchronous: Fire and forget
            asyncio.create_task(self._replicate_async(entry))
        
        return True
    
    async def _replicate_sync(self, entry: LogEntry):
        """Synchronous replication - wait for acknowledgment"""
        tasks = [
            follower.receive_entry(entry) 
            for follower in self.followers
        ]
        await asyncio.gather(*tasks)
    
    async def _replicate_async(self, entry: LogEntry):
        """Asynchronous replication - don't wait"""
        for follower in self.followers:
            try:
                await follower.receive_entry(entry)
            except Exception as e:
                print(f"Replication to {follower.node_id} failed: {e}")
    
    def read(self, key: str) -> Optional[Any]:
        """Read from leader (guaranteed latest)"""
        return self.data.get(key)

class FollowerNode:
    """Follower (Replica) node that receives replicated writes"""
    
    def __init__(self, node_id: str):
        self.node_id = node_id
        self.data: Dict[str, Any] = {}
        self.last_sequence = 0
        self.replication_lag = 0  # Entries behind leader
    
    async def receive_snapshot(self, data: Dict[str, Any], sequence: int):
        """Receive full snapshot from leader"""
        self.data = data.copy()
        self.last_sequence = sequence
        print(f"[{self.node_id}] Received snapshot at sequence {sequence}")
    
    async def receive_entry(self, entry: LogEntry):
        """Receive and apply a single log entry"""
        # Check for gaps in sequence
        if entry.sequence != self.last_sequence + 1:
            print(f"[{self.node_id}] Gap detected! Expected {self.last_sequence + 1}, got {entry.sequence}")
            # In production: request missing entries from leader
        
        # Apply the operation
        if entry.operation == 'SET':
            self.data[entry.key] = entry.value
        elif entry.operation == 'DELETE':
            self.data.pop(entry.key, None)
        
        self.last_sequence = entry.sequence
        await asyncio.sleep(0.01)  # Simulate network delay
    
    def read(self, key: str) -> Optional[Any]:
        """Read from follower (may be stale!)"""
        return self.data.get(key)

# ============== USAGE EXAMPLE ==============

async def demo_single_leader():
    # Create leader and followers
    leader = LeaderNode("leader-1")
    follower1 = FollowerNode("follower-1")
    follower2 = FollowerNode("follower-2")
    
    leader.add_follower(follower1)
    leader.add_follower(follower2)
    
    # Write to leader
    await leader.write("user:123", {"name": "Alice", "age": 30})
    await leader.write("user:456", {"name": "Bob", "age": 25})
    
    # Wait for async replication
    await asyncio.sleep(0.1)
    
    # Read from leader (latest)
    print(f"Leader read: {leader.read('user:123')}")
    
    # Read from follower (may be slightly stale)
    print(f"Follower1 read: {follower1.read('user:123')}")
    print(f"Follower2 read: {follower2.read('user:123')}")

# asyncio.run(demo_single_leader())
```

### 7.2.2 Multi-Leader Replication

```
┌─────────────────────────────────────────────────────────────┐
│              MULTI-LEADER REPLICATION                        │
│                                                              │
│   Data Center A              Data Center B                  │
│   ┌──────────────┐          ┌──────────────┐               │
│   │   LEADER 1   │◀────────▶│   LEADER 2   │               │
│   │   (Primary)  │  Async   │   (Primary)  │               │
│   └──────┬───────┘  Sync    └──────┬───────┘               │
│          │                          │                        │
│          ▼                          ▼                        │
│   ┌──────────────┐          ┌──────────────┐               │
│   │  Followers   │          │  Followers   │               │
│   └──────────────┘          └──────────────┘               │
│                                                              │
│   Both leaders accept writes!                               │
│   Changes sync between leaders asynchronously.              │
│                                                              │
│   Use Cases:                                                │
│   ├── Multi-datacenter deployment                           │
│   ├── Offline clients (each client is a "leader")           │
│   └── Collaborative editing                                 │
│                                                              │
│   THE BIG PROBLEM: Conflicts!                               │
│   ─────────────────────────────                             │
│   User writes "A" to Leader 1                               │
│   User writes "B" to Leader 2                               │
│   Same key, different values! What's correct?               │
└─────────────────────────────────────────────────────────────┘
```

**Conflict Resolution Strategies**:

```python
# conflict_resolution.py
from dataclasses import dataclass
from datetime import datetime
from typing import Any, Optional, List
import hashlib

@dataclass
class VersionedValue:
    """Value with version information for conflict detection"""
    value: Any
    timestamp: datetime
    node_id: str
    version: int  # Logical clock / version vector
    
class ConflictResolver:
    """Different strategies for resolving write conflicts"""
    
    @staticmethod
    def last_write_wins(v1: VersionedValue, v2: VersionedValue) -> VersionedValue:
        """
        LWW: Most recent timestamp wins.
        Simple but can lose data!
        """
        if v1.timestamp > v2.timestamp:
            return v1
        elif v2.timestamp > v1.timestamp:
            return v2
        else:
            # Tie-breaker: use node_id
            return v1 if v1.node_id > v2.node_id else v2
    
    @staticmethod
    def merge_values(v1: VersionedValue, v2: VersionedValue) -> VersionedValue:
        """
        Merge: Combine both values (for compatible types).
        Good for sets, counters, etc.
        """
        # Example: Merging shopping carts
        if isinstance(v1.value, dict) and isinstance(v2.value, dict):
            merged = {**v1.value, **v2.value}
            return VersionedValue(
                value=merged,
                timestamp=max(v1.timestamp, v2.timestamp),
                node_id=v1.node_id,
                version=max(v1.version, v2.version) + 1
            )
        # Fall back to LWW
        return ConflictResolver.last_write_wins(v1, v2)
    
    @staticmethod
    def keep_all(v1: VersionedValue, v2: VersionedValue) -> VersionedValue:
        """
        Keep all conflicting versions (let application decide).
        Used by DynamoDB, Riak.
        """
        return VersionedValue(
            value=[v1.value, v2.value],  # Return both!
            timestamp=max(v1.timestamp, v2.timestamp),
            node_id="merged",
            version=max(v1.version, v2.version) + 1
        )

# ============== CRDT: Conflict-Free Replicated Data Types ==============

class GCounter:
    """
    Grow-only Counter - a simple CRDT.
    Each node maintains its own count.
    Total = sum of all node counts.
    Automatically merges without conflicts!
    """
    
    def __init__(self, node_id: str):
        self.node_id = node_id
        self.counts: dict[str, int] = {}
    
    def increment(self, amount: int = 1):
        """Increment this node's counter"""
        if self.node_id not in self.counts:
            self.counts[self.node_id] = 0
        self.counts[self.node_id] += amount
    
    def value(self) -> int:
        """Get total count across all nodes"""
        return sum(self.counts.values())
    
    def merge(self, other: 'GCounter'):
        """Merge with another counter - no conflicts possible!"""
        for node_id, count in other.counts.items():
            self.counts[node_id] = max(
                self.counts.get(node_id, 0),
                count
            )

class LWWRegister:
    """
    Last-Writer-Wins Register - stores single value.
    Uses timestamp for conflict resolution.
    """
    
    def __init__(self, node_id: str):
        self.node_id = node_id
        self.value: Any = None
        self.timestamp: datetime = datetime.min
    
    def set(self, value: Any):
        """Set value with current timestamp"""
        self.value = value
        self.timestamp = datetime.utcnow()
    
    def merge(self, other: 'LWWRegister'):
        """Merge - latest timestamp wins"""
        if other.timestamp > self.timestamp:
            self.value = other.value
            self.timestamp = other.timestamp

class GSet:
    """
    Grow-only Set - elements can only be added.
    Perfect for "likes", "views", etc.
    """
    
    def __init__(self):
        self.elements: set = set()
    
    def add(self, element: Any):
        self.elements.add(element)
    
    def contains(self, element: Any) -> bool:
        return element in self.elements
    
    def merge(self, other: 'GSet'):
        """Merge = union of sets"""
        self.elements = self.elements.union(other.elements)

# ============== DEMO: CRDTs in Action ==============

def demo_crdt():
    # Scenario: Two servers counting page views
    
    # Server A gets some views
    counter_a = GCounter("server-a")
    counter_a.increment(100)
    counter_a.increment(50)
    
    # Server B gets some views (independently)
    counter_b = GCounter("server-b")
    counter_b.increment(75)
    counter_b.increment(25)
    
    print(f"Server A count: {counter_a.value()}")  # 150
    print(f"Server B count: {counter_b.value()}")  # 100
    
    # Sync: merge both directions
    counter_a.merge(counter_b)
    counter_b.merge(counter_a)
    
    # Both now have same total!
    print(f"After merge - Server A: {counter_a.value()}")  # 250
    print(f"After merge - Server B: {counter_b.value()}")  # 250

# demo_crdt()
```

### 7.2.3 Leaderless Replication

```
┌─────────────────────────────────────────────────────────────┐
│              LEADERLESS REPLICATION                          │
│                                                              │
│   No designated leader - any node accepts writes!           │
│                                                              │
│   ┌────────┐      ┌────────┐      ┌────────┐              │
│   │ Node 1 │◀────▶│ Node 2 │◀────▶│ Node 3 │              │
│   └────────┘      └────────┘      └────────┘              │
│        ▲                               ▲                    │
│        │                               │                    │
│        └───────────────────────────────┘                    │
│                                                              │
│   QUORUM: How many nodes must respond?                      │
│   ─────────────────────────────────────                     │
│                                                              │
│   N = Total nodes (e.g., 3)                                 │
│   W = Write quorum (nodes that must ACK write)              │
│   R = Read quorum (nodes to read from)                      │
│                                                              │
│   Rule: W + R > N  (ensures overlap)                        │
│                                                              │
│   Example with N=3:                                         │
│   ├── W=2, R=2: Strong consistency (overlap guaranteed)     │
│   ├── W=1, R=3: Fast writes, slow reads                     │
│   ├── W=3, R=1: Slow writes, fast reads                     │
│   └── W=1, R=1: Fast but weak (no guarantee!)              │
│                                                              │
│   Used by: Cassandra, DynamoDB, Riak                        │
└─────────────────────────────────────────────────────────────┘
```

```python
# leaderless_replication.py
import asyncio
from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple
from datetime import datetime
import random

@dataclass
class VersionedData:
    value: Any
    version: int
    timestamp: datetime
    
class LeaderlessNode:
    """A node in a leaderless replicated system"""
    
    def __init__(self, node_id: str):
        self.node_id = node_id
        self.data: Dict[str, VersionedData] = {}
        self.peers: List['LeaderlessNode'] = []
        self.is_healthy = True
    
    def add_peer(self, peer: 'LeaderlessNode'):
        self.peers.append(peer)
    
    async def local_write(self, key: str, value: Any, version: int) -> bool:
        """Write to local storage"""
        if not self.is_healthy:
            raise Exception(f"Node {self.node_id} is down")
        
        current = self.data.get(key)
        
        # Only write if version is newer
        if current is None or version > current.version:
            self.data[key] = VersionedData(
                value=value,
                version=version,
                timestamp=datetime.utcnow()
            )
            return True
        return False
    
    async def local_read(self, key: str) -> Optional[VersionedData]:
        """Read from local storage"""
        if not self.is_healthy:
            raise Exception(f"Node {self.node_id} is down")
        
        await asyncio.sleep(0.01)  # Simulate latency
        return self.data.get(key)

class LeaderlessCluster:
    """Coordinator for leaderless quorum operations"""
    
    def __init__(self, nodes: List[LeaderlessNode]):
        self.nodes = nodes
        self.n = len(nodes)
        # Default quorums for strong consistency
        self.write_quorum = self.n // 2 + 1
        self.read_quorum = self.n // 2 + 1
    
    async def write(self, key: str, value: Any) -> bool:
        """
        Quorum write: Send to all, wait for W acknowledgments.
        """
        # Get current max version
        current_version = await self._get_max_version(key)
        new_version = current_version + 1
        
        # Send write to all nodes
        tasks = [
            self._write_to_node(node, key, value, new_version)
            for node in self.nodes
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Count successful writes
        successes = sum(1 for r in results if r is True)
        
        if successes >= self.write_quorum:
            print(f"Write succeeded: {successes}/{self.n} nodes")
            return True
        else:
            print(f"Write failed: only {successes}/{self.n} nodes (need {self.write_quorum})")
            return False
    
    async def _write_to_node(
        self, 
        node: LeaderlessNode, 
        key: str, 
        value: Any,
        version: int
    ) -> bool:
        try:
            return await node.local_write(key, value, version)
        except Exception as e:
            print(f"Write to {node.node_id} failed: {e}")
            return False
    
    async def _get_max_version(self, key: str) -> int:
        """Get the highest version number across nodes"""
        tasks = [node.local_read(key) for node in self.nodes if node.is_healthy]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        max_version = 0
        for r in results:
            if isinstance(r, VersionedData):
                max_version = max(max_version, r.version)
        
        return max_version
    
    async def read(self, key: str) -> Optional[Any]:
        """
        Quorum read: Read from R nodes, return latest version.
        """
        # Read from all nodes
        tasks = [
            self._read_from_node(node, key)
            for node in self.nodes
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Filter successful reads
        valid_reads = [
            r for r in results 
            if isinstance(r, VersionedData)
        ]
        
        if len(valid_reads) < self.read_quorum:
            print(f"Read failed: only {len(valid_reads)} responses (need {self.read_quorum})")
            return None
        
        # Return value with highest version
        latest = max(valid_reads, key=lambda x: x.version)
        
        # Read repair: update stale nodes
        await self._read_repair(key, latest, valid_reads)
        
        return latest.value
    
    async def _read_from_node(
        self, 
        node: LeaderlessNode, 
        key: str
    ) -> Optional[VersionedData]:
        try:
            return await node.local_read(key)
        except Exception as e:
            return None
    
    async def _read_repair(
        self, 
        key: str, 
        latest: VersionedData,
        all_reads: List[VersionedData]
    ):
        """Update nodes that have stale data"""
        for node in self.nodes:
            if node.is_healthy:
                current = await node.local_read(key)
                if current is None or current.version < latest.version:
                    await node.local_write(key, latest.value, latest.version)
                    print(f"Read repair: updated {node.node_id}")

# ============== DEMO ==============

async def demo_quorum():
    # Create 3-node cluster
    nodes = [LeaderlessNode(f"node-{i}") for i in range(3)]
    
    # Connect peers
    for node in nodes:
        node.peers = [n for n in nodes if n != node]
    
    cluster = LeaderlessCluster(nodes)
    
    # Write with quorum
    await cluster.write("user:123", {"name": "Alice"})
    
    # Read with quorum
    value = await cluster.read("user:123")
    print(f"Read value: {value}")
    
    # Simulate node failure
    print("\n--- Node 2 goes down ---")
    nodes[2].is_healthy = False
    
    # Write still succeeds (2 of 3 = quorum)
    await cluster.write("user:123", {"name": "Alice Updated"})
    
    # Read still succeeds
    value = await cluster.read("user:123")
    print(f"Read after failure: {value}")
    
    # Node comes back
    print("\n--- Node 2 recovers ---")
    nodes[2].is_healthy = True
    
    # Read triggers repair
    value = await cluster.read("user:123")
    
    # Check all nodes now have same data
    for node in nodes:
        data = await node.local_read("user:123")
        print(f"{node.node_id}: version={data.version}, value={data.value}")

# asyncio.run(demo_quorum())
```

---

## 7.3 Partitioning (Sharding)

### 7.3.1 Why Partition?

```
┌─────────────────────────────────────────────────────────────┐
│              WHY PARTITIONING?                               │
│                                                              │
│  Problem: Data too big for one machine                      │
│                                                              │
│  Single Node:                                               │
│  ┌──────────────────────────────────────┐                  │
│  │         1 TB of user data            │                  │
│  │         10,000 writes/sec            │  ← Can't handle! │
│  └──────────────────────────────────────┘                  │
│                                                              │
│  Partitioned (3 shards):                                    │
│  ┌────────────┐ ┌────────────┐ ┌────────────┐             │
│  │  Shard 1   │ │  Shard 2   │ │  Shard 3   │             │
│  │  333 GB    │ │  333 GB    │ │  333 GB    │             │
│  │  3,333 w/s │ │  3,333 w/s │ │  3,333 w/s │             │
│  │  Users A-H │ │  Users I-P │ │  Users Q-Z │             │
│  └────────────┘ └────────────┘ └────────────┘             │
│                                                              │
│  Benefits:                                                  │
│  ├── More storage (sum of all shards)                       │
│  ├── More throughput (queries parallelized)                 │
│  ├── Better locality (data close to users)                  │
│  └── Isolation (one shard failure doesn't affect others)    │
└─────────────────────────────────────────────────────────────┘
```

### 7.3.2 Partitioning Strategies

```
┌─────────────────────────────────────────────────────────────┐
│           PARTITIONING STRATEGIES                            │
│                                                              │
│  1. RANGE PARTITIONING                                      │
│  ──────────────────────                                     │
│  Keys are sorted, ranges assigned to partitions             │
│                                                              │
│  Shard 1: A-H     Shard 2: I-P     Shard 3: Q-Z            │
│  ────────────     ────────────     ────────────            │
│  Alice           Isaac            Quinn                     │
│  Bob             Julia            Robert                    │
│  Carol           Kevin            Sarah                     │
│                                                              │
│  Pros: Range queries efficient, sorted within shard         │
│  Cons: Hot spots if keys not evenly distributed             │
│        (e.g., all new users have names starting with 'A')   │
│                                                              │
│                                                              │
│  2. HASH PARTITIONING                                       │
│  ─────────────────────                                      │
│  Hash the key, use hash to pick partition                   │
│                                                              │
│  hash("Alice") % 3 = 1 → Shard 1                           │
│  hash("Bob")   % 3 = 0 → Shard 0                           │
│  hash("Carol") % 3 = 2 → Shard 2                           │
│                                                              │
│  Pros: Even distribution, no hot spots                      │
│  Cons: Range queries impossible, must query all shards      │
│                                                              │
│                                                              │
│  3. CONSISTENT HASHING                                      │
│  ────────────────────────                                   │
│  Hash ring - minimizes data movement when nodes change      │
│  (Explained in detail below)                                │
└─────────────────────────────────────────────────────────────┘
```

### 7.3.3 Consistent Hashing Deep Dive

```
┌─────────────────────────────────────────────────────────────┐
│              CONSISTENT HASHING                              │
│                                                              │
│  Problem with Simple Hash:                                  │
│  ─────────────────────────                                  │
│  partition = hash(key) % num_servers                        │
│                                                              │
│  With 3 servers: hash("Alice") % 3 = 1                      │
│  Add 4th server: hash("Alice") % 4 = 2  ← DIFFERENT!       │
│                                                              │
│  Adding one server moves ~75% of all keys!                  │
│                                                              │
│                                                              │
│  Consistent Hashing Solution:                               │
│  ────────────────────────────                               │
│  Imagine a ring from 0 to 2^32                              │
│                                                              │
│                     0                                        │
│                     │                                        │
│            Node C ──┼── Node A                              │
│                    /│\                                       │
│                   / │ \                                      │
│                  /  │  \                                     │
│         2^32*3/4    │    2^32*1/4                           │
│                \    │   /                                    │
│                 \   │  /                                     │
│                  \  │ /                                      │
│            Node B ──┼──                                      │
│                     │                                        │
│                  2^32/2                                      │
│                                                              │
│  - Hash nodes onto the ring                                 │
│  - Hash keys onto the ring                                  │
│  - Key goes to next node clockwise                          │
│                                                              │
│  Adding a node only moves keys between neighbors!           │
│  ~1/N keys move instead of ~100%                           │
└─────────────────────────────────────────────────────────────┘
```

```python
# consistent_hashing.py
import hashlib
from typing import Dict, List, Optional, Any
from bisect import bisect_right
from dataclasses import dataclass

class ConsistentHash:
    """
    Consistent hashing implementation with virtual nodes.
    """
    
    def __init__(self, num_virtual_nodes: int = 150):
        self.num_virtual_nodes = num_virtual_nodes
        self.ring: Dict[int, str] = {}  # hash -> node_id
        self.sorted_keys: List[int] = []
        self.nodes: Dict[str, Any] = {}  # node_id -> node data
    
    def _hash(self, key: str) -> int:
        """Generate hash for a key"""
        return int(hashlib.md5(key.encode()).hexdigest(), 16)
    
    def add_node(self, node_id: str, node_data: Any = None):
        """Add a node to the ring with virtual nodes"""
        self.nodes[node_id] = node_data
        
        # Create virtual nodes for better distribution
        for i in range(self.num_virtual_nodes):
            virtual_key = f"{node_id}:vn{i}"
            hash_value = self._hash(virtual_key)
            self.ring[hash_value] = node_id
            self.sorted_keys.append(hash_value)
        
        self.sorted_keys.sort()
        print(f"Added node {node_id} with {self.num_virtual_nodes} virtual nodes")
    
    def remove_node(self, node_id: str):
        """Remove a node and its virtual nodes from the ring"""
        if node_id not in self.nodes:
            return
        
        # Remove all virtual nodes
        for i in range(self.num_virtual_nodes):
            virtual_key = f"{node_id}:vn{i}"
            hash_value = self._hash(virtual_key)
            del self.ring[hash_value]
            self.sorted_keys.remove(hash_value)
        
        del self.nodes[node_id]
        print(f"Removed node {node_id}")
    
    def get_node(self, key: str) -> Optional[str]:
        """Get the node responsible for a key"""
        if not self.ring:
            return None
        
        hash_value = self._hash(key)
        
        # Find first node with hash >= key hash (clockwise)
        idx = bisect_right(self.sorted_keys, hash_value)
        
        # Wrap around to beginning of ring
        if idx == len(self.sorted_keys):
            idx = 0
        
        return self.ring[self.sorted_keys[idx]]
    
    def get_nodes_for_key(self, key: str, num_replicas: int = 3) -> List[str]:
        """Get multiple nodes for replication"""
        if not self.ring:
            return []
        
        hash_value = self._hash(key)
        idx = bisect_right(self.sorted_keys, hash_value)
        
        nodes = []
        seen_nodes = set()
        
        # Walk clockwise, collecting unique nodes
        for i in range(len(self.sorted_keys)):
            node_idx = (idx + i) % len(self.sorted_keys)
            node_id = self.ring[self.sorted_keys[node_idx]]
            
            if node_id not in seen_nodes:
                nodes.append(node_id)
                seen_nodes.add(node_id)
                
                if len(nodes) == num_replicas:
                    break
        
        return nodes

# ============== SHARDED DATABASE ==============

class ShardedDatabase:
    """Database that uses consistent hashing for sharding"""
    
    def __init__(self):
        self.hash_ring = ConsistentHash(num_virtual_nodes=100)
        self.shards: Dict[str, Dict[str, Any]] = {}
    
    def add_shard(self, shard_id: str):
        """Add a new shard"""
        self.shards[shard_id] = {}
        self.hash_ring.add_node(shard_id)
    
    def remove_shard(self, shard_id: str):
        """Remove a shard (must migrate data first!)"""
        self.hash_ring.remove_node(shard_id)
        # In production: migrate data to other shards before removing
        del self.shards[shard_id]
    
    def set(self, key: str, value: Any):
        """Set a value (routed to correct shard)"""
        shard_id = self.hash_ring.get_node(key)
        if shard_id is None:
            raise Exception("No shards available")
        
        self.shards[shard_id][key] = value
        print(f"SET {key} -> shard {shard_id}")
    
    def get(self, key: str) -> Optional[Any]:
        """Get a value (routed to correct shard)"""
        shard_id = self.hash_ring.get_node(key)
        if shard_id is None:
            return None
        
        print(f"GET {key} <- shard {shard_id}")
        return self.shards[shard_id].get(key)
    
    def get_distribution(self) -> Dict[str, int]:
        """Show key distribution across shards"""
        return {
            shard_id: len(data) 
            for shard_id, data in self.shards.items()
        }

# ============== DEMO ==============

def demo_consistent_hashing():
    db = ShardedDatabase()
    
    # Add initial shards
    db.add_shard("shard-1")
    db.add_shard("shard-2")
    db.add_shard("shard-3")
    
    # Insert data
    users = [f"user:{i}" for i in range(100)]
    for user in users:
        db.set(user, {"name": f"User {user}"})
    
    print(f"\nDistribution: {db.get_distribution()}")
    
    # Add a new shard - only ~1/4 of keys should move
    print("\n--- Adding shard-4 ---")
    db.add_shard("shard-4")
    
    # Check where data should go now
    new_distribution = {shard: 0 for shard in db.shards}
    for user in users:
        shard = db.hash_ring.get_node(user)
        new_distribution[shard] += 1
    
    print(f"New distribution: {new_distribution}")
    
    # Remove a shard
    print("\n--- Removing shard-2 ---")
    db.remove_shard("shard-2")
    
    new_distribution = {shard: 0 for shard in db.shards}
    for user in users:
        shard = db.hash_ring.get_node(user)
        new_distribution[shard] += 1
    
    print(f"After removal: {new_distribution}")

# demo_consistent_hashing()
```

### 7.3.4 Hot Spots and Skew

```
┌─────────────────────────────────────────────────────────────┐
│              HOT SPOTS AND SKEW                              │
│                                                              │
│  The Problem:                                               │
│  ────────────                                               │
│  Even with good hashing, some keys get WAY more traffic.    │
│                                                              │
│  Example: Celebrity tweet (millions of reads)               │
│                                                              │
│  Shard 1: 1,000 req/s     ← Normal                         │
│  Shard 2: 1,000 req/s     ← Normal                         │
│  Shard 3: 500,000 req/s   ← HOT! (has celebrity data)      │
│                                                              │
│                                                              │
│  Solutions:                                                 │
│  ──────────                                                 │
│                                                              │
│  1. ADD RANDOM SUFFIX TO HOT KEYS                          │
│     "tweet:123" → "tweet:123:0", "tweet:123:1", etc.       │
│     Spreads hot key across multiple shards                  │
│     Reads must query all suffixes and merge                 │
│                                                              │
│  2. APPLICATION-LEVEL CACHING                              │
│     Cache hot data in memory before it hits the shard       │
│                                                              │
│  3. DEDICATED SHARDS FOR HOT DATA                          │
│     Move known hot users to their own shard cluster         │
│                                                              │
│  4. READ REPLICAS                                          │
│     Add more read replicas for hot shards                   │
└─────────────────────────────────────────────────────────────┘
```

```python
# hot_key_handling.py
import random
from typing import Any, List, Optional

class HotKeyHandler:
    """Handle hot keys by splitting across multiple sub-keys"""
    
    def __init__(self, db, num_splits: int = 10):
        self.db = db
        self.num_splits = num_splits
        self.hot_keys: set = set()
    
    def mark_hot(self, key: str):
        """Mark a key as hot (should be split)"""
        self.hot_keys.add(key)
    
    def set(self, key: str, value: Any):
        """Set value, splitting if key is hot"""
        if key in self.hot_keys:
            # Write to all splits
            for i in range(self.num_splits):
                split_key = f"{key}:split:{i}"
                self.db.set(split_key, value)
        else:
            self.db.set(key, value)
    
    def get(self, key: str) -> Optional[Any]:
        """Get value, reading from random split if hot"""
        if key in self.hot_keys:
            # Read from random split (load balancing)
            split = random.randint(0, self.num_splits - 1)
            split_key = f"{key}:split:{split}"
            return self.db.get(split_key)
        else:
            return self.db.get(key)
    
    def increment(self, key: str, amount: int = 1) -> int:
        """
        For counters: increment random split, 
        aggregate all splits for total
        """
        if key in self.hot_keys:
            # Increment random split
            split = random.randint(0, self.num_splits - 1)
            split_key = f"{key}:split:{split}"
            current = self.db.get(split_key) or 0
            self.db.set(split_key, current + amount)
            
            # Return approximate total (or sum all for exact)
            return (current + amount) * self.num_splits  # Approximate
        else:
            current = self.db.get(key) or 0
            self.db.set(key, current + amount)
            return current + amount
```

---

## 7.4 Combining Replication and Partitioning

```
┌─────────────────────────────────────────────────────────────┐
│        REPLICATION + PARTITIONING TOGETHER                   │
│                                                              │
│  Real systems use BOTH for scalability AND availability.    │
│                                                              │
│                                                              │
│  ┌─────────────────────────────────────────────────────┐   │
│  │                     CLUSTER                          │   │
│  │                                                      │   │
│  │   Partition 1          Partition 2          Partition 3 │
│  │   (Users A-H)          (Users I-P)          (Users Q-Z) │
│  │                                                      │   │
│  │   ┌─────────┐          ┌─────────┐          ┌─────────┐│
│  │   │Leader P1│          │Leader P2│          │Leader P3││
│  │   └────┬────┘          └────┬────┘          └────┬────┘│
│  │        │                    │                    │     │
│  │   ┌────┴────┐          ┌────┴────┐          ┌────┴────┐│
│  │   │Replica  │          │Replica  │          │Replica  ││
│  │   │P1-R1    │          │P2-R1    │          │P3-R1    ││
│  │   └─────────┘          └─────────┘          └─────────┘│
│  │   ┌─────────┐          ┌─────────┐          ┌─────────┐│
│  │   │Replica  │          │Replica  │          │Replica  ││
│  │   │P1-R2    │          │P2-R2    │          │P3-R2    ││
│  │   └─────────┘          └─────────┘          └─────────┘│
│  │                                                      │   │
│  └─────────────────────────────────────────────────────┘   │
│                                                              │
│  Each partition is independently replicated.                │
│  Lose a node? Partition still has replicas.                 │
│  Need more storage? Add more partitions.                    │
│  Need more read throughput? Add more replicas.              │
└─────────────────────────────────────────────────────────────┘
```

---

## Summary

| Concept | Purpose | Trade-offs |
|---------|---------|------------|
| **Single-Leader** | Simple, consistent | Leader bottleneck |
| **Multi-Leader** | Multi-region writes | Conflict resolution |
| **Leaderless** | High availability | Eventual consistency |
| **Range Partitioning** | Efficient range queries | Hot spots |
| **Hash Partitioning** | Even distribution | No range queries |
| **Consistent Hashing** | Minimal reshuffling | More complex |

---

## Practice Exercises

1. Implement single-leader replication with failover
2. Build a CRDT counter that merges correctly
3. Implement consistent hashing with virtual nodes
4. Create a sharded key-value store
5. Simulate hot spots and implement mitigation

**Next Chapter**: Consistency models and the CAP theorem!
