# Chapter 5: Serialization and Protocols

> **Goal**: Understand how data is encoded for transmission and storage in distributed systems

---

## 5.1 What is Serialization?

```
┌─────────────────────────────────────────────────────────────┐
│                   SERIALIZATION                              │
│                                                              │
│  Serialization: Converting in-memory objects to bytes       │
│  Deserialization: Converting bytes back to objects          │
│                                                              │
│   Python Object                    Bytes/String             │
│   ┌───────────────┐               ┌─────────────────┐      │
│   │ {             │  Serialize    │                 │      │
│   │   "name": "A",│  ──────────▶  │ {"name":"A",    │      │
│   │   "age": 25   │               │  "age":25}      │      │
│   │ }             │  ◀──────────  │                 │      │
│   └───────────────┘  Deserialize  └─────────────────┘      │
│                                                              │
│  Why it matters:                                            │
│  ├── Network: Objects can't travel over wire (only bytes)   │
│  ├── Storage: Databases store bytes, not Python objects     │
│  ├── Performance: Serialization can be a bottleneck         │
│  └── Compatibility: Different languages/versions must agree │
└─────────────────────────────────────────────────────────────┘
```

---

## 5.2 JSON (JavaScript Object Notation)

The most common format for web APIs.

### Characteristics

```
┌─────────────────────────────────────────────────────────────┐
│                        JSON                                  │
│                                                              │
│  Pros:                          Cons:                       │
│  ├── Human readable             ├── Verbose (large size)    │
│  ├── Universal support          ├── Slow parsing            │
│  ├── Self-describing            ├── No schema enforcement   │
│  ├── Easy debugging             ├── Limited types           │
│  └── Browser native             └── No binary data          │
│                                                              │
│  Supported Types:                                           │
│  ├── String: "hello"                                        │
│  ├── Number: 42, 3.14                                       │
│  ├── Boolean: true, false                                   │
│  ├── Null: null                                             │
│  ├── Array: [1, 2, 3]                                       │
│  └── Object: {"key": "value"}                               │
│                                                              │
│  NOT Supported:                                             │
│  ├── Dates (use ISO string)                                 │
│  ├── Binary data (use base64)                               │
│  ├── Infinity/NaN                                           │
│  ├── Comments                                               │
│  └── Trailing commas                                        │
└─────────────────────────────────────────────────────────────┘
```

### Python JSON Handling

```python
import json
from datetime import datetime, date
from decimal import Decimal
from enum import Enum
from dataclasses import dataclass, asdict
from typing import Any
import uuid

# ============== BASIC JSON ==============

# Standard serialization
data = {
    "name": "Alice",
    "age": 30,
    "active": True,
    "scores": [95, 87, 91],
    "metadata": None
}

# Serialize (Python -> JSON string)
json_string = json.dumps(data)
print(json_string)
# {"name": "Alice", "age": 30, "active": true, "scores": [95, 87, 91], "metadata": null}

# Pretty print
json_pretty = json.dumps(data, indent=2)
print(json_pretty)

# Deserialize (JSON string -> Python)
parsed = json.loads(json_string)
print(parsed["name"])  # Alice

# ============== HANDLING SPECIAL TYPES ==============

class CustomEncoder(json.JSONEncoder):
    """Handle types JSON doesn't natively support"""
    
    def default(self, obj: Any) -> Any:
        # Datetime -> ISO format string
        if isinstance(obj, datetime):
            return obj.isoformat()
        
        # Date -> ISO format string
        if isinstance(obj, date):
            return obj.isoformat()
        
        # Decimal -> string (preserve precision)
        if isinstance(obj, Decimal):
            return str(obj)
        
        # UUID -> string
        if isinstance(obj, uuid.UUID):
            return str(obj)
        
        # Enum -> value
        if isinstance(obj, Enum):
            return obj.value
        
        # Dataclass -> dict
        if hasattr(obj, '__dataclass_fields__'):
            return asdict(obj)
        
        # Set -> list
        if isinstance(obj, set):
            return list(obj)
        
        # Bytes -> base64
        if isinstance(obj, bytes):
            import base64
            return base64.b64encode(obj).decode('ascii')
        
        return super().default(obj)

# Usage
class Status(Enum):
    ACTIVE = "active"
    INACTIVE = "inactive"

@dataclass
class User:
    id: uuid.UUID
    name: str
    created_at: datetime
    balance: Decimal
    status: Status

user = User(
    id=uuid.uuid4(),
    name="Alice",
    created_at=datetime.now(),
    balance=Decimal("1234.56"),
    status=Status.ACTIVE
)

# Serialize with custom encoder
json_output = json.dumps(user, cls=CustomEncoder, indent=2)
print(json_output)
# {
#   "id": "550e8400-e29b-41d4-a716-446655440000",
#   "name": "Alice", 
#   "created_at": "2024-01-15T10:30:00.123456",
#   "balance": "1234.56",
#   "status": "active"
# }

# ============== CUSTOM DECODER ==============

def custom_decoder(dct: dict) -> Any:
    """Decode special types back to Python objects"""
    
    # Detect and convert datetime strings
    for key, value in dct.items():
        if isinstance(value, str):
            # Try parsing as datetime
            try:
                if 'T' in value and len(value) > 18:
                    dct[key] = datetime.fromisoformat(value)
            except ValueError:
                pass
    
    return dct

# Usage
parsed_user = json.loads(json_output, object_hook=custom_decoder)

# ============== PERFORMANCE: ORJSON ==============

# orjson is 10x faster than standard json
import orjson

# Serialize
json_bytes = orjson.dumps(data)  # Returns bytes, not string!

# With options
json_bytes = orjson.dumps(
    data,
    option=orjson.OPT_INDENT_2 | orjson.OPT_SORT_KEYS
)

# Deserialize
parsed = orjson.loads(json_bytes)

# orjson handles datetime natively!
data_with_datetime = {"timestamp": datetime.now()}
json_bytes = orjson.dumps(data_with_datetime)  # Works!

# ============== STREAMING JSON ==============

# For large files, don't load all into memory
import ijson

async def process_large_json_file(filepath: str):
    """Stream parse a large JSON file"""
    with open(filepath, 'rb') as f:
        # Parse array items one by one
        for item in ijson.items(f, 'items.item'):
            yield item  # Process each item without loading all

# Generate JSON incrementally
def generate_json_stream(items):
    """Generate JSON array without building full string"""
    yield '['
    first = True
    for item in items:
        if not first:
            yield ','
        yield json.dumps(item)
        first = False
    yield ']'
```

---

## 5.3 Protocol Buffers (Protobuf)

Google's binary serialization format - much faster and smaller than JSON.

```
┌─────────────────────────────────────────────────────────────┐
│                   PROTOCOL BUFFERS                           │
│                                                              │
│  Pros:                          Cons:                       │
│  ├── Very compact (binary)      ├── Not human readable      │
│  ├── Very fast                  ├── Requires schema         │
│  ├── Strong typing              ├── Code generation needed  │
│  ├── Schema evolution           ├── Less flexible           │
│  └── Cross-language             └── Harder debugging        │
│                                                              │
│  Size Comparison (same data):                               │
│  ├── JSON:     100 bytes                                    │
│  ├── Protobuf:  35 bytes (65% smaller!)                    │
│                                                              │
│  Speed Comparison:                                          │
│  ├── JSON parse:     1.0x (baseline)                        │
│  ├── Protobuf parse: 0.1x (10x faster!)                    │
└─────────────────────────────────────────────────────────────┘
```

### Protocol Buffer Schema

```protobuf
// user.proto
syntax = "proto3";

package myapp;

// Import common types
import "google/protobuf/timestamp.proto";

// Enum definition
enum UserStatus {
    USER_STATUS_UNSPECIFIED = 0;  // Always have 0 as unspecified
    USER_STATUS_ACTIVE = 1;
    USER_STATUS_INACTIVE = 2;
    USER_STATUS_BANNED = 3;
}

// Message definition
message User {
    // Field numbers are important! Never reuse.
    int32 id = 1;
    string username = 2;
    string email = 3;
    UserStatus status = 4;
    google.protobuf.Timestamp created_at = 5;
    
    // Optional field
    optional string bio = 6;
    
    // Repeated field (array)
    repeated string tags = 7;
    
    // Nested message
    Address address = 8;
    
    // Map field
    map<string, string> metadata = 9;
}

message Address {
    string street = 1;
    string city = 2;
    string country = 3;
    string postal_code = 4;
}

// Request/Response messages for services
message GetUserRequest {
    int32 user_id = 1;
}

message GetUserResponse {
    User user = 1;
}

message ListUsersRequest {
    int32 page_size = 1;
    string page_token = 2;
}

message ListUsersResponse {
    repeated User users = 1;
    string next_page_token = 2;
}
```

### Python with Protobuf

```bash
# Install
pip install protobuf

# Generate Python code
protoc --python_out=. user.proto
# Creates: user_pb2.py
```

```python
# Using generated protobuf classes
from user_pb2 import User, UserStatus, Address, GetUserRequest
from google.protobuf.timestamp_pb2 import Timestamp
from google.protobuf.json_format import MessageToJson, Parse
import time

# Create a User message
user = User()
user.id = 123
user.username = "alice"
user.email = "alice@example.com"
user.status = UserStatus.USER_STATUS_ACTIVE

# Set timestamp
user.created_at.FromDatetime(datetime.now())

# Set optional field
user.bio = "Software Engineer"

# Add to repeated field
user.tags.extend(["python", "distributed-systems"])

# Set nested message
user.address.street = "123 Main St"
user.address.city = "NYC"
user.address.country = "USA"

# Set map field
user.metadata["department"] = "engineering"
user.metadata["level"] = "senior"

# Serialize to bytes
binary_data = user.SerializeToString()
print(f"Binary size: {len(binary_data)} bytes")

# Deserialize from bytes
parsed_user = User()
parsed_user.ParseFromString(binary_data)
print(f"Parsed username: {parsed_user.username}")

# Convert to/from JSON (for debugging)
json_string = MessageToJson(user)
print(json_string)

user_from_json = Parse(json_string, User())

# ============== SIZE COMPARISON ==============

import json

# Same data in JSON
json_data = {
    "id": 123,
    "username": "alice",
    "email": "alice@example.com",
    "status": "active",
    "created_at": "2024-01-15T10:30:00Z",
    "bio": "Software Engineer",
    "tags": ["python", "distributed-systems"],
    "address": {
        "street": "123 Main St",
        "city": "NYC",
        "country": "USA"
    },
    "metadata": {
        "department": "engineering",
        "level": "senior"
    }
}

json_bytes = json.dumps(json_data).encode()
print(f"JSON size: {len(json_bytes)} bytes")
print(f"Protobuf size: {len(binary_data)} bytes")
print(f"Savings: {(1 - len(binary_data)/len(json_bytes)) * 100:.1f}%")
```

---

## 5.4 MessagePack

A binary format that's like "binary JSON" - simpler than Protobuf but faster than JSON.

```
┌─────────────────────────────────────────────────────────────┐
│                    MESSAGEPACK                               │
│                                                              │
│  "It's like JSON, but fast and small"                       │
│                                                              │
│  Pros:                          Cons:                       │
│  ├── No schema needed           ├── Not human readable      │
│  ├── Drop-in JSON replacement   ├── Slightly larger than    │
│  ├── Faster than JSON           │   protobuf                │
│  ├── Smaller than JSON          ├── Less type safety        │
│  └── Easy to use                └── No schema evolution     │
│                                                              │
│  When to use:                                               │
│  ├── Redis cache values                                     │
│  ├── Internal APIs                                          │
│  ├── Real-time data                                         │
│  └── When you want binary without schema                    │
└─────────────────────────────────────────────────────────────┘
```

```python
import msgpack
from datetime import datetime
import json

# Basic usage (like JSON)
data = {
    "name": "Alice",
    "age": 30,
    "scores": [95, 87, 91],
    "active": True
}

# Serialize
packed = msgpack.packb(data)
print(f"MessagePack: {len(packed)} bytes")
print(f"JSON: {len(json.dumps(data).encode())} bytes")

# Deserialize
unpacked = msgpack.unpackb(packed)
print(unpacked)

# ============== CUSTOM TYPES ==============

def encode_datetime(obj):
    """Encode datetime as extension type"""
    if isinstance(obj, datetime):
        return msgpack.ExtType(1, obj.isoformat().encode())
    raise TypeError(f"Unknown type: {type(obj)}")

def decode_datetime(code, data):
    """Decode extension type back to datetime"""
    if code == 1:
        return datetime.fromisoformat(data.decode())
    return msgpack.ExtType(code, data)

# Usage with datetime
data_with_dt = {
    "event": "user_signup",
    "timestamp": datetime.now()
}

packed = msgpack.packb(data_with_dt, default=encode_datetime)
unpacked = msgpack.unpackb(packed, ext_hook=decode_datetime)
print(f"Timestamp: {unpacked['timestamp']}")

# ============== STREAMING ==============

from io import BytesIO

# Pack multiple objects into stream
buffer = BytesIO()
packer = msgpack.Packer()

for i in range(5):
    buffer.write(packer.pack({"id": i, "data": f"item_{i}"}))

# Unpack stream
buffer.seek(0)
unpacker = msgpack.Unpacker(buffer)
for item in unpacker:
    print(item)
```

---

## 5.5 Avro

Schema-based format popular in data pipelines (Kafka, Hadoop).

```
┌─────────────────────────────────────────────────────────────┐
│                        AVRO                                  │
│                                                              │
│  Key Feature: Schema stored WITH data                       │
│                                                              │
│  ┌──────────────────────────────────────────────────┐      │
│  │ Avro File:                                        │      │
│  │ ┌────────────────┬────────────────────────────┐  │      │
│  │ │ Schema (JSON)  │  Binary Data               │  │      │
│  │ │                │  [record1][record2][...]   │  │      │
│  │ └────────────────┴────────────────────────────┘  │      │
│  └──────────────────────────────────────────────────┘      │
│                                                              │
│  Pros:                          Cons:                       │
│  ├── Schema in file itself      ├── Larger than protobuf   │
│  ├── Great schema evolution     ├── More complex           │
│  ├── Dynamic typing possible    ├── Slower than protobuf   │
│  ├── First-class Kafka support  └── Java-centric ecosystem │
│  └── Splittable for MapReduce                               │
│                                                              │
│  Best for:                                                  │
│  ├── Kafka messages                                         │
│  ├── Data lake storage                                      │
│  └── Analytics pipelines                                    │
└─────────────────────────────────────────────────────────────┘
```

```python
import avro.schema
from avro.datafile import DataFileReader, DataFileWriter
from avro.io import DatumReader, DatumWriter, BinaryEncoder, BinaryDecoder
import io
import json

# Define schema
schema_dict = {
    "type": "record",
    "name": "User",
    "namespace": "com.example",
    "fields": [
        {"name": "id", "type": "int"},
        {"name": "username", "type": "string"},
        {"name": "email", "type": "string"},
        {
            "name": "status", 
            "type": {
                "type": "enum",
                "name": "UserStatus",
                "symbols": ["ACTIVE", "INACTIVE", "BANNED"]
            }
        },
        {
            "name": "bio",
            "type": ["null", "string"],  # Union type (optional)
            "default": None
        },
        {
            "name": "tags",
            "type": {"type": "array", "items": "string"},
            "default": []
        }
    ]
}

schema = avro.schema.parse(json.dumps(schema_dict))

# Write to file
users = [
    {"id": 1, "username": "alice", "email": "alice@example.com", 
     "status": "ACTIVE", "bio": "Engineer", "tags": ["python"]},
    {"id": 2, "username": "bob", "email": "bob@example.com",
     "status": "INACTIVE", "bio": None, "tags": []},
]

with open("users.avro", "wb") as f:
    writer = DataFileWriter(f, DatumWriter(), schema)
    for user in users:
        writer.append(user)
    writer.close()

# Read from file
with open("users.avro", "rb") as f:
    reader = DataFileReader(f, DatumReader())
    for user in reader:
        print(user)
    reader.close()

# ============== BINARY ENCODING (for Kafka) ==============

def avro_encode(schema, data: dict) -> bytes:
    """Encode single record to bytes"""
    writer = DatumWriter(schema)
    buffer = io.BytesIO()
    encoder = BinaryEncoder(buffer)
    writer.write(data, encoder)
    return buffer.getvalue()

def avro_decode(schema, data: bytes) -> dict:
    """Decode bytes to record"""
    reader = DatumReader(schema)
    buffer = io.BytesIO(data)
    decoder = BinaryDecoder(buffer)
    return reader.read(decoder)

# Usage
user_data = {
    "id": 123,
    "username": "charlie",
    "email": "charlie@example.com",
    "status": "ACTIVE",
    "bio": {"string": "Developer"},  # Union type needs explicit wrapper
    "tags": ["java", "kafka"]
}

binary = avro_encode(schema, user_data)
print(f"Avro binary size: {len(binary)} bytes")

decoded = avro_decode(schema, binary)
print(decoded)
```

---

## 5.6 Schema Evolution

**Critical for distributed systems**: How do you update your data format without breaking existing services?

```
┌─────────────────────────────────────────────────────────────┐
│                   SCHEMA EVOLUTION                           │
│                                                              │
│  The Problem:                                               │
│  ─────────────                                              │
│  Version 1 Schema          Version 2 Schema                 │
│  ┌──────────────┐         ┌──────────────────┐             │
│  │ id: int      │   →     │ id: int          │             │
│  │ name: string │         │ name: string     │             │
│  └──────────────┘         │ email: string    │ (NEW!)      │
│                           │ age: int         │ (NEW!)      │
│                           └──────────────────┘             │
│                                                              │
│  Old Producer ─────────▶ New Consumer                       │
│  (v1 data)                (expects v2)                      │
│                                                              │
│  New Producer ─────────▶ Old Consumer                       │
│  (v2 data)                (expects v1)                      │
│                                                              │
│  Both need to work!                                         │
└─────────────────────────────────────────────────────────────┘
```

### Compatibility Types

```
┌─────────────────────────────────────────────────────────────┐
│              COMPATIBILITY TYPES                             │
│                                                              │
│  BACKWARD COMPATIBLE                                        │
│  ─────────────────────                                      │
│  New code can read OLD data                                 │
│                                                              │
│  Rules:                                                     │
│  ├── Can ADD fields with defaults                           │
│  ├── Can REMOVE fields                                      │
│  └── Cannot add required fields                             │
│                                                              │
│  Example: New consumer, old producer still running          │
│                                                              │
│                                                              │
│  FORWARD COMPATIBLE                                         │
│  ────────────────────                                       │
│  Old code can read NEW data                                 │
│                                                              │
│  Rules:                                                     │
│  ├── Can ADD fields (old code ignores them)                 │
│  ├── Can REMOVE optional fields                             │
│  └── Cannot remove required fields                          │
│                                                              │
│  Example: Gradual rollout of new producer                   │
│                                                              │
│                                                              │
│  FULL COMPATIBLE (Gold Standard)                            │
│  ─────────────────────────────────                          │
│  Both backward AND forward compatible                       │
│                                                              │
│  Rules:                                                     │
│  ├── Only ADD optional fields with defaults                 │
│  ├── Only REMOVE optional fields                            │
│  └── Never change field types                               │
│                                                              │
│  Most restrictive but safest!                               │
└─────────────────────────────────────────────────────────────┘
```

### Protobuf Evolution Rules

```protobuf
// user_v1.proto
message User {
    int32 id = 1;
    string name = 2;
}

// user_v2.proto - SAFE changes
message User {
    int32 id = 1;
    string name = 2;
    
    // SAFE: Add new optional field
    optional string email = 3;
    
    // SAFE: Add new field with default
    int32 age = 4;  // defaults to 0
    
    // SAFE: Deprecate (but don't remove number!)
    reserved 5;
    reserved "old_field";
}

// UNSAFE changes (DON'T DO):
// ├── Changing field number
// ├── Changing field type (int32 → string)
// ├── Removing required field
// └── Reusing field number
```

```python
# Handling schema evolution in Python

# V1 code reading V2 data (forward compatibility)
def read_user_v1(data: bytes):
    """V1 reader - ignores unknown fields"""
    user = UserV1()
    user.ParseFromString(data)  # Unknown fields silently ignored
    return user

# V2 code reading V1 data (backward compatibility)  
def read_user_v2(data: bytes):
    """V2 reader - uses defaults for missing fields"""
    user = UserV2()
    user.ParseFromString(data)
    
    # Missing fields get defaults
    # email = "" (empty string)
    # age = 0 (zero)
    return user
```

### JSON Schema Evolution

```python
from pydantic import BaseModel, Field
from typing import Optional

# Version 1
class UserV1(BaseModel):
    id: int
    name: str

# Version 2 - Backward compatible
class UserV2(BaseModel):
    id: int
    name: str
    email: Optional[str] = None  # Optional with default
    age: int = 0  # Has default
    
    class Config:
        extra = "ignore"  # Ignore unknown fields (forward compat)

# V1 data can be read by V2
v1_data = {"id": 1, "name": "Alice"}
user_v2 = UserV2(**v1_data)  # Works! email=None, age=0

# V2 data can be read by V1 (if extra="ignore")
v2_data = {"id": 1, "name": "Alice", "email": "a@b.com", "age": 30}
user_v1 = UserV1(**{k: v for k, v in v2_data.items() if k in UserV1.__fields__})
```

---

## 5.7 API Versioning

How to handle breaking changes in your APIs.

```
┌─────────────────────────────────────────────────────────────┐
│                   API VERSIONING STRATEGIES                  │
│                                                              │
│  1. URL Path Versioning                                     │
│  ────────────────────────                                   │
│  GET /api/v1/users                                          │
│  GET /api/v2/users                                          │
│                                                              │
│  Pros: Clear, easy to implement                             │
│  Cons: URL pollution, harder caching                        │
│                                                              │
│                                                              │
│  2. Header Versioning                                       │
│  ─────────────────────                                      │
│  GET /api/users                                             │
│  Accept: application/vnd.myapp.v2+json                      │
│                                                              │
│  Pros: Clean URLs                                           │
│  Cons: Hidden, harder to test                               │
│                                                              │
│                                                              │
│  3. Query Parameter                                         │
│  ──────────────────                                         │
│  GET /api/users?version=2                                   │
│                                                              │
│  Pros: Easy to switch                                       │
│  Cons: Can be forgotten, caching issues                     │
│                                                              │
│                                                              │
│  Recommendation: URL Path for major versions                │
│                  + backward compatible minor changes        │
└─────────────────────────────────────────────────────────────┘
```

```python
from fastapi import FastAPI, Header, APIRouter
from typing import Optional

app = FastAPI()

# ============== URL PATH VERSIONING ==============

v1_router = APIRouter(prefix="/api/v1")
v2_router = APIRouter(prefix="/api/v2")

# V1 endpoints
@v1_router.get("/users/{user_id}")
async def get_user_v1(user_id: int):
    return {
        "id": user_id,
        "name": "Alice"
    }

# V2 endpoints (added fields)
@v2_router.get("/users/{user_id}")  
async def get_user_v2(user_id: int):
    return {
        "id": user_id,
        "name": "Alice",
        "email": "alice@example.com",  # New in v2
        "created_at": "2024-01-15T10:00:00Z"  # New in v2
    }

app.include_router(v1_router)
app.include_router(v2_router)

# ============== HEADER VERSIONING ==============

@app.get("/api/users/{user_id}")
async def get_user_header_version(
    user_id: int,
    accept: Optional[str] = Header(None)
):
    # Parse version from Accept header
    version = 1  # default
    if accept and "v2" in accept:
        version = 2
    
    base_response = {"id": user_id, "name": "Alice"}
    
    if version >= 2:
        base_response["email"] = "alice@example.com"
    
    return base_response

# ============== DEPRECATION HEADERS ==============

from fastapi import Response

@v1_router.get("/users")
async def list_users_v1(response: Response):
    # Warn clients about deprecation
    response.headers["Deprecation"] = "true"
    response.headers["Sunset"] = "2024-12-31"
    response.headers["Link"] = '</api/v2/users>; rel="successor-version"'
    
    return [{"id": 1, "name": "Alice"}]
```

---

## 5.8 Comparison and Benchmarks

```python
import json
import msgpack
import time
from dataclasses import dataclass
import orjson

# Test data
data = {
    "users": [
        {
            "id": i,
            "name": f"user_{i}",
            "email": f"user{i}@example.com",
            "active": i % 2 == 0,
            "scores": [95, 87, 91, 88, 92],
            "metadata": {"key": "value", "count": i}
        }
        for i in range(100)
    ]
}

def benchmark_serialization():
    iterations = 1000
    results = {}
    
    # JSON (standard library)
    start = time.perf_counter()
    for _ in range(iterations):
        encoded = json.dumps(data)
        decoded = json.loads(encoded)
    json_time = time.perf_counter() - start
    json_size = len(json.dumps(data).encode())
    results['json'] = {'time': json_time, 'size': json_size}
    
    # ORJSON (fast JSON)
    start = time.perf_counter()
    for _ in range(iterations):
        encoded = orjson.dumps(data)
        decoded = orjson.loads(encoded)
    orjson_time = time.perf_counter() - start
    orjson_size = len(orjson.dumps(data))
    results['orjson'] = {'time': orjson_time, 'size': orjson_size}
    
    # MessagePack
    start = time.perf_counter()
    for _ in range(iterations):
        encoded = msgpack.packb(data)
        decoded = msgpack.unpackb(encoded)
    msgpack_time = time.perf_counter() - start
    msgpack_size = len(msgpack.packb(data))
    results['msgpack'] = {'time': msgpack_time, 'size': msgpack_size}
    
    # Print results
    print(f"{'Format':<12} {'Time (ms)':<12} {'Size (bytes)':<12} {'Relative':<10}")
    print("-" * 50)
    
    baseline = results['json']['time']
    for name, data in results.items():
        relative = f"{data['time']/baseline:.2f}x"
        print(f"{name:<12} {data['time']*1000:<12.2f} {data['size']:<12} {relative:<10}")

benchmark_serialization()

# Typical output:
# Format       Time (ms)    Size (bytes) Relative  
# --------------------------------------------------
# json         450.00       12500        1.00x     
# orjson       45.00        12500        0.10x     
# msgpack      90.00        9800         0.20x     
```

---

## 5.9 Choosing the Right Format

```
┌─────────────────────────────────────────────────────────────┐
│              FORMAT DECISION GUIDE                           │
│                                                              │
│  Need human readability?                                    │
│  │                                                          │
│  YES ─────▶ JSON (or YAML for config)                      │
│  │                                                          │
│  NO                                                         │
│  │                                                          │
│  Need schema enforcement?                                   │
│  │                                                          │
│  YES ─────▶ Protobuf (fastest) or Avro (best evolution)    │
│  │                                                          │
│  NO                                                         │
│  │                                                          │
│  ▼                                                          │
│  MessagePack (binary JSON, simple)                         │
│                                                              │
│                                                              │
│  RECOMMENDATIONS BY USE CASE:                               │
│  ├── Public REST API: JSON                                  │
│  ├── Internal services: gRPC + Protobuf                     │
│  ├── Kafka messages: Avro (with Schema Registry)            │
│  ├── Redis cache: MessagePack or Protobuf                   │
│  ├── Config files: YAML or JSON                             │
│  ├── Logs: JSON (structured logging)                        │
│  └── Real-time games: Protobuf or FlatBuffers              │
└─────────────────────────────────────────────────────────────┘
```

---

## Summary

| Format | Human Readable | Size | Speed | Schema | Best For |
|--------|---------------|------|-------|--------|----------|
| **JSON** | ✅ Yes | Large | Slow | No | Public APIs |
| **orjson** | ✅ Yes | Large | Fast | No | High-perf JSON |
| **MessagePack** | ❌ No | Medium | Fast | No | Caching |
| **Protobuf** | ❌ No | Small | Fastest | Yes | gRPC, internal |
| **Avro** | ❌ No | Small | Fast | Yes | Kafka, data lakes |

---

## Practice Exercises

1. Benchmark JSON vs MessagePack vs Protobuf for your data
2. Implement a custom JSON encoder for your domain types
3. Define a Protobuf schema and generate Python code
4. Practice schema evolution: add/remove fields safely
5. Implement API versioning in a FastAPI app

**Next Chapter**: Service discovery and load balancing!
