# Chapter 17: Data Pipeline Patterns

> **Goal**: Master batch and stream processing patterns for handling large-scale data

---

## 17.1 Batch vs Stream Processing

```
┌─────────────────────────────────────────────────────────────┐
│           BATCH vs STREAM PROCESSING                         │
│                                                              │
│  BATCH PROCESSING                                           │
│  ─────────────────                                          │
│  Process large amounts of data at once, periodically.       │
│                                                              │
│  ┌──────────┐                      ┌──────────┐            │
│  │  Data    │    ┌──────────┐      │  Output  │            │
│  │  Source  │───▶│  Batch   │─────▶│  (Files, │            │
│  │ (Files,  │    │   Job    │      │   DB,    │            │
│  │  Tables) │    │ (hourly) │      │  Report) │            │
│  └──────────┘    └──────────┘      └──────────┘            │
│                                                              │
│  Characteristics:                                           │
│  ├── High latency (minutes to hours)                        │
│  ├── High throughput                                        │
│  ├── Process bounded dataset                                │
│  ├── Easy to reprocess                                      │
│  └── Simple error handling                                  │
│                                                              │
│  Examples: Daily reports, ML model training, ETL            │
│                                                              │
│                                                              │
│  STREAM PROCESSING                                          │
│  ──────────────────                                         │
│  Process data continuously as it arrives.                   │
│                                                              │
│  ┌──────────┐                      ┌──────────┐            │
│  │  Events  │    ┌──────────┐      │ Real-time│            │
│  │  Stream  │═══▶│  Stream  │═════▶│  Output  │            │
│  │ (Kafka,  │    │ Processor│      │ (Alerts, │            │
│  │  Kinesis)│    │(cont.)   │      │  Updates)│            │
│  └──────────┘    └──────────┘      └──────────┘            │
│                                                              │
│  Characteristics:                                           │
│  ├── Low latency (milliseconds to seconds)                  │
│  ├── Continuous processing                                  │
│  ├── Process unbounded stream                               │
│  ├── Complex exactly-once semantics                         │
│  └── Stateful processing challenges                         │
│                                                              │
│  Examples: Fraud detection, real-time analytics, alerts     │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

---

## 17.2 Batch Processing

### MapReduce Paradigm

```
┌─────────────────────────────────────────────────────────────┐
│                    MAPREDUCE                                 │
│                                                              │
│  Classic paradigm for distributed batch processing.         │
│                                                              │
│  Input Data                                                 │
│  ┌───────────────────────────────────────────────────┐     │
│  │ log1.txt: "error user1 login failed"              │     │
│  │ log2.txt: "error user2 timeout"                   │     │
│  │ log3.txt: "info user1 login success"              │     │
│  │ log4.txt: "error user1 timeout"                   │     │
│  └───────────────────────────────────────────────────┘     │
│                          │                                   │
│                          ▼                                   │
│  ┌───────────────────────────────────────────────────┐     │
│  │                    MAP PHASE                       │     │
│  │  Split data, apply map function to each piece     │     │
│  │                                                    │     │
│  │  Mapper 1: ("error", 1), ("user1", 1)             │     │
│  │  Mapper 2: ("error", 1), ("user2", 1)             │     │
│  │  Mapper 3: ("info", 1), ("user1", 1)              │     │
│  │  Mapper 4: ("error", 1), ("user1", 1)             │     │
│  └───────────────────────────────────────────────────┘     │
│                          │                                   │
│                          ▼                                   │
│  ┌───────────────────────────────────────────────────┐     │
│  │                 SHUFFLE & SORT                     │     │
│  │  Group by key, send to reducers                   │     │
│  │                                                    │     │
│  │  "error": [1, 1, 1]                               │     │
│  │  "info": [1]                                       │     │
│  │  "user1": [1, 1, 1]                               │     │
│  │  "user2": [1]                                      │     │
│  └───────────────────────────────────────────────────┘     │
│                          │                                   │
│                          ▼                                   │
│  ┌───────────────────────────────────────────────────┐     │
│  │                  REDUCE PHASE                      │     │
│  │  Aggregate values for each key                    │     │
│  │                                                    │     │
│  │  Output: {"error": 3, "info": 1,                  │     │
│  │           "user1": 3, "user2": 1}                 │     │
│  └───────────────────────────────────────────────────┘     │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

### Python Batch Processing Examples

```python
# batch_processing.py
"""
Batch processing patterns using Python.
For production, use Spark, Dask, or similar frameworks.
"""

import asyncio
from dataclasses import dataclass
from typing import Iterator, List, Dict, Any, Callable, TypeVar
from datetime import datetime, timedelta
import json
from collections import defaultdict
from concurrent.futures import ProcessPoolExecutor
import multiprocessing as mp

T = TypeVar('T')
R = TypeVar('R')

# ============== SIMPLE MAPREDUCE ==============

class MapReduce:
    """
    Simple MapReduce implementation for understanding.
    For production, use PySpark or Dask.
    """
    
    def __init__(self, num_workers: int = None):
        self.num_workers = num_workers or mp.cpu_count()
    
    def map(
        self,
        data: List[T],
        mapper: Callable[[T], List[tuple]]
    ) -> List[tuple]:
        """Apply mapper function to each element"""
        results = []
        
        # Use multiprocessing for parallelism
        with ProcessPoolExecutor(max_workers=self.num_workers) as executor:
            # Map each chunk
            chunk_size = max(1, len(data) // self.num_workers)
            chunks = [
                data[i:i + chunk_size] 
                for i in range(0, len(data), chunk_size)
            ]
            
            def process_chunk(chunk):
                result = []
                for item in chunk:
                    result.extend(mapper(item))
                return result
            
            for chunk_result in executor.map(process_chunk, chunks):
                results.extend(chunk_result)
        
        return results
    
    def shuffle(self, mapped_data: List[tuple]) -> Dict[str, List[Any]]:
        """Group by key"""
        grouped = defaultdict(list)
        for key, value in mapped_data:
            grouped[key].append(value)
        return dict(grouped)
    
    def reduce(
        self,
        grouped_data: Dict[str, List[Any]],
        reducer: Callable[[str, List[Any]], tuple]
    ) -> Dict[str, Any]:
        """Apply reducer to each group"""
        results = {}
        for key, values in grouped_data.items():
            result_key, result_value = reducer(key, values)
            results[result_key] = result_value
        return results
    
    def execute(
        self,
        data: List[T],
        mapper: Callable[[T], List[tuple]],
        reducer: Callable[[str, List[Any]], tuple]
    ) -> Dict[str, Any]:
        """Execute full MapReduce pipeline"""
        # Map
        mapped = self.map(data, mapper)
        
        # Shuffle
        shuffled = self.shuffle(mapped)
        
        # Reduce
        return self.reduce(shuffled, reducer)

# Example: Word Count
def word_count_mapper(line: str) -> List[tuple]:
    """Map each word to (word, 1)"""
    words = line.lower().split()
    return [(word, 1) for word in words]

def word_count_reducer(word: str, counts: List[int]) -> tuple:
    """Sum counts for each word"""
    return (word, sum(counts))

def demo_word_count():
    mr = MapReduce()
    
    data = [
        "Hello World Hello",
        "World of Distributed Systems",
        "Hello Distributed World"
    ]
    
    result = mr.execute(data, word_count_mapper, word_count_reducer)
    print("Word counts:", result)
    # {'hello': 3, 'world': 3, 'of': 1, 'distributed': 2, 'systems': 1}

# ============== ETL PIPELINE ==============

@dataclass
class ETLStep:
    """Single step in ETL pipeline"""
    name: str
    transform: Callable[[Any], Any]

class ETLPipeline:
    """
    Extract-Transform-Load pipeline for batch processing.
    """
    
    def __init__(self, name: str):
        self.name = name
        self.steps: List[ETLStep] = []
        self.error_handler = None
    
    def add_step(self, name: str, transform: Callable) -> 'ETLPipeline':
        """Add transformation step (fluent interface)"""
        self.steps.append(ETLStep(name=name, transform=transform))
        return self
    
    def on_error(self, handler: Callable) -> 'ETLPipeline':
        """Set error handler"""
        self.error_handler = handler
        return self
    
    def run(self, data: Any) -> Any:
        """Execute pipeline on data"""
        result = data
        
        for step in self.steps:
            try:
                print(f"  Running step: {step.name}")
                result = step.transform(result)
            except Exception as e:
                print(f"  Error in step {step.name}: {e}")
                if self.error_handler:
                    result = self.error_handler(e, result, step)
                else:
                    raise
        
        return result
    
    def run_batch(
        self, 
        items: List[Any], 
        batch_size: int = 1000
    ) -> List[Any]:
        """Process items in batches"""
        results = []
        total = len(items)
        
        for i in range(0, total, batch_size):
            batch = items[i:i + batch_size]
            print(f"Processing batch {i//batch_size + 1} ({len(batch)} items)")
            
            batch_results = [self.run(item) for item in batch]
            results.extend(batch_results)
        
        return results

# Example ETL Pipeline
def create_user_etl_pipeline() -> ETLPipeline:
    """Create pipeline to process user data"""
    
    def extract_fields(raw_data: dict) -> dict:
        """Extract relevant fields"""
        return {
            'user_id': raw_data.get('id'),
            'email': raw_data.get('email', '').lower(),
            'name': raw_data.get('name', ''),
            'created_at': raw_data.get('created_at'),
            'raw_address': raw_data.get('address', {})
        }
    
    def transform_address(data: dict) -> dict:
        """Normalize address"""
        addr = data.pop('raw_address', {})
        data['city'] = addr.get('city', '').title()
        data['country'] = addr.get('country', '').upper()
        return data
    
    def validate(data: dict) -> dict:
        """Validate required fields"""
        if not data.get('email'):
            raise ValueError("Email is required")
        if '@' not in data['email']:
            raise ValueError("Invalid email format")
        return data
    
    def add_metadata(data: dict) -> dict:
        """Add processing metadata"""
        data['processed_at'] = datetime.utcnow().isoformat()
        data['etl_version'] = '1.0'
        return data
    
    return (
        ETLPipeline("user-etl")
        .add_step("extract", extract_fields)
        .add_step("transform_address", transform_address)
        .add_step("validate", validate)
        .add_step("add_metadata", add_metadata)
        .on_error(lambda e, data, step: {**data, 'error': str(e), 'failed_step': step.name})
    )

# ============== SCHEDULED BATCH JOBS ==============

class BatchJobScheduler:
    """Simple batch job scheduler"""
    
    def __init__(self):
        self.jobs: Dict[str, dict] = {}
        self.running = False
    
    def schedule(
        self,
        name: str,
        job: Callable,
        interval_hours: int,
        args: tuple = ()
    ):
        """Schedule a recurring batch job"""
        self.jobs[name] = {
            'job': job,
            'interval': timedelta(hours=interval_hours),
            'last_run': None,
            'args': args
        }
    
    async def run(self):
        """Start the scheduler"""
        self.running = True
        print("Batch scheduler started")
        
        while self.running:
            now = datetime.utcnow()
            
            for name, config in self.jobs.items():
                last_run = config['last_run']
                interval = config['interval']
                
                should_run = (
                    last_run is None or 
                    (now - last_run) >= interval
                )
                
                if should_run:
                    print(f"\n[{now}] Running job: {name}")
                    try:
                        config['job'](*config['args'])
                        config['last_run'] = now
                        print(f"[{now}] Job {name} completed")
                    except Exception as e:
                        print(f"[{now}] Job {name} failed: {e}")
            
            await asyncio.sleep(60)  # Check every minute
    
    def stop(self):
        self.running = False

# Example batch jobs
def daily_report_job():
    """Generate daily report"""
    print("Generating daily report...")
    # Query database, aggregate data, create report

def hourly_sync_job(source: str, target: str):
    """Sync data between systems"""
    print(f"Syncing from {source} to {target}...")
    # Extract from source, transform, load to target

def demo_scheduler():
    scheduler = BatchJobScheduler()
    
    scheduler.schedule(
        name="daily-report",
        job=daily_report_job,
        interval_hours=24
    )
    
    scheduler.schedule(
        name="hourly-sync",
        job=hourly_sync_job,
        interval_hours=1,
        args=("system-a", "system-b")
    )
    
    # asyncio.run(scheduler.run())
```

---

## 17.3 Stream Processing

### Core Concepts

```
┌─────────────────────────────────────────────────────────────┐
│            STREAM PROCESSING CONCEPTS                        │
│                                                              │
│  EVENT                                                      │
│  ─────                                                      │
│  Single record in the stream.                               │
│  {"user_id": "123", "action": "click", "ts": "..."}        │
│                                                              │
│                                                              │
│  STREAM                                                     │
│  ──────                                                     │
│  Unbounded sequence of events.                              │
│  [event1] → [event2] → [event3] → [event4] → ...           │
│                                                              │
│                                                              │
│  WINDOWING                                                  │
│  ─────────                                                  │
│  Group events into finite chunks for aggregation.           │
│                                                              │
│  Tumbling Window (non-overlapping):                         │
│  ├────────┤├────────┤├────────┤                            │
│  │ 0-5min ││ 5-10min││10-15min│                            │
│  ├────────┤├────────┤├────────┤                            │
│                                                              │
│  Sliding Window (overlapping):                              │
│  ├────────────┤                                             │
│       ├────────────┤                                        │
│            ├────────────┤                                   │
│                                                              │
│  Session Window (activity-based):                           │
│  ├──────┤      ├────────────┤    ├───┤                     │
│  │user A│      │   user A   │    │ A │                     │
│  ├──────┤      ├────────────┤    ├───┤                     │
│  (gap)          (gap)              (gap)                    │
│                                                              │
│                                                              │
│  WATERMARKS                                                 │
│  ──────────                                                 │
│  Track event time progress, handle late events.             │
│                                                              │
│  Event Time: When event occurred (in the data)              │
│  Processing Time: When event is processed (wall clock)      │
│  Watermark: "All events before this time have arrived"      │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

### Python Stream Processing

```python
# stream_processing.py
"""
Stream processing patterns in Python.
For production, use Kafka Streams, Flink, or similar.
"""

import asyncio
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from typing import (
    AsyncIterator, Callable, Dict, List, Any, 
    Optional, TypeVar, Generic
)
from collections import defaultdict
import json
import time

T = TypeVar('T')
R = TypeVar('R')

# ============== EVENT & STREAM ==============

@dataclass
class Event(Generic[T]):
    """A single event in a stream"""
    key: str
    value: T
    timestamp: datetime
    partition: int = 0

class EventStream(Generic[T]):
    """
    Unbounded stream of events.
    Provides operators for stream processing.
    """
    
    def __init__(self, source: AsyncIterator[Event[T]]):
        self._source = source
    
    async def __aiter__(self):
        async for event in self._source:
            yield event
    
    def map(self, func: Callable[[T], R]) -> 'EventStream[R]':
        """Transform each event"""
        async def mapped():
            async for event in self._source:
                yield Event(
                    key=event.key,
                    value=func(event.value),
                    timestamp=event.timestamp,
                    partition=event.partition
                )
        return EventStream(mapped())
    
    def filter(self, predicate: Callable[[T], bool]) -> 'EventStream[T]':
        """Filter events"""
        async def filtered():
            async for event in self._source:
                if predicate(event.value):
                    yield event
        return EventStream(filtered())
    
    def key_by(self, key_func: Callable[[T], str]) -> 'EventStream[T]':
        """Re-key events"""
        async def rekeyed():
            async for event in self._source:
                yield Event(
                    key=key_func(event.value),
                    value=event.value,
                    timestamp=event.timestamp,
                    partition=event.partition
                )
        return EventStream(rekeyed())
    
    async def to_list(self, max_events: int = None) -> List[Event[T]]:
        """Collect events to list (for testing)"""
        events = []
        async for event in self._source:
            events.append(event)
            if max_events and len(events) >= max_events:
                break
        return events

# ============== WINDOWING ==============

@dataclass
class Window:
    """A time window"""
    start: datetime
    end: datetime
    
    def contains(self, timestamp: datetime) -> bool:
        return self.start <= timestamp < self.end

class TumblingWindow:
    """
    Non-overlapping fixed-size windows.
    Each event belongs to exactly one window.
    """
    
    def __init__(self, size: timedelta):
        self.size = size
    
    def get_window(self, timestamp: datetime) -> Window:
        """Get window for a timestamp"""
        # Align to window boundaries
        window_start_ts = (
            int(timestamp.timestamp()) // 
            int(self.size.total_seconds()) * 
            int(self.size.total_seconds())
        )
        start = datetime.fromtimestamp(window_start_ts)
        end = start + self.size
        return Window(start=start, end=end)

class SlidingWindow:
    """
    Overlapping windows.
    Each event may belong to multiple windows.
    """
    
    def __init__(self, size: timedelta, slide: timedelta):
        self.size = size
        self.slide = slide
    
    def get_windows(self, timestamp: datetime) -> List[Window]:
        """Get all windows containing this timestamp"""
        windows = []
        
        # Find earliest window that could contain this timestamp
        earliest_start = timestamp - self.size + self.slide
        
        # Align to slide boundary
        slide_seconds = int(self.slide.total_seconds())
        aligned_ts = (
            int(earliest_start.timestamp()) // 
            slide_seconds * slide_seconds
        )
        
        current_start = datetime.fromtimestamp(aligned_ts)
        
        while current_start <= timestamp:
            window = Window(
                start=current_start,
                end=current_start + self.size
            )
            if window.contains(timestamp):
                windows.append(window)
            current_start += self.slide
        
        return windows

class SessionWindow:
    """
    Dynamic windows based on activity gaps.
    Window closes after period of inactivity.
    """
    
    def __init__(self, gap: timedelta):
        self.gap = gap
        self.sessions: Dict[str, Window] = {}  # key -> current session
    
    def process(self, key: str, timestamp: datetime) -> Optional[Window]:
        """
        Process event and return closed window if any.
        """
        if key in self.sessions:
            current = self.sessions[key]
            
            # Check if within gap
            if timestamp <= current.end + self.gap:
                # Extend session
                self.sessions[key] = Window(
                    start=current.start,
                    end=timestamp
                )
                return None
            else:
                # Gap exceeded - close session and start new
                closed = current
                self.sessions[key] = Window(start=timestamp, end=timestamp)
                return closed
        else:
            # New session
            self.sessions[key] = Window(start=timestamp, end=timestamp)
            return None

# ============== AGGREGATIONS ==============

class WindowedAggregator(Generic[T, R]):
    """
    Aggregate events within windows.
    """
    
    def __init__(
        self,
        window: TumblingWindow,
        aggregate_func: Callable[[List[T]], R],
        key_func: Callable[[T], str] = lambda x: "default"
    ):
        self.window = window
        self.aggregate_func = aggregate_func
        self.key_func = key_func
        self.buffers: Dict[str, Dict[Window, List[T]]] = defaultdict(
            lambda: defaultdict(list)
        )
        self.watermark: Optional[datetime] = None
    
    def process(self, event: Event[T]) -> List[tuple]:
        """
        Process event and return completed aggregations.
        Returns list of (key, window, result) tuples.
        """
        key = self.key_func(event.value)
        window = self.window.get_window(event.timestamp)
        
        # Add to buffer
        self.buffers[key][window].append(event.value)
        
        # Update watermark
        if self.watermark is None or event.timestamp > self.watermark:
            self.watermark = event.timestamp
        
        # Check for completed windows (watermark passed)
        completed = []
        for k, windows in list(self.buffers.items()):
            for w, values in list(windows.items()):
                if self.watermark and w.end <= self.watermark:
                    result = self.aggregate_func(values)
                    completed.append((k, w, result))
                    del windows[w]
        
        return completed

# ============== STREAM PROCESSOR ==============

class StreamProcessor:
    """
    Complete stream processing application.
    """
    
    def __init__(self, name: str):
        self.name = name
        self.handlers: List[Callable] = []
        self.running = False
    
    def add_handler(self, handler: Callable):
        """Add event handler"""
        self.handlers.append(handler)
    
    async def process(self, source: AsyncIterator[Event]):
        """Process events from source"""
        self.running = True
        
        async for event in source:
            if not self.running:
                break
            
            for handler in self.handlers:
                try:
                    await handler(event)
                except Exception as e:
                    print(f"Handler error: {e}")
    
    def stop(self):
        self.running = False

# ============== EXAMPLE: REAL-TIME ANALYTICS ==============

@dataclass
class ClickEvent:
    user_id: str
    page: str
    timestamp: datetime

@dataclass
class PageViewCount:
    page: str
    count: int
    window_start: datetime
    window_end: datetime

class RealTimeAnalytics:
    """
    Real-time page view analytics.
    """
    
    def __init__(self):
        self.window = TumblingWindow(size=timedelta(minutes=1))
        self.aggregator = WindowedAggregator(
            window=self.window,
            aggregate_func=len,  # Count events
            key_func=lambda e: e.page
        )
        self.results: List[PageViewCount] = []
    
    async def process_event(self, event: Event[ClickEvent]):
        """Process a click event"""
        completed = self.aggregator.process(event)
        
        for page, window, count in completed:
            result = PageViewCount(
                page=page,
                count=count,
                window_start=window.start,
                window_end=window.end
            )
            self.results.append(result)
            print(f"Page views: {page} = {count} ({window.start} to {window.end})")

# ============== EXAMPLE: FRAUD DETECTION ==============

@dataclass
class Transaction:
    user_id: str
    amount: float
    merchant: str
    location: str
    timestamp: datetime

class FraudDetector:
    """
    Detect potentially fraudulent transactions.
    """
    
    def __init__(self):
        # Track recent transactions per user (session window)
        self.user_sessions = SessionWindow(gap=timedelta(hours=1))
        self.recent_transactions: Dict[str, List[Transaction]] = defaultdict(list)
        
        # Alert thresholds
        self.max_amount = 10000
        self.max_transactions_per_hour = 10
        self.suspicious_velocity_km_per_hour = 500
    
    async def process(self, event: Event[Transaction]) -> List[str]:
        """Process transaction and return alerts"""
        tx = event.value
        alerts = []
        
        # Rule 1: High amount
        if tx.amount > self.max_amount:
            alerts.append(f"HIGH_AMOUNT: ${tx.amount} by {tx.user_id}")
        
        # Rule 2: Too many transactions
        self.recent_transactions[tx.user_id].append(tx)
        recent = [
            t for t in self.recent_transactions[tx.user_id]
            if (tx.timestamp - t.timestamp) <= timedelta(hours=1)
        ]
        self.recent_transactions[tx.user_id] = recent
        
        if len(recent) > self.max_transactions_per_hour:
            alerts.append(
                f"HIGH_VELOCITY: {len(recent)} transactions in 1 hour by {tx.user_id}"
            )
        
        # Rule 3: Impossible travel (simplified)
        if len(recent) >= 2:
            prev_tx = recent[-2]
            if prev_tx.location != tx.location:
                time_diff = (tx.timestamp - prev_tx.timestamp).total_seconds() / 3600
                if time_diff < 1:  # Less than 1 hour between different locations
                    alerts.append(
                        f"IMPOSSIBLE_TRAVEL: {tx.user_id} from {prev_tx.location} "
                        f"to {tx.location} in {time_diff:.2f} hours"
                    )
        
        return alerts

# ============== DEMO ==============

async def generate_click_events() -> AsyncIterator[Event[ClickEvent]]:
    """Generate sample click events"""
    pages = ["/home", "/products", "/cart", "/checkout"]
    users = ["user1", "user2", "user3"]
    
    for i in range(100):
        yield Event(
            key=f"event-{i}",
            value=ClickEvent(
                user_id=users[i % len(users)],
                page=pages[i % len(pages)],
                timestamp=datetime.utcnow() + timedelta(seconds=i * 2)
            ),
            timestamp=datetime.utcnow() + timedelta(seconds=i * 2)
        )
        await asyncio.sleep(0.1)

async def demo_stream_processing():
    print("=== Stream Processing Demo ===\n")
    
    # Create analytics processor
    analytics = RealTimeAnalytics()
    
    # Process events
    async for event in generate_click_events():
        await analytics.process_event(event)
    
    print("\n=== Final Results ===")
    for result in analytics.results:
        print(f"  {result.page}: {result.count} views")

# asyncio.run(demo_stream_processing())
```

---

## 17.4 Lambda Architecture

```
┌─────────────────────────────────────────────────────────────┐
│                  LAMBDA ARCHITECTURE                         │
│                                                              │
│  Combines batch and stream processing for best of both.     │
│                                                              │
│                                                              │
│                    ┌─────────────────┐                      │
│                    │   Data Source   │                      │
│                    │   (All Data)    │                      │
│                    └────────┬────────┘                      │
│                             │                                │
│              ┌──────────────┴──────────────┐                │
│              │                             │                 │
│              ▼                             ▼                 │
│   ┌────────────────────┐       ┌────────────────────┐      │
│   │    BATCH LAYER     │       │    SPEED LAYER     │      │
│   │                    │       │                    │      │
│   │  • All historical  │       │  • Recent data     │      │
│   │  • High latency    │       │  • Low latency     │      │
│   │  • High accuracy   │       │  • Approximate     │      │
│   │  • Recomputes all  │       │  • Real-time       │      │
│   │                    │       │                    │      │
│   │  (Spark, Hadoop)   │       │  (Kafka Streams)   │      │
│   └─────────┬──────────┘       └─────────┬──────────┘      │
│             │                             │                  │
│             │   Batch Views              │  Real-time Views │
│             │                             │                  │
│             └──────────────┬──────────────┘                 │
│                            │                                 │
│                            ▼                                 │
│              ┌─────────────────────────┐                    │
│              │     SERVING LAYER       │                    │
│              │                         │                    │
│              │  Merge batch + realtime │                    │
│              │  to serve queries       │                    │
│              │                         │                    │
│              └─────────────────────────┘                    │
│                            │                                 │
│                            ▼                                 │
│                    ┌──────────────┐                         │
│                    │   Queries    │                         │
│                    └──────────────┘                         │
│                                                              │
│  Pros:                      Cons:                           │
│  ├── Accuracy of batch      ├── Complex (two systems)       │
│  ├── Speed of streaming     ├── Duplicate logic             │
│  └── Fault tolerant         └── Hard to maintain            │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

```python
# lambda_architecture.py
"""
Lambda architecture implementation.
Combines batch and stream processing.
"""

from dataclasses import dataclass
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from collections import defaultdict
import asyncio

@dataclass
class PageView:
    page: str
    user_id: str
    timestamp: datetime

@dataclass
class ViewCount:
    page: str
    count: int
    timestamp: datetime

# ============== BATCH LAYER ==============

class BatchLayer:
    """
    Processes all historical data for accuracy.
    Runs periodically (e.g., hourly, daily).
    """
    
    def __init__(self):
        self.master_dataset: List[PageView] = []  # All historical data
        self.batch_views: Dict[str, int] = {}  # Pre-computed views
        self.last_batch_time: Optional[datetime] = None
    
    def append_to_master(self, events: List[PageView]):
        """Append new data to master dataset"""
        self.master_dataset.extend(events)
    
    def run_batch_job(self) -> Dict[str, int]:
        """
        Recompute all views from scratch.
        This is slow but accurate.
        """
        print("Running batch job on all historical data...")
        
        # Count views per page (simple aggregation)
        counts = defaultdict(int)
        for view in self.master_dataset:
            counts[view.page] += 1
        
        self.batch_views = dict(counts)
        self.last_batch_time = datetime.utcnow()
        
        print(f"Batch job complete. Processed {len(self.master_dataset)} records.")
        return self.batch_views
    
    def get_batch_views(self) -> Dict[str, int]:
        """Get pre-computed batch views"""
        return self.batch_views

# ============== SPEED LAYER ==============

class SpeedLayer:
    """
    Processes only recent data for low latency.
    Provides real-time updates between batch jobs.
    """
    
    def __init__(self):
        self.realtime_views: Dict[str, int] = defaultdict(int)
        self.events_since_batch: List[PageView] = []
        self.batch_cutoff: Optional[datetime] = None
    
    def set_batch_cutoff(self, cutoff: datetime):
        """Set the time when last batch job ran"""
        self.batch_cutoff = cutoff
        # Clear old real-time data (now in batch)
        self.realtime_views.clear()
        self.events_since_batch.clear()
    
    def process_event(self, event: PageView):
        """Process real-time event"""
        # Only process events after batch cutoff
        if self.batch_cutoff and event.timestamp < self.batch_cutoff:
            return
        
        self.events_since_batch.append(event)
        self.realtime_views[event.page] += 1
    
    def get_realtime_views(self) -> Dict[str, int]:
        """Get real-time view counts (since last batch)"""
        return dict(self.realtime_views)
    
    def get_events_for_batch(self) -> List[PageView]:
        """Get events to be included in next batch"""
        return self.events_since_batch.copy()

# ============== SERVING LAYER ==============

class ServingLayer:
    """
    Merges batch and real-time views for queries.
    """
    
    def __init__(self, batch_layer: BatchLayer, speed_layer: SpeedLayer):
        self.batch_layer = batch_layer
        self.speed_layer = speed_layer
    
    def query_page_views(self, page: Optional[str] = None) -> Dict[str, int]:
        """
        Query page view counts.
        Merges batch views with real-time updates.
        """
        batch_views = self.batch_layer.get_batch_views()
        realtime_views = self.speed_layer.get_realtime_views()
        
        # Merge: batch + realtime
        merged = defaultdict(int)
        
        for p, count in batch_views.items():
            merged[p] = count
        
        for p, count in realtime_views.items():
            merged[p] += count
        
        if page:
            return {page: merged.get(page, 0)}
        
        return dict(merged)

# ============== LAMBDA SYSTEM ==============

class LambdaArchitecture:
    """
    Complete Lambda architecture system.
    """
    
    def __init__(self, batch_interval_seconds: int = 60):
        self.batch_layer = BatchLayer()
        self.speed_layer = SpeedLayer()
        self.serving_layer = ServingLayer(self.batch_layer, self.speed_layer)
        self.batch_interval = batch_interval_seconds
        self.running = False
    
    def ingest(self, event: PageView):
        """Ingest new event into both layers"""
        # Speed layer processes immediately
        self.speed_layer.process_event(event)
        
        # Batch layer stores for next batch job
        self.batch_layer.append_to_master([event])
    
    async def run_batch_scheduler(self):
        """Periodically run batch jobs"""
        self.running = True
        
        while self.running:
            # Run batch job
            self.batch_layer.run_batch_job()
            
            # Update speed layer cutoff
            self.speed_layer.set_batch_cutoff(datetime.utcnow())
            
            # Wait for next batch
            await asyncio.sleep(self.batch_interval)
    
    def query(self, page: Optional[str] = None) -> Dict[str, int]:
        """Query merged views"""
        return self.serving_layer.query_page_views(page)
    
    def stop(self):
        self.running = False

# ============== DEMO ==============

async def demo_lambda():
    print("=== Lambda Architecture Demo ===\n")
    
    # Create system with 10-second batch interval
    system = LambdaArchitecture(batch_interval_seconds=10)
    
    # Start batch scheduler in background
    batch_task = asyncio.create_task(system.run_batch_scheduler())
    
    # Simulate ingesting events
    pages = ["/home", "/products", "/cart"]
    
    for i in range(30):
        event = PageView(
            page=pages[i % len(pages)],
            user_id=f"user-{i % 5}",
            timestamp=datetime.utcnow()
        )
        system.ingest(event)
        
        # Query current state
        if i % 5 == 0:
            results = system.query()
            print(f"Event {i}: Current counts: {results}")
        
        await asyncio.sleep(0.5)
    
    system.stop()
    batch_task.cancel()

# asyncio.run(demo_lambda())
```

---

## 17.5 Kappa Architecture

```
┌─────────────────────────────────────────────────────────────┐
│                   KAPPA ARCHITECTURE                         │
│                                                              │
│  Simplification: Everything is a stream!                    │
│  No separate batch layer.                                   │
│                                                              │
│                    ┌─────────────────┐                      │
│                    │   Data Source   │                      │
│                    └────────┬────────┘                      │
│                             │                                │
│                             ▼                                │
│              ┌─────────────────────────┐                    │
│              │     Event Log (Kafka)   │                    │
│              │   Immutable, replayable │                    │
│              │   All historical data   │                    │
│              └────────────┬────────────┘                    │
│                           │                                  │
│                           ▼                                  │
│              ┌─────────────────────────┐                    │
│              │    STREAM PROCESSOR     │                    │
│              │                         │                    │
│              │  • Real-time processing │                    │
│              │  • Can replay from any  │                    │
│              │    point in time        │                    │
│              │                         │                    │
│              └────────────┬────────────┘                    │
│                           │                                  │
│                           ▼                                  │
│              ┌─────────────────────────┐                    │
│              │     SERVING LAYER       │                    │
│              └─────────────────────────┘                    │
│                                                              │
│                                                              │
│  Reprocessing in Kappa:                                     │
│  ──────────────────────                                     │
│  Need to fix a bug or change logic?                         │
│                                                              │
│  1. Deploy new processor version (v2)                       │
│  2. Start v2 from beginning of log                          │
│  3. When v2 catches up, switch serving                      │
│  4. Shut down v1                                            │
│                                                              │
│    Log: [e1][e2][e3][e4][e5][e6][e7][e8]...                │
│           └─────────────────────────────▶ v1 (current)     │
│           └───────────────────▶ v2 (catching up)           │
│                                                              │
│                                                              │
│  Pros:                      Cons:                           │
│  ├── Simpler (one system)   ├── Need replayable log        │
│  ├── Single codebase        ├── Reprocessing can be slow   │
│  └── Easier operations      └── Log storage costs          │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

---

## 17.6 Change Data Capture (CDC)

```
┌─────────────────────────────────────────────────────────────┐
│              CHANGE DATA CAPTURE (CDC)                       │
│                                                              │
│  Capture database changes as a stream of events.            │
│                                                              │
│                                                              │
│   ┌───────────────────┐                                     │
│   │    PostgreSQL     │                                     │
│   │    ┌───────────┐  │                                     │
│   │    │  Table:   │  │                                     │
│   │    │  users    │  │                                     │
│   │    └───────────┘  │                                     │
│   │         │         │                                     │
│   │         ▼         │                                     │
│   │  ┌────────────┐   │                                     │
│   │  │ WAL (Write │   │                                     │
│   │  │ Ahead Log) │   │                                     │
│   │  └─────┬──────┘   │                                     │
│   └────────│──────────┘                                     │
│            │                                                 │
│            ▼                                                 │
│   ┌─────────────────┐                                       │
│   │    Debezium     │  (CDC connector)                      │
│   │  (reads WAL)    │                                       │
│   └────────┬────────┘                                       │
│            │                                                 │
│            ▼                                                 │
│   ┌─────────────────┐                                       │
│   │     Kafka       │                                       │
│   │                 │                                       │
│   │ Topic: db.users │                                       │
│   │ {"op":"c",...}  │  (create)                            │
│   │ {"op":"u",...}  │  (update)                            │
│   │ {"op":"d",...}  │  (delete)                            │
│   └────────┬────────┘                                       │
│            │                                                 │
│     ┌──────┴──────┬──────────────┐                         │
│     ▼             ▼              ▼                          │
│ ┌────────┐  ┌──────────┐  ┌────────────┐                  │
│ │ Search │  │  Cache   │  │  Analytics │                   │
│ │ Index  │  │ Invalidate│ │  Pipeline  │                   │
│ └────────┘  └──────────┘  └────────────┘                   │
│                                                              │
│                                                              │
│  Use Cases:                                                 │
│  ├── Keep search index in sync                              │
│  ├── Invalidate caches on change                            │
│  ├── Replicate to data warehouse                            │
│  ├── Trigger workflows on data changes                      │
│  └── Build materialized views                               │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

```python
# cdc_processing.py
"""
Change Data Capture processing example.
"""

from dataclasses import dataclass
from datetime import datetime
from typing import Dict, Any, Optional, Callable, List
from enum import Enum
import json
import asyncio

class OperationType(Enum):
    CREATE = "c"
    UPDATE = "u"
    DELETE = "d"
    READ = "r"  # Snapshot read

@dataclass
class CDCEvent:
    """
    CDC event format (similar to Debezium).
    """
    operation: OperationType
    table: str
    before: Optional[Dict[str, Any]]  # Previous state (for update/delete)
    after: Optional[Dict[str, Any]]   # New state (for create/update)
    timestamp: datetime
    source: Dict[str, Any]  # Source metadata (position, etc.)

class CDCProcessor:
    """
    Process CDC events for various use cases.
    """
    
    def __init__(self):
        self.handlers: Dict[str, List[Callable]] = {}
    
    def on_table(self, table: str, handler: Callable):
        """Register handler for table changes"""
        if table not in self.handlers:
            self.handlers[table] = []
        self.handlers[table].append(handler)
    
    async def process(self, event: CDCEvent):
        """Process a CDC event"""
        handlers = self.handlers.get(event.table, [])
        
        for handler in handlers:
            try:
                await handler(event)
            except Exception as e:
                print(f"Handler error: {e}")

# ============== USE CASE: SEARCH INDEX SYNC ==============

class SearchIndexSyncer:
    """
    Keep Elasticsearch index in sync with database.
    """
    
    def __init__(self, es_client):
        self.es = es_client
        self.index_name = "users"
    
    async def handle_user_change(self, event: CDCEvent):
        """Handle user table changes"""
        
        if event.operation == OperationType.CREATE:
            # Index new user
            user = event.after
            await self.es.index(
                index=self.index_name,
                id=user['id'],
                document={
                    'name': user['name'],
                    'email': user['email'],
                    'created_at': user['created_at']
                }
            )
            print(f"Indexed user {user['id']}")
        
        elif event.operation == OperationType.UPDATE:
            # Update existing document
            user = event.after
            await self.es.update(
                index=self.index_name,
                id=user['id'],
                doc={
                    'name': user['name'],
                    'email': user['email']
                }
            )
            print(f"Updated user {user['id']} in index")
        
        elif event.operation == OperationType.DELETE:
            # Remove from index
            user = event.before
            await self.es.delete(
                index=self.index_name,
                id=user['id']
            )
            print(f"Removed user {user['id']} from index")

# ============== USE CASE: CACHE INVALIDATION ==============

class CacheInvalidator:
    """
    Invalidate cache entries on data changes.
    """
    
    def __init__(self, redis_client):
        self.redis = redis_client
    
    async def handle_user_change(self, event: CDCEvent):
        """Invalidate user cache on any change"""
        
        # Get user ID from before or after
        user_data = event.after or event.before
        user_id = user_data['id']
        
        # Delete cached data
        cache_keys = [
            f"user:{user_id}",
            f"user:{user_id}:profile",
            f"user:{user_id}:permissions"
        ]
        
        for key in cache_keys:
            await self.redis.delete(key)
        
        print(f"Invalidated cache for user {user_id}")

# ============== USE CASE: AUDIT LOG ==============

class AuditLogger:
    """
    Build audit log from CDC events.
    """
    
    def __init__(self):
        self.audit_log: List[Dict] = []
    
    async def handle_change(self, event: CDCEvent):
        """Log all changes for audit"""
        
        audit_entry = {
            'timestamp': event.timestamp.isoformat(),
            'table': event.table,
            'operation': event.operation.value,
            'changes': self._compute_changes(event)
        }
        
        self.audit_log.append(audit_entry)
        print(f"Audit: {event.operation.name} on {event.table}")
    
    def _compute_changes(self, event: CDCEvent) -> Dict:
        """Compute what changed"""
        if event.operation == OperationType.CREATE:
            return {'new': event.after}
        
        elif event.operation == OperationType.DELETE:
            return {'deleted': event.before}
        
        elif event.operation == OperationType.UPDATE:
            changes = {}
            for key in event.after:
                old_val = event.before.get(key) if event.before else None
                new_val = event.after.get(key)
                if old_val != new_val:
                    changes[key] = {'from': old_val, 'to': new_val}
            return changes
        
        return {}

# ============== DEMO ==============

async def demo_cdc():
    print("=== CDC Processing Demo ===\n")
    
    # Create processor
    processor = CDCProcessor()
    
    # Register handlers
    audit = AuditLogger()
    processor.on_table("users", audit.handle_change)
    
    # Simulate CDC events
    events = [
        CDCEvent(
            operation=OperationType.CREATE,
            table="users",
            before=None,
            after={"id": 1, "name": "Alice", "email": "alice@example.com"},
            timestamp=datetime.utcnow(),
            source={"position": 1}
        ),
        CDCEvent(
            operation=OperationType.UPDATE,
            table="users",
            before={"id": 1, "name": "Alice", "email": "alice@example.com"},
            after={"id": 1, "name": "Alice Smith", "email": "alice@example.com"},
            timestamp=datetime.utcnow(),
            source={"position": 2}
        ),
        CDCEvent(
            operation=OperationType.DELETE,
            table="users",
            before={"id": 1, "name": "Alice Smith", "email": "alice@example.com"},
            after=None,
            timestamp=datetime.utcnow(),
            source={"position": 3}
        ),
    ]
    
    for event in events:
        await processor.process(event)
    
    print("\n=== Audit Log ===")
    for entry in audit.audit_log:
        print(f"  {entry}")

# asyncio.run(demo_cdc())
```

---

## Summary

| Pattern | Latency | Throughput | Complexity | Use Case |
|---------|---------|------------|------------|----------|
| **Batch** | High | Very High | Low | Reports, ETL |
| **Stream** | Low | High | Medium | Real-time alerts |
| **Lambda** | Low | Very High | High | Both accuracy and speed |
| **Kappa** | Low | High | Medium | Simplified Lambda |
| **CDC** | Low | Medium | Low | Data sync |

---

## Practice Exercises

1. Implement a MapReduce word count
2. Build a streaming aggregation with tumbling windows
3. Create a Lambda architecture for page views
4. Implement CDC-based cache invalidation
5. Build a fraud detection stream processor

**Next Chapter**: Monitoring and observability - how to know what's happening!
