# Chapter 15: Reliability Patterns

> **Goal**: Build resilient systems that gracefully handle failures

---

## 15.1 Why Reliability Patterns Matter

```
┌─────────────────────────────────────────────────────────────┐
│              THE REALITY OF DISTRIBUTED SYSTEMS              │
│                                                              │
│  In distributed systems, failure is not exceptional -       │
│  IT'S THE NORM.                                             │
│                                                              │
│  Things that WILL fail:                                     │
│  ├── Network connections (timeout, packet loss)             │
│  ├── Services (crash, overload, bugs)                       │
│  ├── Databases (disk full, replication lag)                 │
│  ├── Third-party APIs (rate limits, outages)                │
│  └── Infrastructure (VM termination, AZ failures)           │
│                                                              │
│                                                              │
│  Without Reliability Patterns:                              │
│  ─────────────────────────────                              │
│                                                              │
│  Service A ───▶ Service B ───▶ Service C                   │
│                     │                                        │
│                     ✗ (fails)                               │
│                     │                                        │
│                     └───▶ Service A hangs                   │
│                           └───▶ All requests queue          │
│                                 └───▶ Service A crashes     │
│                                       └───▶ CASCADING FAILURE│
│                                                              │
│  One service failure takes down the entire system!          │
│                                                              │
│                                                              │
│  With Reliability Patterns:                                 │
│  ─────────────────────────                                  │
│                                                              │
│  Service A ───▶ Service B ───▶ Service C                   │
│                     │                                        │
│                     ✗ (fails)                               │
│                     │                                        │
│                     └───▶ Circuit breaker OPENS             │
│                           └───▶ Fallback response           │
│                                 └───▶ System continues ✓    │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

---

## 15.2 Timeout Pattern

### The First Line of Defense

```
┌─────────────────────────────────────────────────────────────┐
│                    TIMEOUT PATTERN                           │
│                                                              │
│  Rule #1: ALWAYS SET TIMEOUTS                               │
│                                                              │
│  Without timeout:                                           │
│  ─────────────────                                          │
│  Client ──request──▶ Server (hung)                          │
│         ◀─waiting forever...                                │
│                                                              │
│  Resources exhausted, thread pool full, system dies.        │
│                                                              │
│                                                              │
│  Types of Timeouts:                                         │
│  ──────────────────                                         │
│                                                              │
│  1. CONNECTION TIMEOUT                                      │
│     Time to establish TCP connection                        │
│     Usually: 1-5 seconds                                    │
│                                                              │
│  2. READ/SOCKET TIMEOUT                                     │
│     Time to wait for response after sending request         │
│     Usually: 5-30 seconds (depends on operation)            │
│                                                              │
│  3. TOTAL/REQUEST TIMEOUT                                   │
│     Total time including retries                            │
│     Usually: 30-60 seconds                                  │
│                                                              │
│                                                              │
│  Setting Timeouts:                                          │
│  ─────────────────                                          │
│  • Too short: Legitimate requests fail                      │
│  • Too long: Resources held too long                        │
│  • Rule: timeout < caller's timeout (propagate deadlines)   │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

```python
# timeout_patterns.py
import asyncio
import httpx
from typing import Optional, Any, TypeVar
from functools import wraps
import time

# ============== BASIC TIMEOUTS ==============

async def fetch_with_timeout(url: str, timeout_seconds: float = 5.0) -> dict:
    """HTTP request with proper timeout configuration"""
    
    # Configure different timeout types
    timeout = httpx.Timeout(
        connect=2.0,      # Connection timeout
        read=10.0,        # Read timeout
        write=5.0,        # Write timeout
        pool=5.0          # Pool timeout (waiting for connection)
    )
    
    async with httpx.AsyncClient(timeout=timeout) as client:
        try:
            response = await client.get(url)
            return response.json()
        except httpx.TimeoutException as e:
            raise TimeoutError(f"Request timed out: {e}")

# ============== DEADLINE PROPAGATION ==============

class DeadlineContext:
    """
    Propagate deadlines across service calls.
    Each service has less time than its caller.
    """
    
    def __init__(self, deadline: float):
        self.deadline = deadline  # Absolute time when request expires
    
    @property
    def remaining(self) -> float:
        """Time remaining until deadline"""
        return max(0, self.deadline - time.time())
    
    @property
    def is_expired(self) -> bool:
        return self.remaining <= 0
    
    def with_buffer(self, buffer_seconds: float = 0.5) -> 'DeadlineContext':
        """Create child deadline with buffer for our processing"""
        return DeadlineContext(self.deadline - buffer_seconds)

async def service_a_handler(request_data: dict, deadline: DeadlineContext):
    """
    Service A - receives request with deadline.
    Propagates tighter deadline to Service B.
    """
    if deadline.is_expired:
        raise TimeoutError("Deadline already expired")
    
    # Give Service B less time (leave buffer for our processing)
    child_deadline = deadline.with_buffer(0.5)
    
    # Make call with remaining time as timeout
    async with httpx.AsyncClient(timeout=child_deadline.remaining) as client:
        response = await client.post(
            "http://service-b/process",
            json=request_data,
            headers={"X-Deadline": str(child_deadline.deadline)}
        )
        return response.json()

# ============== ASYNC TIMEOUT WRAPPER ==============

T = TypeVar('T')

async def with_timeout(
    coro, 
    timeout_seconds: float,
    fallback: Optional[T] = None
) -> T:
    """Execute coroutine with timeout, return fallback on timeout"""
    try:
        return await asyncio.wait_for(coro, timeout=timeout_seconds)
    except asyncio.TimeoutError:
        if fallback is not None:
            return fallback
        raise

# Usage
async def example():
    # With fallback
    result = await with_timeout(
        fetch_with_timeout("http://slow-service/data"),
        timeout_seconds=5.0,
        fallback={"data": "cached_value"}
    )
    return result

# ============== ADAPTIVE TIMEOUTS ==============

class AdaptiveTimeout:
    """
    Dynamically adjust timeout based on response times.
    Increases timeout when service is slow, decreases when fast.
    """
    
    def __init__(
        self,
        initial_timeout: float = 5.0,
        min_timeout: float = 1.0,
        max_timeout: float = 30.0,
        percentile: float = 0.99
    ):
        self.min_timeout = min_timeout
        self.max_timeout = max_timeout
        self.percentile = percentile
        self.response_times: list = []
        self.current_timeout = initial_timeout
    
    def record(self, response_time: float):
        """Record a response time"""
        self.response_times.append(response_time)
        
        # Keep last 100 samples
        if len(self.response_times) > 100:
            self.response_times = self.response_times[-100:]
        
        # Update timeout based on percentile
        if len(self.response_times) >= 10:
            sorted_times = sorted(self.response_times)
            idx = int(len(sorted_times) * self.percentile)
            p99 = sorted_times[min(idx, len(sorted_times) - 1)]
            
            # Set timeout to P99 * 1.5 (with buffer)
            self.current_timeout = max(
                self.min_timeout,
                min(self.max_timeout, p99 * 1.5)
            )
    
    @property
    def timeout(self) -> float:
        return self.current_timeout

# Usage
adaptive_timeout = AdaptiveTimeout()

async def fetch_with_adaptive_timeout(url: str):
    start = time.time()
    
    async with httpx.AsyncClient(timeout=adaptive_timeout.timeout) as client:
        response = await client.get(url)
        
        elapsed = time.time() - start
        adaptive_timeout.record(elapsed)  # Update timeout based on actual response
        
        return response.json()
```

---

## 15.3 Retry Pattern

### Retry with Exponential Backoff

```
┌─────────────────────────────────────────────────────────────┐
│                    RETRY PATTERN                             │
│                                                              │
│  Not all failures are permanent. Retrying can help:         │
│  ├── Network glitches (transient)                           │
│  ├── Service restarts (brief unavailability)                │
│  ├── Rate limiting (back off and retry)                     │
│  └── Database locks (temporary contention)                  │
│                                                              │
│                                                              │
│  EXPONENTIAL BACKOFF                                        │
│  ────────────────────                                       │
│                                                              │
│  Attempt 1: Immediate                                       │
│  Attempt 2: Wait 1 second                                   │
│  Attempt 3: Wait 2 seconds                                  │
│  Attempt 4: Wait 4 seconds                                  │
│  Attempt 5: Wait 8 seconds                                  │
│            ↓                                                 │
│  delay = base_delay * (2 ^ attempt)                         │
│                                                              │
│                                                              │
│  WITH JITTER (Recommended)                                  │
│  ─────────────────────────                                  │
│                                                              │
│  Without jitter: All clients retry at same time!            │
│                  (thundering herd)                          │
│                                                              │
│  With jitter: Randomize delay to spread out retries         │
│                                                              │
│  delay = base_delay * (2 ^ attempt) * random(0.5, 1.5)     │
│                                                              │
│                                                              │
│  WHICH ERRORS TO RETRY?                                     │
│  ──────────────────────                                     │
│  ✅ Retry: 408, 429, 500, 502, 503, 504                    │
│  ❌ Don't retry: 400, 401, 403, 404, 422                   │
│                                                              │
│  Don't retry client errors - they won't succeed!            │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

```python
# retry_patterns.py
import asyncio
import random
import httpx
from typing import Callable, TypeVar, Optional, Set, Type
from functools import wraps
from dataclasses import dataclass
import logging

logger = logging.getLogger(__name__)

T = TypeVar('T')

# ============== BASIC RETRY ==============

@dataclass
class RetryConfig:
    max_attempts: int = 3
    base_delay: float = 1.0
    max_delay: float = 60.0
    exponential_base: float = 2.0
    jitter: bool = True
    retryable_exceptions: tuple = (Exception,)
    retryable_status_codes: Set[int] = None
    
    def __post_init__(self):
        if self.retryable_status_codes is None:
            self.retryable_status_codes = {408, 429, 500, 502, 503, 504}

def calculate_delay(
    attempt: int,
    config: RetryConfig
) -> float:
    """Calculate delay with exponential backoff and jitter"""
    delay = config.base_delay * (config.exponential_base ** attempt)
    delay = min(delay, config.max_delay)
    
    if config.jitter:
        # Add random jitter (±50%)
        delay = delay * random.uniform(0.5, 1.5)
    
    return delay

async def retry_async(
    func: Callable,
    config: RetryConfig = None,
    *args, **kwargs
) -> T:
    """
    Retry an async function with exponential backoff.
    """
    config = config or RetryConfig()
    last_exception = None
    
    for attempt in range(config.max_attempts):
        try:
            return await func(*args, **kwargs)
            
        except config.retryable_exceptions as e:
            last_exception = e
            
            if attempt < config.max_attempts - 1:
                delay = calculate_delay(attempt, config)
                logger.warning(
                    f"Attempt {attempt + 1} failed: {e}. "
                    f"Retrying in {delay:.2f}s..."
                )
                await asyncio.sleep(delay)
            else:
                logger.error(f"All {config.max_attempts} attempts failed")
    
    raise last_exception

# ============== RETRY DECORATOR ==============

def with_retry(
    max_attempts: int = 3,
    base_delay: float = 1.0,
    exceptions: tuple = (Exception,)
):
    """Decorator for adding retry logic to async functions"""
    
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        async def wrapper(*args, **kwargs):
            config = RetryConfig(
                max_attempts=max_attempts,
                base_delay=base_delay,
                retryable_exceptions=exceptions
            )
            return await retry_async(func, config, *args, **kwargs)
        return wrapper
    return decorator

# Usage
@with_retry(max_attempts=3, base_delay=1.0, exceptions=(httpx.RequestError,))
async def fetch_data(url: str):
    async with httpx.AsyncClient(timeout=5.0) as client:
        response = await client.get(url)
        response.raise_for_status()
        return response.json()

# ============== HTTP RETRY WITH STATUS CHECK ==============

class RetryableHTTPClient:
    """HTTP client with built-in retry logic"""
    
    def __init__(self, config: RetryConfig = None):
        self.config = config or RetryConfig()
    
    async def request(
        self,
        method: str,
        url: str,
        **kwargs
    ) -> httpx.Response:
        """Make HTTP request with retry"""
        last_exception = None
        
        for attempt in range(self.config.max_attempts):
            try:
                async with httpx.AsyncClient(timeout=10.0) as client:
                    response = await client.request(method, url, **kwargs)
                    
                    # Check if status code is retryable
                    if response.status_code in self.config.retryable_status_codes:
                        raise RetryableHTTPError(
                            f"Retryable status: {response.status_code}"
                        )
                    
                    return response
                    
            except (httpx.RequestError, RetryableHTTPError) as e:
                last_exception = e
                
                if attempt < self.config.max_attempts - 1:
                    delay = calculate_delay(attempt, self.config)
                    
                    # Check for Retry-After header
                    if isinstance(e, RetryableHTTPError) and hasattr(e, 'response'):
                        retry_after = e.response.headers.get('Retry-After')
                        if retry_after:
                            delay = float(retry_after)
                    
                    logger.warning(f"Retry {attempt + 1}: {e}, waiting {delay:.1f}s")
                    await asyncio.sleep(delay)
        
        raise last_exception
    
    async def get(self, url: str, **kwargs):
        return await self.request("GET", url, **kwargs)
    
    async def post(self, url: str, **kwargs):
        return await self.request("POST", url, **kwargs)

class RetryableHTTPError(Exception):
    pass

# ============== RETRY WITH CIRCUIT BREAKER INTEGRATION ==============

class SmartRetry:
    """
    Retry that stops if circuit breaker is open.
    Prevents wasting retries on known-bad services.
    """
    
    def __init__(self, circuit_breaker: 'CircuitBreaker', config: RetryConfig = None):
        self.circuit_breaker = circuit_breaker
        self.config = config or RetryConfig()
    
    async def execute(self, func: Callable, *args, **kwargs) -> T:
        """Execute with retry, respecting circuit breaker state"""
        
        for attempt in range(self.config.max_attempts):
            # Check circuit breaker first
            if not self.circuit_breaker.allow_request():
                raise CircuitOpenError("Circuit breaker is open, skipping retries")
            
            try:
                result = await func(*args, **kwargs)
                self.circuit_breaker.record_success()
                return result
                
            except Exception as e:
                self.circuit_breaker.record_failure()
                
                # If circuit just opened, stop retrying
                if not self.circuit_breaker.allow_request():
                    raise CircuitOpenError(f"Circuit opened after failure: {e}")
                
                if attempt < self.config.max_attempts - 1:
                    delay = calculate_delay(attempt, self.config)
                    await asyncio.sleep(delay)
                else:
                    raise

class CircuitOpenError(Exception):
    pass
```

---

## 15.4 Circuit Breaker Pattern

### Preventing Cascade Failures

```
┌─────────────────────────────────────────────────────────────┐
│                 CIRCUIT BREAKER                              │
│                                                              │
│  Inspired by electrical circuit breakers:                   │
│  When current is too high, breaker trips to prevent fire.   │
│                                                              │
│  In software: When failures are too high, stop calling      │
│  the failing service to prevent cascade failures.           │
│                                                              │
│                                                              │
│  THREE STATES:                                              │
│  ──────────────                                             │
│                                                              │
│  ┌─────────────────────────────────────────────────────┐   │
│  │                                                      │   │
│  │    ┌──────────┐                    ┌──────────┐     │   │
│  │    │  CLOSED  │───(failures > N)──▶│   OPEN   │     │   │
│  │    │ (normal) │                    │ (failing)│     │   │
│  │    └────┬─────┘                    └────┬─────┘     │   │
│  │         │                               │           │   │
│  │         │                    (timeout expires)      │   │
│  │         │                               │           │   │
│  │         │                               ▼           │   │
│  │         │                        ┌───────────┐      │   │
│  │         │◀────(success)──────────│ HALF-OPEN │      │   │
│  │         │                        │  (testing)│      │   │
│  │         │                        └─────┬─────┘      │   │
│  │         │                              │            │   │
│  │         │◀──────────(failure)──────────┘            │   │
│  │                      (back to OPEN)                 │   │
│  │                                                      │   │
│  └─────────────────────────────────────────────────────┘   │
│                                                              │
│  CLOSED: Normal operation, requests go through             │
│  OPEN: Failing, reject requests immediately (fail fast)    │
│  HALF-OPEN: Testing if service recovered                   │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

```python
# circuit_breaker.py
import asyncio
import time
from enum import Enum
from typing import Callable, Optional, Any
from dataclasses import dataclass
from functools import wraps
import logging

logger = logging.getLogger(__name__)

class CircuitState(Enum):
    CLOSED = "closed"       # Normal operation
    OPEN = "open"           # Failing, reject requests
    HALF_OPEN = "half_open" # Testing recovery

@dataclass
class CircuitBreakerConfig:
    failure_threshold: int = 5      # Failures before opening
    success_threshold: int = 3      # Successes to close from half-open
    timeout: float = 30.0           # Seconds before trying half-open
    half_open_max_calls: int = 3    # Max calls in half-open state

class CircuitBreaker:
    """
    Circuit breaker implementation.
    Prevents cascade failures by failing fast when service is down.
    """
    
    def __init__(self, name: str, config: CircuitBreakerConfig = None):
        self.name = name
        self.config = config or CircuitBreakerConfig()
        
        self.state = CircuitState.CLOSED
        self.failure_count = 0
        self.success_count = 0
        self.last_failure_time: Optional[float] = None
        self.half_open_calls = 0
    
    def allow_request(self) -> bool:
        """Check if request should be allowed"""
        if self.state == CircuitState.CLOSED:
            return True
        
        if self.state == CircuitState.OPEN:
            # Check if timeout has passed
            if self._should_attempt_reset():
                self._transition_to_half_open()
                return True
            return False
        
        if self.state == CircuitState.HALF_OPEN:
            # Allow limited requests in half-open
            if self.half_open_calls < self.config.half_open_max_calls:
                self.half_open_calls += 1
                return True
            return False
        
        return False
    
    def record_success(self):
        """Record successful call"""
        if self.state == CircuitState.HALF_OPEN:
            self.success_count += 1
            
            if self.success_count >= self.config.success_threshold:
                self._transition_to_closed()
        else:
            # Reset failure count on success
            self.failure_count = 0
    
    def record_failure(self):
        """Record failed call"""
        self.failure_count += 1
        self.last_failure_time = time.time()
        
        if self.state == CircuitState.HALF_OPEN:
            # Any failure in half-open goes back to open
            self._transition_to_open()
        elif self.failure_count >= self.config.failure_threshold:
            self._transition_to_open()
    
    def _should_attempt_reset(self) -> bool:
        """Check if we should try half-open state"""
        if self.last_failure_time is None:
            return True
        return time.time() - self.last_failure_time >= self.config.timeout
    
    def _transition_to_open(self):
        """Transition to OPEN state"""
        logger.warning(f"Circuit {self.name}: OPEN (failing)")
        self.state = CircuitState.OPEN
        self.success_count = 0
        self.half_open_calls = 0
    
    def _transition_to_half_open(self):
        """Transition to HALF_OPEN state"""
        logger.info(f"Circuit {self.name}: HALF-OPEN (testing)")
        self.state = CircuitState.HALF_OPEN
        self.success_count = 0
        self.half_open_calls = 0
    
    def _transition_to_closed(self):
        """Transition to CLOSED state"""
        logger.info(f"Circuit {self.name}: CLOSED (recovered)")
        self.state = CircuitState.CLOSED
        self.failure_count = 0
        self.success_count = 0

class CircuitOpenException(Exception):
    """Raised when circuit breaker is open"""
    pass

# ============== DECORATOR VERSION ==============

def circuit_breaker(
    name: str,
    failure_threshold: int = 5,
    timeout: float = 30.0,
    fallback: Callable = None
):
    """
    Decorator that adds circuit breaker to a function.
    """
    config = CircuitBreakerConfig(
        failure_threshold=failure_threshold,
        timeout=timeout
    )
    cb = CircuitBreaker(name, config)
    
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        async def wrapper(*args, **kwargs):
            if not cb.allow_request():
                if fallback:
                    return await fallback(*args, **kwargs)
                raise CircuitOpenException(f"Circuit {name} is open")
            
            try:
                result = await func(*args, **kwargs)
                cb.record_success()
                return result
            except Exception as e:
                cb.record_failure()
                if fallback and cb.state == CircuitState.OPEN:
                    return await fallback(*args, **kwargs)
                raise
        
        # Expose circuit breaker for monitoring
        wrapper.circuit_breaker = cb
        return wrapper
    
    return decorator

# ============== USAGE ==============

# Define fallback function
async def get_user_fallback(user_id: int):
    """Return cached or default user when service is down"""
    return {"id": user_id, "name": "Unknown", "cached": True}

# Apply circuit breaker
@circuit_breaker(
    name="user-service",
    failure_threshold=5,
    timeout=30.0,
    fallback=get_user_fallback
)
async def get_user(user_id: int):
    """Get user from user service"""
    import httpx
    async with httpx.AsyncClient(timeout=5.0) as client:
        response = await client.get(f"http://user-service/users/{user_id}")
        response.raise_for_status()
        return response.json()

# ============== CIRCUIT BREAKER REGISTRY ==============

class CircuitBreakerRegistry:
    """
    Central registry for all circuit breakers.
    Useful for monitoring and management.
    """
    
    def __init__(self):
        self.breakers: dict[str, CircuitBreaker] = {}
    
    def get_or_create(
        self, 
        name: str, 
        config: CircuitBreakerConfig = None
    ) -> CircuitBreaker:
        """Get existing or create new circuit breaker"""
        if name not in self.breakers:
            self.breakers[name] = CircuitBreaker(name, config)
        return self.breakers[name]
    
    def get_status(self) -> dict:
        """Get status of all circuit breakers"""
        return {
            name: {
                "state": cb.state.value,
                "failures": cb.failure_count,
                "successes": cb.success_count
            }
            for name, cb in self.breakers.items()
        }

# Global registry
registry = CircuitBreakerRegistry()

# FastAPI endpoint for monitoring
from fastapi import FastAPI
app = FastAPI()

@app.get("/health/circuits")
async def circuit_status():
    """Return status of all circuit breakers"""
    return registry.get_status()
```

---

## 15.5 Bulkhead Pattern

### Isolating Failures

```
┌─────────────────────────────────────────────────────────────┐
│                  BULKHEAD PATTERN                            │
│                                                              │
│  Inspired by ship bulkheads:                                │
│  Ships have watertight compartments. If one floods,         │
│  the ship doesn't sink - damage is isolated.                │
│                                                              │
│                                                              │
│  WITHOUT BULKHEAD:                                          │
│  ─────────────────                                          │
│                                                              │
│  ┌─────────────────────────────────────────────────────┐   │
│  │               Shared Thread Pool (100)               │   │
│  │                                                      │   │
│  │   Service A calls: ████████████████████             │   │
│  │   Service B calls: ████████████████████████████████ │◀──Slow!
│  │   Service C calls: ██████                           │   │
│  │                                                      │   │
│  └─────────────────────────────────────────────────────┘   │
│                                                              │
│  Slow Service B uses all threads!                           │
│  Service A and C can't get threads - they fail too!        │
│                                                              │
│                                                              │
│  WITH BULKHEAD:                                             │
│  ──────────────                                             │
│                                                              │
│  ┌──────────────────┐ ┌──────────────────┐ ┌────────────┐ │
│  │  Pool A (30)     │ │  Pool B (50)     │ │ Pool C (20)│ │
│  │  ████████        │ │  ████████████████│ │ ████       │ │
│  │  (OK)            │ │  (exhausted but  │ │ (OK)       │ │
│  │                  │ │   isolated!)     │ │            │ │
│  └──────────────────┘ └──────────────────┘ └────────────┘ │
│                                                              │
│  Service B failure is isolated!                             │
│  Service A and C continue working.                          │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

```python
# bulkhead.py
import asyncio
from typing import Callable, TypeVar, Optional
from dataclasses import dataclass
from functools import wraps
import time

T = TypeVar('T')

@dataclass
class BulkheadConfig:
    max_concurrent: int = 10      # Max concurrent executions
    max_queue: int = 100          # Max waiting in queue
    timeout: float = 30.0         # Max time to wait for slot

class BulkheadFullException(Exception):
    """Raised when bulkhead is at capacity"""
    pass

class Bulkhead:
    """
    Bulkhead implementation using semaphores.
    Limits concurrent calls to a resource.
    """
    
    def __init__(self, name: str, config: BulkheadConfig = None):
        self.name = name
        self.config = config or BulkheadConfig()
        
        # Semaphore for concurrent execution limit
        self._semaphore = asyncio.Semaphore(self.config.max_concurrent)
        
        # Track queue size
        self._queue_size = 0
        self._active_count = 0
    
    @property
    def available(self) -> int:
        """Number of available slots"""
        return self.config.max_concurrent - self._active_count
    
    @property
    def queued(self) -> int:
        """Number of requests waiting"""
        return self._queue_size
    
    async def execute(self, func: Callable, *args, **kwargs) -> T:
        """Execute function within bulkhead limits"""
        
        # Check if queue is full
        if self._queue_size >= self.config.max_queue:
            raise BulkheadFullException(
                f"Bulkhead {self.name} queue is full "
                f"({self._queue_size}/{self.config.max_queue})"
            )
        
        self._queue_size += 1
        
        try:
            # Wait for available slot with timeout
            acquired = await asyncio.wait_for(
                self._semaphore.acquire(),
                timeout=self.config.timeout
            )
            
            if not acquired:
                raise BulkheadFullException(f"Bulkhead {self.name} is full")
            
            self._queue_size -= 1
            self._active_count += 1
            
            try:
                return await func(*args, **kwargs)
            finally:
                self._active_count -= 1
                self._semaphore.release()
                
        except asyncio.TimeoutError:
            self._queue_size -= 1
            raise BulkheadFullException(
                f"Timeout waiting for bulkhead {self.name}"
            )

# ============== DECORATOR VERSION ==============

def bulkhead(
    name: str,
    max_concurrent: int = 10,
    max_queue: int = 100,
    fallback: Callable = None
):
    """Decorator that adds bulkhead to a function"""
    config = BulkheadConfig(max_concurrent=max_concurrent, max_queue=max_queue)
    bh = Bulkhead(name, config)
    
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        async def wrapper(*args, **kwargs):
            try:
                return await bh.execute(func, *args, **kwargs)
            except BulkheadFullException:
                if fallback:
                    return await fallback(*args, **kwargs)
                raise
        
        wrapper.bulkhead = bh
        return wrapper
    
    return decorator

# ============== SERVICE WITH ISOLATED BULKHEADS ==============

class ResilientServiceClient:
    """
    Client with separate bulkheads for different operations.
    """
    
    def __init__(self):
        # Separate bulkheads for different operation types
        self.read_bulkhead = Bulkhead("reads", BulkheadConfig(max_concurrent=50))
        self.write_bulkhead = Bulkhead("writes", BulkheadConfig(max_concurrent=20))
        self.search_bulkhead = Bulkhead("search", BulkheadConfig(max_concurrent=10))
    
    async def get(self, resource_id: str):
        """Read operation - high concurrency allowed"""
        return await self.read_bulkhead.execute(
            self._do_get, resource_id
        )
    
    async def create(self, data: dict):
        """Write operation - limited concurrency"""
        return await self.write_bulkhead.execute(
            self._do_create, data
        )
    
    async def search(self, query: str):
        """Search operation - very limited (expensive)"""
        return await self.search_bulkhead.execute(
            self._do_search, query
        )
    
    async def _do_get(self, resource_id: str):
        import httpx
        async with httpx.AsyncClient() as client:
            response = await client.get(f"http://api/resources/{resource_id}")
            return response.json()
    
    async def _do_create(self, data: dict):
        import httpx
        async with httpx.AsyncClient() as client:
            response = await client.post("http://api/resources", json=data)
            return response.json()
    
    async def _do_search(self, query: str):
        import httpx
        async with httpx.AsyncClient() as client:
            response = await client.get(f"http://api/search?q={query}")
            return response.json()
    
    def get_status(self) -> dict:
        """Get bulkhead status"""
        return {
            "reads": {
                "active": self.read_bulkhead._active_count,
                "queued": self.read_bulkhead.queued,
                "available": self.read_bulkhead.available
            },
            "writes": {
                "active": self.write_bulkhead._active_count,
                "queued": self.write_bulkhead.queued,
                "available": self.write_bulkhead.available
            },
            "search": {
                "active": self.search_bulkhead._active_count,
                "queued": self.search_bulkhead.queued,
                "available": self.search_bulkhead.available
            }
        }
```

---

## 15.6 Idempotency Pattern

```
┌─────────────────────────────────────────────────────────────┐
│                  IDEMPOTENCY                                 │
│                                                              │
│  An operation is IDEMPOTENT if executing it multiple        │
│  times has the same effect as executing it once.            │
│                                                              │
│  Why it matters in distributed systems:                     │
│  ─────────────────────────────────────                      │
│                                                              │
│  Client ──request──▶ Server                                 │
│         ◀──response── (lost!)                               │
│                                                              │
│  Client: "Did it work? I'll retry..."                       │
│  Client ──request──▶ Server (again!)                        │
│                                                              │
│  Without idempotency: Duplicate action!                     │
│  With idempotency: Same result, no duplicate.               │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

```python
# idempotency.py
from fastapi import FastAPI, Header, HTTPException
from pydantic import BaseModel
import redis.asyncio as redis
import json
import hashlib
from typing import Optional, Any
from datetime import timedelta

app = FastAPI()
redis_client = redis.from_url("redis://localhost:6379")

class PaymentRequest(BaseModel):
    amount: float
    currency: str
    recipient: str

class IdempotencyStore:
    """Store for idempotency keys and their responses"""
    
    def __init__(self, redis_client: redis.Redis, ttl_hours: int = 24):
        self.redis = redis_client
        self.ttl = timedelta(hours=ttl_hours)
    
    async def get(self, key: str) -> Optional[dict]:
        """Get stored response for idempotency key"""
        data = await self.redis.get(f"idempotency:{key}")
        if data:
            return json.loads(data)
        return None
    
    async def set(self, key: str, response: dict, status: str = "completed"):
        """Store response for idempotency key"""
        await self.redis.setex(
            f"idempotency:{key}",
            int(self.ttl.total_seconds()),
            json.dumps({"status": status, "response": response})
        )
    
    async def set_in_progress(self, key: str) -> bool:
        """
        Mark key as in progress. Returns False if already exists.
        Prevents concurrent duplicate requests.
        """
        result = await self.redis.set(
            f"idempotency:{key}",
            json.dumps({"status": "in_progress"}),
            nx=True,  # Only if not exists
            ex=60  # Short TTL for in-progress
        )
        return result is not None

idempotency_store = IdempotencyStore(redis_client)

@app.post("/payments")
async def create_payment(
    payment: PaymentRequest,
    idempotency_key: str = Header(..., alias="Idempotency-Key")
):
    """
    Create payment with idempotency.
    Same idempotency key always returns same response.
    """
    
    # Check if we've seen this key before
    existing = await idempotency_store.get(idempotency_key)
    
    if existing:
        if existing["status"] == "in_progress":
            raise HTTPException(
                status_code=409,
                detail="Request with this idempotency key is already in progress"
            )
        # Return cached response
        return existing["response"]
    
    # Mark as in progress
    if not await idempotency_store.set_in_progress(idempotency_key):
        raise HTTPException(
            status_code=409,
            detail="Concurrent request with same idempotency key"
        )
    
    try:
        # Process payment
        result = await process_payment(payment)
        
        # Store response
        await idempotency_store.set(idempotency_key, result)
        
        return result
        
    except Exception as e:
        # Remove in-progress marker on failure
        await redis_client.delete(f"idempotency:{idempotency_key}")
        raise

async def process_payment(payment: PaymentRequest) -> dict:
    """Actual payment processing logic"""
    # In real implementation: call payment gateway
    import uuid
    return {
        "payment_id": str(uuid.uuid4()),
        "amount": payment.amount,
        "currency": payment.currency,
        "status": "completed"
    }
```

---

## 15.7 Combining Patterns

```python
# resilient_client.py
"""
Combining all reliability patterns into a resilient HTTP client.
"""
import httpx
import asyncio
from dataclasses import dataclass
from typing import Optional, Callable, Any

@dataclass
class ResilienceConfig:
    # Timeout
    timeout: float = 10.0
    
    # Retry
    max_retries: int = 3
    retry_base_delay: float = 1.0
    
    # Circuit Breaker
    failure_threshold: int = 5
    circuit_timeout: float = 30.0
    
    # Bulkhead
    max_concurrent: int = 20

class ResilientClient:
    """
    HTTP client with all reliability patterns built in.
    """
    
    def __init__(
        self,
        name: str,
        base_url: str,
        config: ResilienceConfig = None,
        fallback: Callable = None
    ):
        self.name = name
        self.base_url = base_url
        self.config = config or ResilienceConfig()
        self.fallback = fallback
        
        # Initialize patterns
        self.circuit = CircuitBreaker(name, CircuitBreakerConfig(
            failure_threshold=self.config.failure_threshold,
            timeout=self.config.circuit_timeout
        ))
        self.bulkhead = Bulkhead(name, BulkheadConfig(
            max_concurrent=self.config.max_concurrent
        ))
    
    async def request(
        self,
        method: str,
        path: str,
        **kwargs
    ) -> Any:
        """Make resilient HTTP request"""
        
        # Check circuit breaker
        if not self.circuit.allow_request():
            if self.fallback:
                return await self.fallback(method, path, **kwargs)
            raise CircuitOpenException(f"Circuit {self.name} is open")
        
        # Execute within bulkhead
        try:
            return await self.bulkhead.execute(
                self._request_with_retry,
                method, path, **kwargs
            )
        except BulkheadFullException:
            if self.fallback:
                return await self.fallback(method, path, **kwargs)
            raise
    
    async def _request_with_retry(
        self,
        method: str,
        path: str,
        **kwargs
    ) -> Any:
        """Request with retry logic"""
        last_error = None
        
        for attempt in range(self.config.max_retries):
            try:
                result = await self._do_request(method, path, **kwargs)
                self.circuit.record_success()
                return result
                
            except Exception as e:
                last_error = e
                self.circuit.record_failure()
                
                if attempt < self.config.max_retries - 1:
                    delay = self.config.retry_base_delay * (2 ** attempt)
                    await asyncio.sleep(delay)
        
        raise last_error
    
    async def _do_request(
        self,
        method: str,
        path: str,
        **kwargs
    ) -> Any:
        """Actual HTTP request with timeout"""
        async with httpx.AsyncClient(
            base_url=self.base_url,
            timeout=self.config.timeout
        ) as client:
            response = await client.request(method, path, **kwargs)
            response.raise_for_status()
            return response.json()
    
    async def get(self, path: str, **kwargs):
        return await self.request("GET", path, **kwargs)
    
    async def post(self, path: str, **kwargs):
        return await self.request("POST", path, **kwargs)

# Usage
async def user_fallback(method, path, **kwargs):
    return {"id": 0, "name": "Unknown", "cached": True}

user_client = ResilientClient(
    name="user-service",
    base_url="http://user-service:8000",
    fallback=user_fallback
)

# All patterns applied automatically!
user = await user_client.get("/users/123")
```

---

## Summary

| Pattern | Purpose | When to Use |
|---------|---------|-------------|
| **Timeout** | Prevent indefinite waits | Always |
| **Retry** | Handle transient failures | Idempotent operations |
| **Circuit Breaker** | Fail fast, prevent cascade | External dependencies |
| **Bulkhead** | Isolate failures | Different operation types |
| **Idempotency** | Safe retries | State-changing operations |

---

## Practice Exercises

1. Implement a circuit breaker with monitoring metrics
2. Build a retry mechanism with exponential backoff and jitter
3. Create a bulkhead that limits concurrent requests per user
4. Implement idempotency for a payment API
5. Combine all patterns into a resilient service client

**Next Chapter**: Scalability Patterns!
