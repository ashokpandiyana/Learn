# Chapter 8: Consistency Models

> **Goal**: Understand the spectrum of consistency guarantees and their trade-offs

---

## 8.1 What is Consistency?

```
┌─────────────────────────────────────────────────────────────┐
│              WHAT IS CONSISTENCY?                            │
│                                                              │
│  In distributed systems, "consistency" means:               │
│  "What guarantees do readers have about seeing writes?"     │
│                                                              │
│                                                              │
│  Simple Example:                                            │
│  ────────────────                                           │
│  Time 0: Balance = $100                                     │
│  Time 1: User A withdraws $50 (writes to Node 1)           │
│  Time 2: User B checks balance (reads from Node 2)         │
│                                                              │
│  What does User B see?                                      │
│                                                              │
│  Strong Consistency:  $50 (always sees latest)             │
│  Eventual Consistency: $100 or $50 (depends on replication) │
│                                                              │
│                                                              │
│  The Fundamental Trade-off:                                 │
│  ─────────────────────────                                  │
│                                                              │
│  Stronger Consistency = More Coordination = Higher Latency  │
│  Weaker Consistency = Less Coordination = Lower Latency     │
│                                                              │
│  There's no free lunch!                                     │
└─────────────────────────────────────────────────────────────┘
```

---

## 8.2 The CAP Theorem

### The Theorem Explained

```
┌─────────────────────────────────────────────────────────────┐
│                   THE CAP THEOREM                            │
│                                                              │
│  "A distributed system can only provide TWO of THREE        │
│   guarantees simultaneously"                                │
│                                                              │
│                      Consistency                            │
│                          /\                                  │
│                         /  \                                 │
│                        /    \                                │
│                       / PICK \                               │
│                      /  TWO   \                              │
│                     /          \                             │
│                    /____________\                            │
│          Availability          Partition                    │
│                                Tolerance                    │
│                                                              │
│                                                              │
│  C - CONSISTENCY                                            │
│  ─────────────────                                          │
│  All nodes see the same data at the same time.             │
│  Every read returns the most recent write.                  │
│                                                              │
│  A - AVAILABILITY                                           │
│  ─────────────────                                          │
│  Every request receives a response (success or failure).   │
│  No request times out or hangs forever.                    │
│                                                              │
│  P - PARTITION TOLERANCE                                    │
│  ───────────────────────                                    │
│  System continues operating despite network partitions.     │
│  Messages between nodes can be delayed or lost.            │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

### The Reality of CAP

```
┌─────────────────────────────────────────────────────────────┐
│              CAP: THE REAL CHOICE                            │
│                                                              │
│  In practice, you MUST have Partition Tolerance.            │
│  Networks WILL fail. This is not optional.                  │
│                                                              │
│  So the real choice is:                                     │
│                                                              │
│  ┌─────────────────────┐  ┌─────────────────────┐          │
│  │   CP (Consistent +  │  │  AP (Available +    │          │
│  │   Partition Tolerant│  │  Partition Tolerant)│          │
│  ├─────────────────────┤  ├─────────────────────┤          │
│  │ During partition:   │  │ During partition:   │          │
│  │ Reject some requests│  │ Serve stale data    │          │
│  │ to maintain         │  │ to remain           │          │
│  │ consistency         │  │ available           │          │
│  ├─────────────────────┤  ├─────────────────────┤          │
│  │ Examples:           │  │ Examples:           │          │
│  │ - MongoDB           │  │ - Cassandra         │          │
│  │ - Redis Cluster     │  │ - DynamoDB          │          │
│  │ - HBase             │  │ - CouchDB           │          │
│  │ - Zookeeper         │  │ - Riak              │          │
│  └─────────────────────┘  └─────────────────────┘          │
│                                                              │
│                                                              │
│  IMPORTANT NUANCE:                                          │
│  ─────────────────                                          │
│  CAP is about behavior DURING a partition.                  │
│  When network is healthy, you can have all three!           │
│                                                              │
│  Most of the time: Everything works fine                    │
│  During partition: You must choose C or A                   │
└─────────────────────────────────────────────────────────────┘
```

```python
# cap_demonstration.py
import asyncio
from enum import Enum
from typing import Any, Optional
from dataclasses import dataclass
from datetime import datetime

class PartitionState(Enum):
    HEALTHY = "healthy"
    PARTITIONED = "partitioned"

@dataclass
class Node:
    node_id: str
    data: dict
    
class CPSystem:
    """
    CP System: Prioritizes Consistency over Availability.
    During partition, rejects writes/reads that can't be confirmed.
    """
    
    def __init__(self, nodes: list[Node]):
        self.nodes = nodes
        self.partition_state = PartitionState.HEALTHY
        self.quorum_size = len(nodes) // 2 + 1
    
    async def write(self, key: str, value: Any) -> bool:
        """Write requires quorum - may fail during partition"""
        if self.partition_state == PartitionState.PARTITIONED:
            reachable = self._get_reachable_nodes()
            
            if len(reachable) < self.quorum_size:
                # CP Choice: Reject write to maintain consistency
                raise Exception(
                    f"Cannot write: only {len(reachable)} nodes reachable, "
                    f"need {self.quorum_size} for quorum"
                )
        
        # Write to all reachable nodes
        for node in self._get_reachable_nodes():
            node.data[key] = value
        
        return True
    
    async def read(self, key: str) -> Optional[Any]:
        """Read requires quorum - may fail during partition"""
        if self.partition_state == PartitionState.PARTITIONED:
            reachable = self._get_reachable_nodes()
            
            if len(reachable) < self.quorum_size:
                # CP Choice: Reject read to ensure consistency
                raise Exception(
                    f"Cannot read: only {len(reachable)} nodes reachable"
                )
        
        # Read from first reachable node (quorum ensures it's current)
        node = self._get_reachable_nodes()[0]
        return node.data.get(key)
    
    def _get_reachable_nodes(self) -> list[Node]:
        if self.partition_state == PartitionState.HEALTHY:
            return self.nodes
        # Simulate partition - only first node reachable
        return [self.nodes[0]]

class APSystem:
    """
    AP System: Prioritizes Availability over Consistency.
    During partition, serves potentially stale data.
    """
    
    def __init__(self, nodes: list[Node]):
        self.nodes = nodes
        self.partition_state = PartitionState.HEALTHY
    
    async def write(self, key: str, value: Any) -> bool:
        """Write always succeeds (to local node at least)"""
        reachable = self._get_reachable_nodes()
        
        if not reachable:
            raise Exception("No nodes reachable at all")
        
        # AP Choice: Write to whatever nodes are reachable
        for node in reachable:
            node.data[key] = {
                'value': value,
                'timestamp': datetime.utcnow(),
                'node': node.node_id
            }
        
        if len(reachable) < len(self.nodes):
            print(f"Warning: Only wrote to {len(reachable)}/{len(self.nodes)} nodes")
        
        return True
    
    async def read(self, key: str) -> Optional[Any]:
        """Read always succeeds (may be stale)"""
        reachable = self._get_reachable_nodes()
        
        if not reachable:
            raise Exception("No nodes reachable")
        
        # AP Choice: Return data from any reachable node
        node = reachable[0]
        data = node.data.get(key)
        
        if data and self.partition_state == PartitionState.PARTITIONED:
            print(f"Warning: Returning potentially stale data from {node.node_id}")
        
        return data['value'] if data else None
    
    def _get_reachable_nodes(self) -> list[Node]:
        if self.partition_state == PartitionState.HEALTHY:
            return self.nodes
        return [self.nodes[0]]

# ============== DEMONSTRATION ==============

async def demo_cap():
    # Create nodes
    nodes = [Node(f"node-{i}", {}) for i in range(3)]
    
    print("=== CP System Demo ===")
    cp = CPSystem(nodes.copy())
    
    # Normal operation
    await cp.write("user:1", "Alice")
    print(f"Read: {await cp.read('user:1')}")
    
    # Simulate partition
    print("\n--- Network Partition ---")
    cp.partition_state = PartitionState.PARTITIONED
    
    try:
        await cp.write("user:2", "Bob")
    except Exception as e:
        print(f"CP Write failed: {e}")
    
    try:
        await cp.read("user:1")
    except Exception as e:
        print(f"CP Read failed: {e}")
    
    print("\n=== AP System Demo ===")
    nodes2 = [Node(f"node-{i}", {}) for i in range(3)]
    ap = APSystem(nodes2)
    
    # Normal operation
    await ap.write("user:1", "Alice")
    print(f"Read: {await ap.read('user:1')}")
    
    # Simulate partition
    print("\n--- Network Partition ---")
    ap.partition_state = PartitionState.PARTITIONED
    
    # AP system still works!
    await ap.write("user:2", "Bob")
    print(f"Read (may be stale): {await ap.read('user:1')}")

# asyncio.run(demo_cap())
```

---

## 8.3 Consistency Levels Spectrum

```
┌─────────────────────────────────────────────────────────────┐
│              CONSISTENCY SPECTRUM                            │
│                                                              │
│  Strong                                               Weak   │
│    │                                                   │     │
│    ▼                                                   ▼     │
│  ┌──────────────────────────────────────────────────────┐  │
│  │ Linear- │ Sequen- │ Causal │ Read-  │ Eventual │     │  │
│  │ izable  │ tial    │        │ Your-  │          │     │  │
│  │         │         │        │ Writes │          │     │  │
│  └──────────────────────────────────────────────────────┘  │
│                                                              │
│  ←─────────────────────────────────────────────────────→   │
│  More Coordination                     Less Coordination    │
│  Higher Latency                        Lower Latency        │
│  Lower Availability                    Higher Availability  │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

### 8.3.1 Linearizability (Strongest)

```
┌─────────────────────────────────────────────────────────────┐
│                  LINEARIZABILITY                             │
│                                                              │
│  "Operations appear to happen instantaneously at some       │
│   point between their start and end."                       │
│                                                              │
│  Also called: "Atomic consistency", "Strong consistency"    │
│                                                              │
│  Timeline:                                                  │
│  ─────────────────────────────────────────────────────────  │
│                                                              │
│  Client A: ├────Write(x=1)────┤                             │
│  Client B:         ├────Read(x)────┤ → Must return 1!       │
│  Client C:                   ├────Read(x)────┤ → Must be 1! │
│                                                              │
│  If Read B starts after Write A starts, AND                 │
│  Read B ends after Write A ends, then Read B sees the write.│
│                                                              │
│                                                              │
│  Guarantees:                                                │
│  ├── Single copy illusion (appears as one node)             │
│  ├── Real-time ordering (respects wall-clock time)          │
│  └── Once a read sees a write, all later reads see it       │
│                                                              │
│  Use Cases:                                                 │
│  ├── Leader election                                        │
│  ├── Distributed locks                                      │
│  ├── Unique constraints                                     │
│  └── Financial transactions                                 │
│                                                              │
│  Cost: Requires synchronization, high latency               │
└─────────────────────────────────────────────────────────────┘
```

```python
# linearizable_register.py
import asyncio
from typing import Any, Optional
import time

class LinearizableRegister:
    """
    A linearizable register using a single leader.
    All operations go through the leader for total ordering.
    """
    
    def __init__(self):
        self.value: Any = None
        self.version: int = 0
        self.lock = asyncio.Lock()
    
    async def write(self, value: Any) -> int:
        """
        Linearizable write - acquires lock, writes, releases.
        """
        async with self.lock:
            # Simulate network/disk latency
            await asyncio.sleep(0.01)
            
            self.version += 1
            self.value = value
            
            return self.version
    
    async def read(self) -> tuple[Any, int]:
        """
        Linearizable read - acquires lock to ensure we see latest.
        """
        async with self.lock:
            await asyncio.sleep(0.01)
            return (self.value, self.version)
    
    async def compare_and_swap(
        self, 
        expected: Any, 
        new_value: Any
    ) -> bool:
        """
        Atomic compare-and-swap - fundamental linearizable operation.
        """
        async with self.lock:
            if self.value == expected:
                self.version += 1
                self.value = new_value
                return True
            return False

# Distributed lock using linearizable register
class DistributedLock:
    """
    Distributed lock using linearizable compare-and-swap.
    """
    
    def __init__(self, register: LinearizableRegister, owner_id: str):
        self.register = register
        self.owner_id = owner_id
    
    async def acquire(self, timeout: float = 10.0) -> bool:
        """Try to acquire the lock"""
        start = time.time()
        
        while time.time() - start < timeout:
            # Try to set lock owner (if currently None)
            success = await self.register.compare_and_swap(
                expected=None,
                new_value=self.owner_id
            )
            
            if success:
                return True
            
            # Lock is held, wait and retry
            await asyncio.sleep(0.1)
        
        return False
    
    async def release(self) -> bool:
        """Release the lock"""
        return await self.register.compare_and_swap(
            expected=self.owner_id,
            new_value=None
        )

async def demo_linearizable_lock():
    register = LinearizableRegister()
    
    async def worker(worker_id: str):
        lock = DistributedLock(register, worker_id)
        
        print(f"{worker_id}: Trying to acquire lock...")
        acquired = await lock.acquire(timeout=5.0)
        
        if acquired:
            print(f"{worker_id}: Got the lock! Doing work...")
            await asyncio.sleep(1)  # Critical section
            await lock.release()
            print(f"{worker_id}: Released lock")
        else:
            print(f"{worker_id}: Couldn't get lock")
    
    # Multiple workers compete for lock
    await asyncio.gather(
        worker("Worker-A"),
        worker("Worker-B"),
        worker("Worker-C"),
    )

# asyncio.run(demo_linearizable_lock())
```

### 8.3.2 Sequential Consistency

```
┌─────────────────────────────────────────────────────────────┐
│              SEQUENTIAL CONSISTENCY                          │
│                                                              │
│  "All operations appear in SOME sequential order that       │
│   respects the program order of each client."               │
│                                                              │
│  Weaker than linearizable: No real-time guarantees!         │
│                                                              │
│  Example:                                                   │
│  ─────────                                                  │
│  Client A: Write(x=1) ... Write(x=2)                        │
│  Client B: Read(x)    ... Read(x)                           │
│                                                              │
│  Valid sequential orderings:                                │
│  ├── W(1), W(2), R→2, R→2  ✓                               │
│  ├── W(1), R→1, W(2), R→2  ✓                               │
│  ├── R→null, W(1), W(2), R→2  ✓                            │
│                                                              │
│  Invalid:                                                   │
│  ├── W(2), W(1), R→?, R→?  ✗ (violates A's program order)  │
│  ├── W(1), R→1, R→null, W(2)  ✗ (B reads go backwards)     │
│                                                              │
│                                                              │
│  Key Difference from Linearizable:                          │
│  ────────────────────────────────                           │
│  Linearizable: Operations respect real-time order           │
│  Sequential:   Operations respect program order only        │
│                                                              │
│  Sequential allows more reordering = better performance     │
└─────────────────────────────────────────────────────────────┘
```

### 8.3.3 Causal Consistency

```
┌─────────────────────────────────────────────────────────────┐
│              CAUSAL CONSISTENCY                              │
│                                                              │
│  "If operation A causally precedes B, everyone sees         │
│   A before B. Concurrent operations may be seen in          │
│   different orders."                                        │
│                                                              │
│                                                              │
│  What is "Causally Precedes"?                               │
│  ────────────────────────────                               │
│  A causally precedes B if:                                  │
│  1. Same client: A happens before B                         │
│  2. A is a write, B is a read that returns A's value        │
│  3. Transitivity: A→C and C→B implies A→B                   │
│                                                              │
│                                                              │
│  Example:                                                   │
│  ─────────                                                  │
│  Alice posts: "I got the job!"                              │
│  Bob sees post, replies: "Congratulations!"                 │
│                                                              │
│  Bob's reply CAUSALLY DEPENDS on Alice's post.              │
│  Everyone must see Alice's post before Bob's reply.         │
│                                                              │
│                                                              │
│  But if Carol posts "Nice weather today" (independent):     │
│  Some users might see Carol's post before Alice's.          │
│  That's fine - they're not causally related!                │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

```python
# causal_consistency.py
from dataclasses import dataclass, field
from typing import Dict, Any, Optional
from datetime import datetime

@dataclass
class VectorClock:
    """
    Vector clock for tracking causality.
    Each entry is node_id -> logical timestamp.
    """
    clock: Dict[str, int] = field(default_factory=dict)
    
    def increment(self, node_id: str):
        """Increment this node's clock"""
        self.clock[node_id] = self.clock.get(node_id, 0) + 1
    
    def merge(self, other: 'VectorClock'):
        """Merge with another vector clock (take max of each)"""
        all_nodes = set(self.clock.keys()) | set(other.clock.keys())
        for node in all_nodes:
            self.clock[node] = max(
                self.clock.get(node, 0),
                other.clock.get(node, 0)
            )
    
    def happens_before(self, other: 'VectorClock') -> bool:
        """Check if self causally precedes other"""
        dominated = False
        for node, time in self.clock.items():
            other_time = other.clock.get(node, 0)
            if time > other_time:
                return False  # self has something newer
            if time < other_time:
                dominated = True
        
        # Also check nodes only in other
        for node in other.clock:
            if node not in self.clock and other.clock[node] > 0:
                dominated = True
        
        return dominated
    
    def concurrent(self, other: 'VectorClock') -> bool:
        """Check if two events are concurrent (no causal order)"""
        return not self.happens_before(other) and not other.happens_before(self)
    
    def copy(self) -> 'VectorClock':
        return VectorClock(self.clock.copy())

@dataclass
class CausalMessage:
    """A message with causal metadata"""
    key: str
    value: Any
    vector_clock: VectorClock
    origin_node: str

class CausalStore:
    """
    Key-value store with causal consistency.
    Uses vector clocks to track and enforce causality.
    """
    
    def __init__(self, node_id: str):
        self.node_id = node_id
        self.data: Dict[str, tuple[Any, VectorClock]] = {}
        self.vector_clock = VectorClock()
        self.pending: list[CausalMessage] = []  # Messages waiting for dependencies
    
    def local_write(self, key: str, value: Any) -> VectorClock:
        """Write locally and generate causal metadata"""
        # Increment our clock
        self.vector_clock.increment(self.node_id)
        
        # Store with current vector clock
        self.data[key] = (value, self.vector_clock.copy())
        
        return self.vector_clock.copy()
    
    def local_read(self, key: str) -> Optional[tuple[Any, VectorClock]]:
        """Read from local store"""
        return self.data.get(key)
    
    def receive_message(self, msg: CausalMessage) -> bool:
        """
        Receive a message from another node.
        Only apply if causal dependencies are satisfied.
        """
        # Check if we've seen all causally preceding writes
        if self._dependencies_satisfied(msg.vector_clock):
            self._apply_message(msg)
            self._check_pending()
            return True
        else:
            # Buffer until dependencies arrive
            self.pending.append(msg)
            print(f"Buffering message for {msg.key} - waiting for dependencies")
            return False
    
    def _dependencies_satisfied(self, msg_clock: VectorClock) -> bool:
        """Check if we've seen all writes this message depends on"""
        for node, time in msg_clock.clock.items():
            if node == self.node_id:
                continue  # Skip self
            our_time = self.vector_clock.clock.get(node, 0)
            if our_time < time - 1:  # We're missing intermediate writes
                return False
        return True
    
    def _apply_message(self, msg: CausalMessage):
        """Apply a message to local store"""
        current = self.data.get(msg.key)
        
        # Only apply if newer (using vector clock comparison)
        if current is None or current[1].happens_before(msg.vector_clock):
            self.data[msg.key] = (msg.value, msg.vector_clock)
            self.vector_clock.merge(msg.vector_clock)
            print(f"Applied: {msg.key} = {msg.value}")
    
    def _check_pending(self):
        """Check if any pending messages can now be applied"""
        still_pending = []
        for msg in self.pending:
            if self._dependencies_satisfied(msg.vector_clock):
                self._apply_message(msg)
            else:
                still_pending.append(msg)
        self.pending = still_pending

# ============== DEMO ==============

def demo_causal_consistency():
    # Create two nodes
    node_a = CausalStore("A")
    node_b = CausalStore("B")
    
    # Node A writes
    print("=== Node A writes x=1 ===")
    clock1 = node_a.local_write("x", 1)
    msg1 = CausalMessage("x", 1, clock1, "A")
    
    # Node A writes again (causally after first write)
    print("\n=== Node A writes y=2 (depends on x=1) ===")
    clock2 = node_a.local_write("y", 2)
    msg2 = CausalMessage("y", 2, clock2, "A")
    
    # Node B receives y=2 BEFORE x=1 (out of order)
    print("\n=== Node B receives y=2 first (out of order) ===")
    node_b.receive_message(msg2)  # Will be buffered!
    
    # Node B receives x=1
    print("\n=== Node B receives x=1 ===")
    node_b.receive_message(msg1)  # Now y=2 can be applied too!
    
    # Check final state
    print(f"\nNode B final state:")
    print(f"  x = {node_b.local_read('x')}")
    print(f"  y = {node_b.local_read('y')}")

# demo_causal_consistency()
```

### 8.3.4 Read-Your-Writes Consistency

```
┌─────────────────────────────────────────────────────────────┐
│           READ-YOUR-WRITES CONSISTENCY                       │
│                                                              │
│  "A client always sees their own writes."                   │
│                                                              │
│  Without it:                                                │
│  ───────────                                                │
│  1. User updates profile picture                            │
│  2. User refreshes page                                     │
│  3. User sees OLD picture! (read went to stale replica)     │
│                                                              │
│  User thinks: "Did my update fail??"                        │
│                                                              │
│                                                              │
│  Implementation Strategies:                                 │
│  ─────────────────────────                                  │
│                                                              │
│  1. STICKY SESSIONS                                         │
│     Route user's requests to same replica always            │
│                                                              │
│  2. VERSION TRACKING                                        │
│     Client tracks last write version                        │
│     Read waits until replica has that version               │
│                                                              │
│  3. READ FROM LEADER                                        │
│     After write, read from leader (or quorum)               │
│     Eventually switch back to replicas                      │
└─────────────────────────────────────────────────────────────┘
```

```python
# read_your_writes.py
from dataclasses import dataclass
from typing import Any, Optional, Dict
import asyncio
import time

@dataclass 
class VersionedValue:
    value: Any
    version: int
    timestamp: float

class ReadYourWritesStore:
    """
    Store that guarantees read-your-writes consistency.
    """
    
    def __init__(self):
        self.data: Dict[str, VersionedValue] = {}
        self.version = 0
        # Track last write version per client
        self.client_versions: Dict[str, int] = {}
    
    async def write(self, client_id: str, key: str, value: Any) -> int:
        """Write and track client's last write version"""
        self.version += 1
        
        self.data[key] = VersionedValue(
            value=value,
            version=self.version,
            timestamp=time.time()
        )
        
        # Remember this client's last write
        self.client_versions[client_id] = self.version
        
        return self.version
    
    async def read(
        self, 
        client_id: str, 
        key: str,
        min_version: Optional[int] = None
    ) -> Optional[Any]:
        """
        Read with read-your-writes guarantee.
        Waits if necessary to ensure client sees their own writes.
        """
        # Determine minimum version to wait for
        required_version = min_version or self.client_versions.get(client_id, 0)
        
        # Wait for data to reach required version
        max_wait = 5.0  # seconds
        start = time.time()
        
        while time.time() - start < max_wait:
            if key in self.data:
                if self.data[key].version >= required_version:
                    return self.data[key].value
            
            # Data not ready yet, wait and retry
            await asyncio.sleep(0.01)
        
        # Timeout - return what we have (or None)
        if key in self.data:
            return self.data[key].value
        return None

# Simulating replicas with different replication lag
class ReplicatedStore:
    def __init__(self):
        self.leader = ReadYourWritesStore()
        self.replicas = [ReadYourWritesStore() for _ in range(2)]
        self.replication_lag = 0.5  # seconds
    
    async def write(self, client_id: str, key: str, value: Any) -> int:
        """Write to leader, async replicate to followers"""
        version = await self.leader.write(client_id, key, value)
        
        # Async replication (with lag)
        asyncio.create_task(self._replicate(key, version))
        
        return version
    
    async def _replicate(self, key: str, version: int):
        """Replicate to followers after delay"""
        await asyncio.sleep(self.replication_lag)
        
        data = self.leader.data.get(key)
        if data:
            for replica in self.replicas:
                replica.data[key] = data
                replica.version = max(replica.version, version)
    
    async def read_with_ryw(
        self, 
        client_id: str, 
        key: str
    ) -> Optional[Any]:
        """
        Read with read-your-writes guarantee.
        Routes to leader if client has recent writes.
        """
        client_version = self.leader.client_versions.get(client_id, 0)
        
        # Check if any replica is caught up
        for replica in self.replicas:
            if replica.version >= client_version:
                # Replica is up to date, can read from it
                data = replica.data.get(key)
                if data and data.version >= client_version:
                    print(f"Reading from replica (version {replica.version})")
                    return data.value
        
        # No replica caught up, read from leader
        print(f"Reading from leader (client needs version {client_version})")
        return await self.leader.read(client_id, key)

async def demo_read_your_writes():
    store = ReplicatedStore()
    
    # Client writes
    print("=== Client A writes ===")
    await store.write("client-A", "profile", {"name": "Alice"})
    
    # Immediate read - should see own write (from leader)
    print("\n=== Client A reads immediately ===")
    result = await store.read_with_ryw("client-A", "profile")
    print(f"Result: {result}")
    
    # Wait for replication
    print("\n=== Waiting for replication... ===")
    await asyncio.sleep(0.6)
    
    # Now reads can go to replica
    print("\n=== Client A reads after replication ===")
    result = await store.read_with_ryw("client-A", "profile")
    print(f"Result: {result}")
    
    # Different client (no RYW guarantee needed)
    print("\n=== Client B reads (different client) ===")
    result = await store.read_with_ryw("client-B", "profile")
    print(f"Result: {result}")

# asyncio.run(demo_read_your_writes())
```

### 8.3.5 Eventual Consistency

```
┌─────────────────────────────────────────────────────────────┐
│              EVENTUAL CONSISTENCY                            │
│                                                              │
│  "If no new updates, all replicas EVENTUALLY converge       │
│   to the same value."                                       │
│                                                              │
│  The Weakest Guarantee:                                     │
│  ├── No guarantee WHEN convergence happens                  │
│  ├── No guarantee what you'll read at any moment            │
│  ├── Only guarantees eventual convergence                   │
│  └── Conflicts must be resolved somehow                     │
│                                                              │
│                                                              │
│  Timeline:                                                  │
│  ─────────────────────────────────────────────────────────  │
│  Time:    T0      T1      T2      T3      T4               │
│                                                              │
│  Write:   x=1                                               │
│  Node 1:  x=1     x=1     x=1     x=1     x=1              │
│  Node 2:  x=?     x=?     x=1     x=1     x=1   ← Converged│
│  Node 3:  x=?     x=?     x=?     x=1     x=1   ← Converged│
│                                                              │
│  "?" means node might return old value or no value          │
│                                                              │
│                                                              │
│  Acceptable for:                                            │
│  ├── Social media likes/views (approximate is OK)           │
│  ├── Product reviews                                        │
│  ├── User preferences                                       │
│  ├── Shopping cart (with merge on checkout)                 │
│  └── DNS propagation                                        │
│                                                              │
│  NOT acceptable for:                                        │
│  ├── Bank balances                                          │
│  ├── Inventory counts                                       │
│  ├── Unique usernames                                       │
│  └── Anything requiring "exactly once"                      │
└─────────────────────────────────────────────────────────────┘
```

---

## 8.4 PACELC Theorem

```
┌─────────────────────────────────────────────────────────────┐
│                  PACELC THEOREM                              │
│                                                              │
│  Extension of CAP that considers normal operation too.      │
│                                                              │
│  ┌─────────────────────────────────────────────────────┐   │
│  │                                                      │   │
│  │   IF there's a PARTITION (P):                       │   │
│  │      Choose AVAILABILITY (A) or CONSISTENCY (C)     │   │
│  │                                                      │   │
│  │   ELSE (normal operation):                          │   │
│  │      Choose LATENCY (L) or CONSISTENCY (C)          │   │
│  │                                                      │   │
│  └─────────────────────────────────────────────────────┘   │
│                                                              │
│                                                              │
│  Database Classifications:                                  │
│  ─────────────────────────                                  │
│                                                              │
│  PA/EL (Partition → Available, Else → Latency)             │
│  ├── Cassandra                                              │
│  ├── DynamoDB                                               │
│  └── Prioritizes availability and speed always              │
│                                                              │
│  PC/EC (Partition → Consistent, Else → Consistent)         │
│  ├── Traditional RDBMS (single node)                        │
│  ├── Spanner                                                │
│  └── Never sacrifices consistency                           │
│                                                              │
│  PA/EC (Partition → Available, Else → Consistent)          │
│  ├── MongoDB (default config)                               │
│  └── Consistent normally, available during partition        │
│                                                              │
│  PC/EL (Partition → Consistent, Else → Latency)            │
│  ├── PNUTS (Yahoo)                                          │
│  └── Fast normally, consistent during partition             │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

---

## 8.5 Choosing the Right Consistency Level

```python
# consistency_decision.py
"""
Decision framework for choosing consistency levels.
"""

from enum import Enum
from dataclasses import dataclass
from typing import List

class ConsistencyLevel(Enum):
    LINEARIZABLE = "linearizable"
    SEQUENTIAL = "sequential"
    CAUSAL = "causal"
    READ_YOUR_WRITES = "read_your_writes"
    EVENTUAL = "eventual"

@dataclass
class UseCase:
    name: str
    requires_accuracy: bool
    requires_ordering: bool
    requires_ryw: bool  # Read-your-writes
    latency_sensitive: bool
    
    def recommended_consistency(self) -> ConsistencyLevel:
        """Recommend consistency level based on requirements"""
        
        if self.requires_accuracy and self.requires_ordering:
            # Need strongest guarantees
            return ConsistencyLevel.LINEARIZABLE
        
        if self.requires_ordering:
            # Need ordering but not real-time
            return ConsistencyLevel.CAUSAL
        
        if self.requires_ryw:
            # User must see own writes
            return ConsistencyLevel.READ_YOUR_WRITES
        
        if self.latency_sensitive:
            # Speed is priority
            return ConsistencyLevel.EVENTUAL
        
        # Default to causal - good balance
        return ConsistencyLevel.CAUSAL

# Example use cases
USE_CASES = [
    UseCase(
        name="Bank Transfer",
        requires_accuracy=True,
        requires_ordering=True,
        requires_ryw=True,
        latency_sensitive=False
    ),
    UseCase(
        name="Social Media Likes",
        requires_accuracy=False,  # Approximate OK
        requires_ordering=False,
        requires_ryw=False,
        latency_sensitive=True
    ),
    UseCase(
        name="User Profile Update",
        requires_accuracy=False,
        requires_ordering=False,
        requires_ryw=True,  # User must see their changes
        latency_sensitive=True
    ),
    UseCase(
        name="Chat Messages",
        requires_accuracy=True,
        requires_ordering=True,  # Must be in order
        requires_ryw=True,
        latency_sensitive=True
    ),
    UseCase(
        name="Product Inventory",
        requires_accuracy=True,  # Can't oversell
        requires_ordering=True,
        requires_ryw=True,
        latency_sensitive=False
    ),
]

def analyze_use_cases():
    print("Consistency Level Recommendations")
    print("=" * 50)
    
    for uc in USE_CASES:
        level = uc.recommended_consistency()
        print(f"\n{uc.name}:")
        print(f"  Recommended: {level.value}")
        print(f"  Accuracy needed: {uc.requires_accuracy}")
        print(f"  Ordering needed: {uc.requires_ordering}")
        print(f"  RYW needed: {uc.requires_ryw}")
        print(f"  Latency sensitive: {uc.latency_sensitive}")

# analyze_use_cases()
```

---

## Summary

| Level | Guarantee | Performance | Use Case |
|-------|-----------|-------------|----------|
| **Linearizable** | Strongest - real-time order | Slowest | Locks, elections |
| **Sequential** | Total order, no real-time | Fast | Logs, queues |
| **Causal** | Respects causality | Good | Social, chat |
| **Read-Your-Writes** | See own writes | Good | User profiles |
| **Eventual** | Eventually converges | Fastest | Metrics, likes |

---

## Practice Exercises

1. Implement a linearizable register with compare-and-swap
2. Build a causal consistency store with vector clocks
3. Create a read-your-writes wrapper for a database
4. Simulate CAP theorem behavior during partitions
5. Classify your application's data by consistency needs

**Next Chapter**: Distributed transactions and the Saga pattern!
