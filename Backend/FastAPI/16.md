# Chapter 16: Interview-Focused Practice

## Introduction to FastAPI Interviews

This chapter prepares you for FastAPI interviews with:
- Common coding tasks
- System design questions
- Debugging scenarios
- Architecture discussions
- Best practices explanations

---

## Common Coding Tasks

### Task 1: Implement Complete CRUD with Authentication

**Interview Question**: "Build a REST API for managing blog posts with user authentication. Include CRUD operations, pagination, and filtering."

```python
# main.py - Complete CRUD Implementation
from fastapi import FastAPI, Depends, HTTPException, status, Query
from fastapi.security import OAuth2PasswordBearer, OAuth2PasswordRequestForm
from sqlalchemy import create_engine, Column, Integer, String, DateTime, ForeignKey, Boolean
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import Session, sessionmaker, relationship
from pydantic import BaseModel, EmailStr, Field
from typing import Optional, List
from datetime import datetime, timedelta
from jose import JWTError, jwt
from passlib.context import CryptContext

# ============================================================================
# Database Setup
# ============================================================================

SQLALCHEMY_DATABASE_URL = "sqlite:///./blog.db"
engine = create_engine(SQLALCHEMY_DATABASE_URL, connect_args={"check_same_thread": False})
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
Base = declarative_base()

# ============================================================================
# Models
# ============================================================================

class User(Base):
    __tablename__ = "users"
    
    id = Column(Integer, primary_key=True, index=True)
    email = Column(String, unique=True, index=True)
    username = Column(String, unique=True, index=True)
    hashed_password = Column(String)
    is_active = Column(Boolean, default=True)
    created_at = Column(DateTime, default=datetime.utcnow)
    
    posts = relationship("Post", back_populates="author")

class Post(Base):
    __tablename__ = "posts"
    
    id = Column(Integer, primary_key=True, index=True)
    title = Column(String, index=True)
    content = Column(String)
    published = Column(Boolean, default=False)
    author_id = Column(Integer, ForeignKey("users.id"))
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    
    author = relationship("User", back_populates="posts")

Base.metadata.create_all(bind=engine)

# ============================================================================
# Pydantic Schemas
# ============================================================================

class UserCreate(BaseModel):
    email: EmailStr
    username: str = Field(..., min_length=3, max_length=50)
    password: str = Field(..., min_length=8)

class UserResponse(BaseModel):
    id: int
    email: EmailStr
    username: str
    is_active: bool
    created_at: datetime
    
    class Config:
        from_attributes = True

class PostBase(BaseModel):
    title: str = Field(..., min_length=1, max_length=200)
    content: str
    published: bool = False

class PostCreate(PostBase):
    pass

class PostUpdate(BaseModel):
    title: Optional[str] = Field(None, min_length=1, max_length=200)
    content: Optional[str] = None
    published: Optional[bool] = None

class PostResponse(PostBase):
    id: int
    author_id: int
    created_at: datetime
    updated_at: datetime
    
    class Config:
        from_attributes = True

class PostWithAuthor(PostResponse):
    author: UserResponse

class Token(BaseModel):
    access_token: str
    token_type: str

# ============================================================================
# Security
# ============================================================================

SECRET_KEY = "your-secret-key-keep-it-secret"
ALGORITHM = "HS256"
ACCESS_TOKEN_EXPIRE_MINUTES = 30

pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")
oauth2_scheme = OAuth2PasswordBearer(tokenUrl="token")

def verify_password(plain_password, hashed_password):
    return pwd_context.verify(plain_password, hashed_password)

def get_password_hash(password):
    return pwd_context.hash(password)

def create_access_token(data: dict):
    to_encode = data.copy()
    expire = datetime.utcnow() + timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
    to_encode.update({"exp": expire})
    return jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)

# ============================================================================
# Dependencies
# ============================================================================

def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

async def get_current_user(
    token: str = Depends(oauth2_scheme),
    db: Session = Depends(get_db)
):
    credentials_exception = HTTPException(
        status_code=status.HTTP_401_UNAUTHORIZED,
        detail="Could not validate credentials",
        headers={"WWW-Authenticate": "Bearer"},
    )
    
    try:
        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
        username: str = payload.get("sub")
        if username is None:
            raise credentials_exception
    except JWTError:
        raise credentials_exception
    
    user = db.query(User).filter(User.username == username).first()
    if user is None:
        raise credentials_exception
    return user

# ============================================================================
# FastAPI App
# ============================================================================

app = FastAPI(title="Blog API")

# Auth endpoints
@app.post("/register", response_model=UserResponse, status_code=status.HTTP_201_CREATED)
def register(user: UserCreate, db: Session = Depends(get_db)):
    """Register new user"""
    # Check if user exists
    if db.query(User).filter(User.email == user.email).first():
        raise HTTPException(status_code=400, detail="Email already registered")
    if db.query(User).filter(User.username == user.username).first():
        raise HTTPException(status_code=400, detail="Username already taken")
    
    # Create user
    db_user = User(
        email=user.email,
        username=user.username,
        hashed_password=get_password_hash(user.password)
    )
    db.add(db_user)
    db.commit()
    db.refresh(db_user)
    return db_user

@app.post("/token", response_model=Token)
def login(form_data: OAuth2PasswordRequestForm = Depends(), db: Session = Depends(get_db)):
    """Login to get access token"""
    user = db.query(User).filter(User.username == form_data.username).first()
    if not user or not verify_password(form_data.password, user.hashed_password):
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Incorrect username or password"
        )
    
    access_token = create_access_token(data={"sub": user.username})
    return {"access_token": access_token, "token_type": "bearer"}

# Post endpoints - CREATE
@app.post("/posts", response_model=PostResponse, status_code=status.HTTP_201_CREATED)
def create_post(
    post: PostCreate,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """Create new post"""
    db_post = Post(**post.dict(), author_id=current_user.id)
    db.add(db_post)
    db.commit()
    db.refresh(db_post)
    return db_post

# Post endpoints - READ
@app.get("/posts", response_model=List[PostWithAuthor])
def list_posts(
    skip: int = Query(0, ge=0),
    limit: int = Query(10, ge=1, le=100),
    published: Optional[bool] = None,
    author_id: Optional[int] = None,
    db: Session = Depends(get_db)
):
    """List posts with pagination and filtering"""
    query = db.query(Post)
    
    # Apply filters
    if published is not None:
        query = query.filter(Post.published == published)
    if author_id is not None:
        query = query.filter(Post.author_id == author_id)
    
    # Pagination
    posts = query.offset(skip).limit(limit).all()
    return posts

@app.get("/posts/{post_id}", response_model=PostWithAuthor)
def get_post(post_id: int, db: Session = Depends(get_db)):
    """Get specific post"""
    post = db.query(Post).filter(Post.id == post_id).first()
    if not post:
        raise HTTPException(status_code=404, detail="Post not found")
    return post

# Post endpoints - UPDATE
@app.patch("/posts/{post_id}", response_model=PostResponse)
def update_post(
    post_id: int,
    post_update: PostUpdate,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """Update post (only author can update)"""
    db_post = db.query(Post).filter(Post.id == post_id).first()
    if not db_post:
        raise HTTPException(status_code=404, detail="Post not found")
    
    # Authorization check
    if db_post.author_id != current_user.id:
        raise HTTPException(status_code=403, detail="Not authorized")
    
    # Update only provided fields
    update_data = post_update.dict(exclude_unset=True)
    for field, value in update_data.items():
        setattr(db_post, field, value)
    
    db_post.updated_at = datetime.utcnow()
    db.commit()
    db.refresh(db_post)
    return db_post

# Post endpoints - DELETE
@app.delete("/posts/{post_id}", status_code=status.HTTP_204_NO_CONTENT)
def delete_post(
    post_id: int,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """Delete post (only author can delete)"""
    db_post = db.query(Post).filter(Post.id == post_id).first()
    if not db_post:
        raise HTTPException(status_code=404, detail="Post not found")
    
    # Authorization check
    if db_post.author_id != current_user.id:
        raise HTTPException(status_code=403, detail="Not authorized")
    
    db.delete(db_post)
    db.commit()
```

**Interview Tips:**
- Explain your design choices (why SQLAlchemy, why JWT)
- Mention security concerns (password hashing, authorization)
- Discuss scalability (pagination, filtering)
- Show you understand HTTP status codes
- Demonstrate proper error handling

---

### Task 2: File Upload with Validation

**Interview Question**: "Implement a file upload endpoint that validates file type and size."

```python
from fastapi import FastAPI, File, UploadFile, HTTPException
from pathlib import Path
import hashlib
import uuid
from typing import List
import magic  # uv pip install python-magic

app = FastAPI()

# Configuration
UPLOAD_DIR = Path("uploads")
UPLOAD_DIR.mkdir(exist_ok=True)

ALLOWED_EXTENSIONS = {'.jpg', '.jpeg', '.png', '.pdf'}
ALLOWED_MIMES = {'image/jpeg', 'image/png', 'application/pdf'}
MAX_FILE_SIZE = 10 * 1024 * 1024  # 10MB

async def validate_file(file: UploadFile) -> bytes:
    """
    Validate file with multiple checks
    
    Interview points to mention:
    - Check file extension (but don't rely on it alone)
    - Verify actual file type using magic numbers
    - Enforce size limits
    - Prevent path traversal attacks
    """
    # Read file content
    content = await file.read()
    
    # 1. Check file size
    if len(content) > MAX_FILE_SIZE:
        raise HTTPException(
            status_code=400,
            detail=f"File too large. Max size: {MAX_FILE_SIZE / (1024*1024)}MB"
        )
    
    # 2. Check file extension
    file_ext = Path(file.filename).suffix.lower()
    if file_ext not in ALLOWED_EXTENSIONS:
        raise HTTPException(
            status_code=400,
            detail=f"File type {file_ext} not allowed"
        )
    
    # 3. Verify actual MIME type (prevents renaming attacks)
    mime_type = magic.from_buffer(content, mime=True)
    if mime_type not in ALLOWED_MIMES:
        raise HTTPException(
            status_code=400,
            detail=f"Invalid file type: {mime_type}"
        )
    
    return content

def generate_safe_filename(original_filename: str) -> str:
    """
    Generate safe filename to prevent path traversal
    
    Interview points:
    - Never use user-provided filename directly
    - Generate unique names to prevent overwrites
    - Preserve extension for proper handling
    """
    file_ext = Path(original_filename).suffix.lower()
    unique_id = uuid.uuid4()
    return f"{unique_id}{file_ext}"

def calculate_checksum(content: bytes) -> str:
    """Calculate file checksum for deduplication"""
    return hashlib.sha256(content).hexdigest()

@app.post("/upload")
async def upload_file(file: UploadFile = File(...)):
    """
    Upload single file with validation
    
    Interview discussion points:
    - Security: validation, safe filenames, virus scanning
    - Storage: local vs cloud (S3, GCS)
    - Deduplication: use checksums
    - Metadata: store original name, size, type
    """
    # Validate file
    content = await validate_file(file)
    
    # Generate safe filename
    safe_filename = generate_safe_filename(file.filename)
    file_path = UPLOAD_DIR / safe_filename
    
    # Calculate checksum for deduplication
    checksum = calculate_checksum(content)
    
    # Save file
    with open(file_path, "wb") as f:
        f.write(content)
    
    return {
        "filename": safe_filename,
        "original_filename": file.filename,
        "size": len(content),
        "checksum": checksum,
        "url": f"/files/{safe_filename}"
    }

@app.post("/upload/multiple")
async def upload_multiple_files(files: List[UploadFile] = File(...)):
    """
    Upload multiple files
    
    Interview points:
    - Handle partial failures gracefully
    - Consider transaction-like behavior
    - Limit number of files per request
    """
    if len(files) > 10:
        raise HTTPException(status_code=400, detail="Max 10 files per request")
    
    results = []
    errors = []
    
    for file in files:
        try:
            content = await validate_file(file)
            safe_filename = generate_safe_filename(file.filename)
            file_path = UPLOAD_DIR / safe_filename
            
            with open(file_path, "wb") as f:
                f.write(content)
            
            results.append({
                "filename": safe_filename,
                "original_filename": file.filename,
                "size": len(content)
            })
        except HTTPException as e:
            errors.append({
                "filename": file.filename,
                "error": e.detail
            })
    
    return {
        "uploaded": results,
        "errors": errors,
        "total": len(files),
        "success": len(results)
    }
```

---

### Task 3: Pagination and Filtering

**Interview Question**: "Implement a product search endpoint with pagination, sorting, and multiple filters."

```python
from fastapi import FastAPI, Query, Depends
from sqlalchemy.orm import Session
from typing import Optional, List
from enum import Enum
from pydantic import BaseModel

app = FastAPI()

class SortOrder(str, Enum):
    asc = "asc"
    desc = "desc"

class PaginatedResponse(BaseModel):
    """
    Standard pagination response
    
    Interview points:
    - Include metadata for client pagination UI
    - Total count for "showing X of Y"
    - Page/pages for navigation
    - Has next/prev for infinite scroll
    """
    items: List[dict]
    total: int
    page: int
    page_size: int
    pages: int
    has_next: bool
    has_prev: bool

@app.get("/products", response_model=PaginatedResponse)
def search_products(
    # Pagination
    page: int = Query(1, ge=1, description="Page number"),
    page_size: int = Query(20, ge=1, le=100, description="Items per page"),
    
    # Search
    q: Optional[str] = Query(None, min_length=2, description="Search query"),
    
    # Filters
    category: Optional[str] = None,
    min_price: Optional[float] = Query(None, ge=0),
    max_price: Optional[float] = Query(None, ge=0),
    in_stock: bool = Query(True),
    tags: List[str] = Query([]),
    
    # Sorting
    sort_by: str = Query("created_at", description="Sort field"),
    sort_order: SortOrder = Query(SortOrder.desc),
    
    db: Session = Depends(get_db)
):
    """
    Product search with comprehensive filtering
    
    Interview discussion:
    - Database indexing strategy for performance
    - Query optimization (avoid N+1, use joins)
    - Caching strategy (Redis for frequent queries)
    - Full-text search vs SQL LIKE (consider Elasticsearch)
    """
    # Base query
    query = db.query(Product)
    
    # Search (full-text search would be better for production)
    if q:
        search_filter = (
            Product.name.ilike(f"%{q}%") |
            Product.description.ilike(f"%{q}%")
        )
        query = query.filter(search_filter)
    
    # Filters
    if category:
        query = query.filter(Product.category == category)
    
    if min_price is not None:
        query = query.filter(Product.price >= min_price)
    
    if max_price is not None:
        query = query.filter(Product.price <= max_price)
    
    if in_stock:
        query = query.filter(Product.stock > 0)
    
    if tags:
        # Assuming tags stored as JSON array
        for tag in tags:
            query = query.filter(Product.tags.contains([tag]))
    
    # Count total (before pagination)
    total = query.count()
    
    # Sorting
    sort_column = getattr(Product, sort_by, Product.created_at)
    if sort_order == SortOrder.desc:
        query = query.order_by(sort_column.desc())
    else:
        query = query.order_by(sort_column.asc())
    
    # Pagination
    skip = (page - 1) * page_size
    items = query.offset(skip).limit(page_size).all()
    
    # Calculate metadata
    pages = (total + page_size - 1) // page_size
    
    return {
        "items": [item.to_dict() for item in items],
        "total": total,
        "page": page,
        "page_size": page_size,
        "pages": pages,
        "has_next": page < pages,
        "has_prev": page > 1
    }
```

---

## System Design Questions

### Question 1: Design a High-Traffic REST API

**Interviewer**: "Design a REST API for an e-commerce platform that handles 10,000 requests per second."

**Architecture Diagram**:
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  CDN (CloudFlare)                    ‚îÇ
‚îÇ              Static Assets, Images                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              Load Balancer (AWS ALB)                 ‚îÇ
‚îÇ          SSL Termination, Health Checks              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ             ‚îÇ             ‚îÇ            ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇFastAPI‚îÇ    ‚îÇFastAPI‚îÇ    ‚îÇFastAPI‚îÇ   ‚îÇFastAPI‚îÇ
    ‚îÇ Pod 1 ‚îÇ    ‚îÇ Pod 2 ‚îÇ    ‚îÇ Pod 3 ‚îÇ   ‚îÇ Pod N ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ            ‚îÇ            ‚îÇ           ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ            ‚îÇ                        ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇRead DB ‚îÇ  ‚îÇMaster‚îÇ  ‚îÇ Redis  ‚îÇ  ‚îÇ  Celery   ‚îÇ
    ‚îÇReplica ‚îÇ  ‚îÇ  DB  ‚îÇ  ‚îÇ Cache  ‚îÇ  ‚îÇ  Workers  ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Discussion Points**:

```python
# 1. Stateless API Design
"""
- No in-memory sessions
- JWT tokens for authentication
- Horizontal scaling capability
"""

# 2. Database Strategy
"""
- Master-Replica setup
- Read from replicas, write to master
- Connection pooling (20-50 connections)
- Query optimization and indexing
"""

# Example: Read replica configuration
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker

# Write DB
master_engine = create_engine(
    "postgresql://master:5432/db",
    pool_size=20,
    max_overflow=10
)

# Read DB
replica_engine = create_engine(
    "postgresql://replica:5432/db",
    pool_size=50,  # More read connections
    max_overflow=20
)

def get_db_read():
    """Use for read operations"""
    SessionLocal = sessionmaker(bind=replica_engine)
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

def get_db_write():
    """Use for write operations"""
    SessionLocal = sessionmaker(bind=master_engine)
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

# 3. Caching Strategy
"""
- Redis for hot data (products, categories)
- Cache-aside pattern
- TTL based on data volatility
  - Product catalog: 1 hour
  - User sessions: 15 minutes
  - Shopping cart: 24 hours
"""

from functools import wraps
import json

def cache_response(key_prefix: str, ttl: int = 300):
    """Cache decorator for endpoints"""
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            # Generate cache key
            cache_key = f"{key_prefix}:{json.dumps(kwargs)}"
            
            # Try cache
            cached = redis.get(cache_key)
            if cached:
                return json.loads(cached)
            
            # Execute function
            result = await func(*args, **kwargs)
            
            # Cache result
            redis.setex(cache_key, ttl, json.dumps(result))
            
            return result
        return wrapper
    return decorator

# 4. Rate Limiting
"""
- Per-IP: 100 req/min for anonymous
- Per-User: 1000 req/min for authenticated
- Redis-based sliding window
"""

# 5. Monitoring
"""
- Prometheus metrics
- Request duration histograms
- Error rate tracking
- Database query performance
- Cache hit/miss ratio
"""
```

---

### Question 2: Design an Authentication Service

**Interviewer**: "Design a centralized authentication service for multiple microservices."

```python
"""
Architecture:

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ          API Gateway / Auth Service              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îÇ
‚îÇ  ‚îÇ  /login    ‚îÇ      ‚îÇ  /refresh  ‚îÇ            ‚îÇ
‚îÇ  ‚îÇ  /logout   ‚îÇ      ‚îÇ  /verify   ‚îÇ            ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ          ‚îÇ          ‚îÇ              ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇUser   ‚îÇ  ‚îÇOrder ‚îÇ  ‚îÇProduct‚îÇ    ‚îÇPayment   ‚îÇ
‚îÇService‚îÇ  ‚îÇService‚îÇ ‚îÇService‚îÇ    ‚îÇService   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
"""

# auth_service.py
from fastapi import FastAPI, Depends, HTTPException, status
from fastapi.security import OAuth2PasswordBearer, OAuth2PasswordRequestForm
from jose import JWTError, jwt
from datetime import datetime, timedelta
from typing import Optional
import redis

app = FastAPI(title="Auth Service")

# Configuration
SECRET_KEY = "your-secret-key"
ALGORITHM = "HS256"
ACCESS_TOKEN_EXPIRE_MINUTES = 15
REFRESH_TOKEN_EXPIRE_DAYS = 7

redis_client = redis.Redis(host='redis', port=6379, decode_responses=True)

class TokenPair(BaseModel):
    access_token: str
    refresh_token: str
    token_type: str = "bearer"

def create_token(data: dict, expires_delta: timedelta, token_type: str):
    """Create JWT token"""
    to_encode = data.copy()
    expire = datetime.utcnow() + expires_delta
    to_encode.update({
        "exp": expire,
        "iat": datetime.utcnow(),
        "type": token_type
    })
    return jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)

@app.post("/auth/login", response_model=TokenPair)
async def login(form_data: OAuth2PasswordRequestForm = Depends()):
    """
    Login endpoint
    
    Interview points:
    - Return both access and refresh tokens
    - Store refresh token in Redis with TTL
    - Implement account lockout after failed attempts
    - Log login attempts for security monitoring
    """
    # Authenticate user
    user = authenticate_user(form_data.username, form_data.password)
    if not user:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Incorrect username or password"
        )
    
    # Create tokens
    access_token = create_token(
        data={"sub": user.username, "user_id": user.id},
        expires_delta=timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES),
        token_type="access"
    )
    
    refresh_token = create_token(
        data={"sub": user.username, "user_id": user.id},
        expires_delta=timedelta(days=REFRESH_TOKEN_EXPIRE_DAYS),
        token_type="refresh"
    )
    
    # Store refresh token in Redis
    redis_client.setex(
        f"refresh_token:{user.id}",
        timedelta(days=REFRESH_TOKEN_EXPIRE_DAYS),
        refresh_token
    )
    
    return {
        "access_token": access_token,
        "refresh_token": refresh_token,
        "token_type": "bearer"
    }

@app.post("/auth/refresh", response_model=TokenPair)
async def refresh_token(refresh_token: str):
    """
    Refresh access token
    
    Interview points:
    - Validate refresh token
    - Check if token is in Redis (not revoked)
    - Issue new token pair
    - Implement token rotation for security
    """
    try:
        payload = jwt.decode(refresh_token, SECRET_KEY, algorithms=[ALGORITHM])
        
        if payload.get("type") != "refresh":
            raise HTTPException(status_code=401, detail="Invalid token type")
        
        user_id = payload.get("user_id")
        
        # Verify token is in Redis
        stored_token = redis_client.get(f"refresh_token:{user_id}")
        if stored_token != refresh_token:
            raise HTTPException(status_code=401, detail="Token revoked or expired")
        
        # Create new token pair
        username = payload.get("sub")
        new_access_token = create_token(
            data={"sub": username, "user_id": user_id},
            expires_delta=timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES),
            token_type="access"
        )
        
        new_refresh_token = create_token(
            data={"sub": username, "user_id": user_id},
            expires_delta=timedelta(days=REFRESH_TOKEN_EXPIRE_DAYS),
            token_type="refresh"
        )
        
        # Update Redis
        redis_client.setex(
            f"refresh_token:{user_id}",
            timedelta(days=REFRESH_TOKEN_EXPIRE_DAYS),
            new_refresh_token
        )
        
        return {
            "access_token": new_access_token,
            "refresh_token": new_refresh_token,
            "token_type": "bearer"
        }
        
    except JWTError:
        raise HTTPException(status_code=401, detail="Invalid token")

@app.post("/auth/logout")
async def logout(token: str = Depends(oauth2_scheme)):
    """
    Logout - revoke tokens
    
    Interview points:
    - Add token to blacklist in Redis
    - Clear refresh token
    - Set blacklist TTL to token expiration time
    """
    try:
        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
        user_id = payload.get("user_id")
        
        # Delete refresh token
        redis_client.delete(f"refresh_token:{user_id}")
        
        # Blacklist access token
        exp_timestamp = payload.get("exp")
        if exp_timestamp:
            ttl = exp_timestamp - datetime.utcnow().timestamp()
            if ttl > 0:
                redis_client.setex(f"blacklist:{token}", int(ttl), "1")
        
        return {"message": "Logged out successfully"}
        
    except JWTError:
        raise HTTPException(status_code=401, detail="Invalid token")

@app.get("/auth/verify")
async def verify_token(token: str = Depends(oauth2_scheme)):
    """
    Verify token for other services
    
    Interview points:
    - Called by API Gateway or microservices
    - Check token validity
    - Check blacklist
    - Return user info for authorization
    """
    # Check blacklist
    if redis_client.exists(f"blacklist:{token}"):
        raise HTTPException(status_code=401, detail="Token revoked")
    
    try:
        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
        
        if payload.get("type") != "access":
            raise HTTPException(status_code=401, detail="Invalid token type")
        
        return {
            "valid": True,
            "user_id": payload.get("user_id"),
            "username": payload.get("sub"),
            "exp": payload.get("exp")
        }
        
    except JWTError:
        raise HTTPException(status_code=401, detail="Invalid token")
```

---

## Debugging Scenarios

### Scenario 1: N+1 Query Problem

**Interviewer shows you slow code**: "This endpoint is taking 5 seconds. Why?"

```python
# ‚ùå BAD CODE (N+1 Problem)
@app.get("/users-with-posts")
def get_users_with_posts(db: Session = Depends(get_db)):
    """
    Problem: For each user, makes separate query for posts
    If 100 users -> 101 queries (1 + 100)
    """
    users = db.query(User).all()  # Query 1
    
    result = []
    for user in users:
        posts = user.posts  # Query 2, 3, 4... N
        result.append({
            "user": user,
            "posts": posts,
            "post_count": len(posts)
        })
    
    return result

# ‚úÖ FIXED CODE (Eager Loading)
from sqlalchemy.orm import joinedload

@app.get("/users-with-posts")
def get_users_with_posts(db: Session = Depends(get_db)):
    """
    Solution: Eager load posts with JOIN
    Now only 1 query regardless of user count
    """
    users = db.query(User).options(
        joinedload(User.posts)  # Load posts in same query
    ).all()
    
    result = []
    for user in users:
        result.append({
            "user": user,
            "posts": user.posts,  # Already loaded, no query
            "post_count": len(user.posts)
        })
    
    return result

# Interview Discussion:
"""
1. Identify the problem: Use logging or query profiling
2. Solutions:
   - joinedload: One query with JOIN (one-to-many)
   - selectinload: Separate query with IN clause (better for large collections)
   - subqueryload: Use subquery
3. Trade-offs:
   - joinedload: May return duplicate data for parent
   - selectinload: Two queries but more efficient for large data
4. Monitoring: Log slow queries, use database query analyzer
"""
```

### Scenario 2: Memory Leak

**Interviewer**: "The API memory usage keeps growing. Find the issue."

```python
# ‚ùå BAD CODE (Memory Leak)
from functools import lru_cache

# Global cache without size limit
results_cache = {}

@app.get("/search")
def search(q: str):
    """
    Problem: Cache grows indefinitely
    Every unique query stays in memory forever
    """
    if q in results_cache:
        return results_cache[q]
    
    results = expensive_search(q)
    results_cache[q] = results  # Never cleaned up!
    return results

# ‚úÖ FIXED CODE (Bounded Cache)
from cachetools import TTLCache
import redis

# Option 1: Bounded in-memory cache
results_cache = TTLCache(maxsize=1000, ttl=3600)  # Max 1000 items, 1 hour TTL

@app.get("/search")
def search(q: str):
    """Solution: Use bounded cache with TTL"""
    if q in results_cache:
        return results_cache[q]
    
    results = expensive_search(q)
    results_cache[q] = results
    return results

# Option 2: Redis cache (better for production)
redis_client = redis.Redis(host='redis', decode_responses=True)

@app.get("/search")
def search(q: str):
    """Better: Use Redis for distributed caching"""
    cache_key = f"search:{q}"
    
    # Try cache
    cached = redis_client.get(cache_key)
    if cached:
        return json.loads(cached)
    
    # Cache miss
    results = expensive_search(q)
    redis_client.setex(cache_key, 3600, json.dumps(results))
    return results

# Interview Discussion:
"""
1. How to identify:
   - Monitor memory usage over time
   - Use memory profilers (memory_profiler, tracemalloc)
   - Check for unbounded data structures

2. Common causes:
   - Unbounded caches
   - Not closing database connections
   - Circular references
   - Large objects in memory

3. Solutions:
   - Use bounded caches (LRU, TTL)
   - Implement cache eviction policies
   - Use external cache (Redis)
   - Profile memory usage
"""
```

---

## Best Practices Explanation

### Question: "Explain FastAPI's Dependency Injection"

**Answer**:
```python
"""
FastAPI's DI system is powerful for:
1. Code reuse
2. Testability
3. Separation of concerns
"""

# Example: Multi-layer dependencies
from fastapi import Depends

# Layer 1: Database
def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

# Layer 2: Repository (depends on Layer 1)
def get_user_repository(db: Session = Depends(get_db)):
    return UserRepository(db)

# Layer 3: Service (depends on Layer 2)
def get_user_service(repo = Depends(get_user_repository)):
    return UserService(repo)

# Layer 4: Endpoint (depends on Layer 3)
@app.get("/users")
def list_users(
    service: UserService = Depends(get_user_service),
    current_user: User = Depends(get_current_user)
):
    return service.list_users()

"""
Benefits:
1. Testability: Override dependencies in tests
2. Lazy evaluation: Only runs when needed
3. Caching: Same dependency reused in same request
4. Type safety: Full IDE autocomplete support
5. No globals: Each request gets isolated dependencies
"""
```

---

## Interview Preparation Checklist

### Technical Questions You Should Master

1. **Why FastAPI over Flask/Django?**
   - Performance (async, Starlette)
   - Automatic documentation
   - Type hints for validation
   - Modern Python features

2. **How does async/await work?**
   - Non-blocking I/O
   - Event loop
   - When to use async vs sync

3. **Explain JWT authentication**
   - Stateless
   - Access vs refresh tokens
   - Token structure (header.payload.signature)

4. **How to scale FastAPI?**
   - Horizontal scaling (multiple instances)
   - Load balancing
   - Stateless design
   - Database connection pooling
   - Caching strategy

5. **Database optimization**
   - Indexing
   - Query optimization
   - N+1 problem and solutions
   - Connection pooling
   - Read replicas

---

## Summary

In Chapter 16, you learned:
- Common coding tasks (CRUD, file upload, pagination)
- System design approaches for high-traffic APIs
- Authentication service architecture
- Debugging scenarios (N+1 queries, memory leaks)
- Best practices explanations
- Interview preparation strategies

**Interview Success Tips:**
- ‚úÖ Write clean, readable code
- ‚úÖ Explain your design decisions
- ‚úÖ Consider edge cases and errors
- ‚úÖ Discuss trade-offs
- ‚úÖ Mention security concerns
- ‚úÖ Think about scalability
- ‚úÖ Ask clarifying questions
- ‚úÖ Communicate clearly

**Practice Recommendations:**
1. Build a complete project from scratch
2. Practice explaining your code
3. Review OWASP Top 10
4. Study system design patterns
5. Practice live coding
6. Review FastAPI documentation
7. Understand async/await deeply
8. Practice with a timer (simulate interview pressure)

**Good luck with your FastAPI interviews!** üöÄ