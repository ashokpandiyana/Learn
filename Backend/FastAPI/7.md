# Chapter 7: Background Tasks, Events & Scheduling

## Introduction to Background Processing

Background tasks are essential for operations that shouldn't block the response:
- Sending emails
- Processing uploaded files
- Generating reports
- Updating caches
- Logging analytics
- Calling external APIs

**Key Principle**: Never make users wait for non-critical operations.

---

## FastAPI Background Tasks

### Basic Background Tasks

```python
from fastapi import FastAPI, BackgroundTasks
import time
from datetime import datetime

app = FastAPI()

def write_log(message: str):
    """Background task to write logs"""
    # This runs after the response is sent
    with open("log.txt", "a") as log_file:
        log_file.write(f"{datetime.now()}: {message}\n")
    time.sleep(2)  # Simulate slow operation

@app.post("/send-notification")
async def send_notification(
    email: str,
    background_tasks: BackgroundTasks
):
    """
    Send notification in background
    Response is sent immediately, notification sent after
    """
    # Add task to background
    background_tasks.add_task(write_log, f"Notification sent to {email}")
    
    # Return immediately (don't wait for task)
    return {"message": "Notification will be sent in background"}

# Multiple background tasks
def send_email(email: str, message: str):
    """Simulate sending email"""
    print(f"Sending email to {email}: {message}")
    time.sleep(1)

def update_analytics(user_id: int, action: str):
    """Update analytics"""
    print(f"Analytics: User {user_id} performed {action}")
    time.sleep(0.5)

@app.post("/process-order")
async def process_order(
    user_id: int,
    order_id: int,
    background_tasks: BackgroundTasks
):
    """Process order with multiple background tasks"""
    # Add multiple tasks - they run sequentially
    background_tasks.add_task(
        send_email,
        f"user{user_id}@example.com",
        f"Order {order_id} confirmed"
    )
    background_tasks.add_task(
        update_analytics,
        user_id,
        "order_created"
    )
    background_tasks.add_task(
        write_log,
        f"Order {order_id} processed for user {user_id}"
    )
    
    return {
        "message": "Order processed",
        "order_id": order_id
    }
```

### Background Tasks with Dependencies

```python
from fastapi import Depends, BackgroundTasks
from sqlalchemy.orm import Session
from database import get_db
from models import User, Order

def send_order_confirmation(
    order_id: int,
    db: Session
):
    """Background task that needs database access"""
    # This needs to create its own session or receive one
    order = db.query(Order).filter(Order.id == order_id).first()
    if order:
        print(f"Sending confirmation for order {order_id}")
        # Send email logic here

@app.post("/orders")
async def create_order(
    order_data: dict,
    background_tasks: BackgroundTasks,
    db: Session = Depends(get_db)
):
    """Create order with background confirmation"""
    # Create order
    order = Order(**order_data)
    db.add(order)
    db.commit()
    db.refresh(order)
    
    # Add background task
    # Note: Pass db session or order_id, not the SQLAlchemy object
    background_tasks.add_task(
        send_order_confirmation,
        order.id,
        db
    )
    
    return order
```

### When to Use FastAPI Background Tasks

**‚úÖ Good for:**
- Quick tasks (< 5 seconds)
- Tasks that can tolerate occasional failure
- Simple post-request operations
- Logging, analytics updates
- Cache invalidation

**‚ùå Not good for:**
- Long-running tasks (> 30 seconds)
- Tasks requiring guaranteed execution
- Tasks that need retry logic
- CPU-intensive operations
- Tasks that need to be scheduled

---

## Startup and Shutdown Events

### Lifespan Context Manager (Modern Approach)

```python
from fastapi import FastAPI
from contextlib import asynccontextmanager
import asyncio

# Global state
ml_model = None
db_connection = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    """
    Lifespan context manager
    Code before yield: runs on startup
    Code after yield: runs on shutdown
    """
    # ========== STARTUP ==========
    print("üöÄ Application starting up...")
    
    # Load ML model
    global ml_model
    print("Loading ML model...")
    ml_model = {"model": "loaded"}  # Simulate loading
    print("‚úÖ ML model loaded")
    
    # Initialize database connection pool
    global db_connection
    print("Initializing database connection pool...")
    db_connection = {"pool": "active"}
    print("‚úÖ Database pool initialized")
    
    # Start background services
    print("Starting background services...")
    # background_service = await start_service()
    
    # Cache warm-up
    print("Warming up cache...")
    await asyncio.sleep(1)  # Simulate cache warming
    print("‚úÖ Cache ready")
    
    print("‚úÖ Application startup complete")
    
    yield  # Application runs here
    
    # ========== SHUTDOWN ==========
    print("üõë Application shutting down...")
    
    # Close database connections
    print("Closing database connections...")
    db_connection = None
    print("‚úÖ Database connections closed")
    
    # Save state
    print("Saving application state...")
    # save_state()
    
    # Clean up resources
    print("Cleaning up resources...")
    ml_model = None
    
    print("‚úÖ Application shutdown complete")

app = FastAPI(lifespan=lifespan)

@app.get("/predict")
async def predict(data: dict):
    """Endpoint using the loaded model"""
    if ml_model is None:
        return {"error": "Model not loaded"}
    return {"prediction": "result", "model": ml_model}

@app.get("/health")
async def health():
    """Health check"""
    return {
        "status": "healthy",
        "model_loaded": ml_model is not None,
        "db_connected": db_connection is not None
    }
```

### Legacy Event Handlers (Still Supported)

```python
from fastapi import FastAPI
import asyncio

app = FastAPI()

# Startup event
@app.on_event("startup")
async def startup_event():
    """Runs once when application starts"""
    print("Starting up...")
    # Initialize resources
    app.state.cache = {}
    app.state.connection_pool = "initialized"

# Shutdown event
@app.on_event("shutdown")
async def shutdown_event():
    """Runs once when application shuts down"""
    print("Shutting down...")
    # Clean up resources
    app.state.cache.clear()

# Multiple startup handlers (run in order)
@app.on_event("startup")
async def load_model():
    """Load ML model"""
    print("Loading model...")
    await asyncio.sleep(1)
    app.state.model = {"loaded": True}

@app.on_event("startup")
async def connect_to_database():
    """Connect to database"""
    print("Connecting to database...")
    app.state.db = "connected"

@app.get("/")
async def root():
    """Access startup-initialized resources"""
    return {
        "cache": app.state.cache,
        "db": app.state.db,
        "model": app.state.model
    }
```

### Real-World Startup Example

```python
from fastapi import FastAPI
from contextlib import asynccontextmanager
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from redis import Redis
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Production-ready lifespan manager"""
    
    # ========== STARTUP ==========
    logger.info("Application startup initiated")
    
    try:
        # 1. Database connection pool
        logger.info("Creating database connection pool...")
        database_url = "postgresql+asyncpg://user:pass@localhost/db"
        engine = create_async_engine(
            database_url,
            pool_size=20,
            max_overflow=10,
            pool_pre_ping=True,
        )
        app.state.db_engine = engine
        logger.info("‚úÖ Database pool created")
        
        # 2. Redis connection
        logger.info("Connecting to Redis...")
        redis_client = Redis(
            host='localhost',
            port=6379,
            db=0,
            decode_responses=True
        )
        # Test connection
        redis_client.ping()
        app.state.redis = redis_client
        logger.info("‚úÖ Redis connected")
        
        # 3. Load configuration
        logger.info("Loading configuration...")
        from config import get_settings
        app.state.settings = get_settings()
        logger.info("‚úÖ Configuration loaded")
        
        # 4. Load ML models (if any)
        logger.info("Loading ML models...")
        # app.state.model = load_model()
        logger.info("‚úÖ Models loaded")
        
        # 5. Start background tasks
        logger.info("Starting background workers...")
        # app.state.worker = start_worker()
        logger.info("‚úÖ Background workers started")
        
        logger.info("üöÄ Application startup complete")
        
    except Exception as e:
        logger.error(f"‚ùå Startup failed: {e}")
        raise
    
    yield  # Application runs
    
    # ========== SHUTDOWN ==========
    logger.info("Application shutdown initiated")
    
    try:
        # 1. Close database connections
        logger.info("Closing database connections...")
        await app.state.db_engine.dispose()
        logger.info("‚úÖ Database connections closed")
        
        # 2. Close Redis connection
        logger.info("Closing Redis connection...")
        app.state.redis.close()
        logger.info("‚úÖ Redis connection closed")
        
        # 3. Stop background workers
        logger.info("Stopping background workers...")
        # await app.state.worker.stop()
        logger.info("‚úÖ Background workers stopped")
        
        logger.info("‚úÖ Application shutdown complete")
        
    except Exception as e:
        logger.error(f"‚ùå Shutdown error: {e}")

app = FastAPI(lifespan=lifespan)
```

---

## Task Queues with Celery

### Installation

```bash
# Install Celery with Redis broker
uv pip install celery redis

# Optional: Flower for monitoring
uv pip install flower
```

### Celery Configuration

```python
# celery_app.py
from celery import Celery
from kombu import Queue

# Create Celery instance
celery_app = Celery(
    "worker",
    broker="redis://localhost:6379/0",
    backend="redis://localhost:6379/0"
)

# Configuration
celery_app.conf.update(
    task_serializer='json',
    accept_content=['json'],
    result_serializer='json',
    timezone='UTC',
    enable_utc=True,
    task_track_started=True,
    task_time_limit=30 * 60,  # 30 minutes
    task_soft_time_limit=25 * 60,  # 25 minutes
    worker_prefetch_multiplier=1,
    worker_max_tasks_per_child=1000,
)

# Task routing
celery_app.conf.task_routes = {
    'tasks.send_email': {'queue': 'emails'},
    'tasks.process_image': {'queue': 'images'},
    'tasks.generate_report': {'queue': 'reports'},
}

# Queues
celery_app.conf.task_queues = (
    Queue('default'),
    Queue('emails'),
    Queue('images'),
    Queue('reports'),
)
```

### Defining Celery Tasks

```python
# tasks.py
from celery_app import celery_app
import time
from datetime import datetime

@celery_app.task(name="tasks.send_email")
def send_email_task(email: str, subject: str, body: str):
    """
    Celery task to send email
    Runs in separate worker process
    """
    print(f"Sending email to {email}")
    print(f"Subject: {subject}")
    time.sleep(2)  # Simulate email sending
    print("Email sent successfully")
    return {"status": "sent", "email": email}

@celery_app.task(name="tasks.process_image", bind=True)
def process_image_task(self, image_path: str):
    """
    Task with progress tracking
    bind=True gives access to task instance (self)
    """
    try:
        print(f"Processing image: {image_path}")
        
        # Update progress
        self.update_state(
            state='PROGRESS',
            meta={'current': 0, 'total': 100}
        )
        
        # Simulate processing steps
        for i in range(1, 101):
            time.sleep(0.05)
            self.update_state(
                state='PROGRESS',
                meta={'current': i, 'total': 100}
            )
        
        return {"status": "completed", "path": image_path}
        
    except Exception as e:
        self.update_state(
            state='FAILURE',
            meta={'error': str(e)}
        )
        raise

@celery_app.task(name="tasks.generate_report")
def generate_report_task(user_id: int, report_type: str):
    """Generate report task"""
    print(f"Generating {report_type} report for user {user_id}")
    time.sleep(5)  # Simulate report generation
    
    report_data = {
        "user_id": user_id,
        "type": report_type,
        "generated_at": datetime.now().isoformat(),
        "data": "report_content"
    }
    
    return report_data

# Task with retry logic
@celery_app.task(
    name="tasks.call_external_api",
    autoretry_for=(Exception,),
    retry_kwargs={'max_retries': 3, 'countdown': 5},
    retry_backoff=True
)
def call_external_api_task(url: str):
    """
    Task with automatic retry
    - Retries on any exception
    - Max 3 retries
    - 5 seconds between retries
    - Exponential backoff
    """
    import requests
    response = requests.get(url, timeout=10)
    response.raise_for_status()
    return response.json()
```

### FastAPI Integration with Celery

```python
# main.py
from fastapi import FastAPI, BackgroundTasks, HTTPException
from pydantic import BaseModel, EmailStr
from tasks import (
    send_email_task,
    process_image_task,
    generate_report_task
)
from celery.result import AsyncResult

app = FastAPI()

# ========== Email Endpoint ==========

class EmailRequest(BaseModel):
    email: EmailStr
    subject: str
    body: str

@app.post("/send-email")
async def send_email(email_request: EmailRequest):
    """
    Send email using Celery
    Returns task_id for tracking
    """
    # Send task to Celery
    task = send_email_task.delay(
        email_request.email,
        email_request.subject,
        email_request.body
    )
    
    return {
        "message": "Email task queued",
        "task_id": task.id,
        "status_url": f"/task/{task.id}"
    }

# ========== Image Processing Endpoint ==========

@app.post("/process-image")
async def process_image(image_path: str):
    """Process image asynchronously"""
    task = process_image_task.delay(image_path)
    
    return {
        "message": "Image processing started",
        "task_id": task.id,
        "status_url": f"/task/{task.id}"
    }

# ========== Report Generation Endpoint ==========

@app.post("/generate-report")
async def generate_report(user_id: int, report_type: str):
    """Generate report asynchronously"""
    task = generate_report_task.delay(user_id, report_type)
    
    return {
        "message": "Report generation started",
        "task_id": task.id,
        "status_url": f"/task/{task.id}"
    }

# ========== Task Status Endpoint ==========

@app.get("/task/{task_id}")
async def get_task_status(task_id: str):
    """
    Get status of Celery task
    
    States:
    - PENDING: Task waiting for execution
    - STARTED: Task has been started
    - SUCCESS: Task executed successfully
    - FAILURE: Task execution failed
    - RETRY: Task is being retried
    - PROGRESS: Custom state for progress tracking
    """
    task_result = AsyncResult(task_id)
    
    if task_result.state == 'PENDING':
        response = {
            "task_id": task_id,
            "state": task_result.state,
            "status": "Task is waiting in queue"
        }
    elif task_result.state == 'PROGRESS':
        response = {
            "task_id": task_id,
            "state": task_result.state,
            "current": task_result.info.get('current', 0),
            "total": task_result.info.get('total', 100),
            "status": "Task is in progress"
        }
    elif task_result.state == 'SUCCESS':
        response = {
            "task_id": task_id,
            "state": task_result.state,
            "result": task_result.result,
            "status": "Task completed successfully"
        }
    elif task_result.state == 'FAILURE':
        response = {
            "task_id": task_id,
            "state": task_result.state,
            "error": str(task_result.info),
            "status": "Task failed"
        }
    else:
        response = {
            "task_id": task_id,
            "state": task_result.state,
            "status": f"Task is {task_result.state.lower()}"
        }
    
    return response

# ========== Cancel Task Endpoint ==========

@app.delete("/task/{task_id}")
async def cancel_task(task_id: str):
    """Cancel a running task"""
    task_result = AsyncResult(task_id)
    task_result.revoke(terminate=True)
    
    return {
        "message": "Task cancellation requested",
        "task_id": task_id
    }
```

### Running Celery Worker

```bash
# Start Celery worker
celery -A celery_app worker --loglevel=info

# Start worker with specific queue
celery -A celery_app worker --loglevel=info -Q emails

# Start multiple workers for different queues
celery -A celery_app worker --loglevel=info -Q emails -n email_worker
celery -A celery_app worker --loglevel=info -Q images -n image_worker

# Start Flower (monitoring UI)
celery -A celery_app flower
# Access at http://localhost:5555
```

---

## Scheduled Tasks (Cron Jobs)

### Celery Beat for Periodic Tasks

```python
# celery_app.py
from celery import Celery
from celery.schedules import crontab

celery_app = Celery(
    "worker",
    broker="redis://localhost:6379/0",
    backend="redis://localhost:6379/0"
)

# Periodic task schedule
celery_app.conf.beat_schedule = {
    # Run every 30 seconds
    'cleanup-temp-files': {
        'task': 'tasks.cleanup_temp_files',
        'schedule': 30.0,
    },
    
    # Run every day at midnight
    'daily-report': {
        'task': 'tasks.generate_daily_report',
        'schedule': crontab(hour=0, minute=0),
    },
    
    # Run every Monday at 8 AM
    'weekly-summary': {
        'task': 'tasks.send_weekly_summary',
        'schedule': crontab(hour=8, minute=0, day_of_week=1),
    },
    
    # Run every 15 minutes
    'sync-data': {
        'task': 'tasks.sync_external_data',
        'schedule': crontab(minute='*/15'),
    },
    
    # Run every hour
    'update-cache': {
        'task': 'tasks.update_cache',
        'schedule': crontab(minute=0),  # Every hour at minute 0
    },
    
    # Run first day of month
    'monthly-billing': {
        'task': 'tasks.process_monthly_billing',
        'schedule': crontab(hour=0, minute=0, day_of_month=1),
    },
}

# tasks.py
from celery_app import celery_app
from datetime import datetime

@celery_app.task
def cleanup_temp_files():
    """Clean up temporary files"""
    print(f"Cleaning temp files at {datetime.now()}")
    # Cleanup logic here
    return "Temp files cleaned"

@celery_app.task
def generate_daily_report():
    """Generate daily report"""
    print(f"Generating daily report at {datetime.now()}")
    # Report generation logic
    return "Daily report generated"

@celery_app.task
def send_weekly_summary():
    """Send weekly summary"""
    print(f"Sending weekly summary at {datetime.now()}")
    # Summary logic
    return "Weekly summary sent"

@celery_app.task
def sync_external_data():
    """Sync data from external source"""
    print(f"Syncing data at {datetime.now()}")
    # Sync logic
    return "Data synced"

@celery_app.task
def update_cache():
    """Update application cache"""
    print(f"Updating cache at {datetime.now()}")
    # Cache update logic
    return "Cache updated"

@celery_app.task
def process_monthly_billing():
    """Process monthly billing"""
    print(f"Processing monthly billing at {datetime.now()}")
    # Billing logic
    return "Billing processed"
```

### Running Celery Beat

```bash
# Start Celery Beat scheduler
celery -A celery_app beat --loglevel=info

# In production, run both worker and beat:
# Terminal 1: Worker
celery -A celery_app worker --loglevel=info

# Terminal 2: Beat scheduler
celery -A celery_app beat --loglevel=info
```

---

## Alternative: APScheduler

For simpler scheduling needs without Celery:

```bash
uv pip install apscheduler
```

```python
# scheduler.py
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from datetime import datetime

scheduler = AsyncIOScheduler()

async def scheduled_task():
    """Task to run on schedule"""
    print(f"Scheduled task running at {datetime.now()}")
    # Task logic here

# Add jobs
scheduler.add_job(
    scheduled_task,
    'interval',
    minutes=5,  # Run every 5 minutes
    id='my_job_1'
)

scheduler.add_job(
    scheduled_task,
    'cron',
    hour=8,
    minute=0,  # Run daily at 8:00 AM
    id='my_job_2'
)

# main.py
from fastapi import FastAPI
from contextlib import asynccontextmanager
from scheduler import scheduler

@asynccontextmanager
async def lifespan(app: FastAPI):
    # Start scheduler on startup
    scheduler.start()
    print("Scheduler started")
    
    yield
    
    # Shutdown scheduler
    scheduler.shutdown()
    print("Scheduler stopped")

app = FastAPI(lifespan=lifespan)

@app.get("/")
async def root():
    return {"message": "Scheduler is running"}
```

---

## Best Practices

### 1. Choose the Right Tool

```python
# FastAPI BackgroundTasks: Quick, simple tasks
@app.post("/quick-task")
async def quick_task(background_tasks: BackgroundTasks):
    background_tasks.add_task(log_action)  # < 5 seconds
    return {"status": "ok"}

# Celery: Long-running, critical tasks
@app.post("/long-task")
async def long_task():
    task = generate_report.delay()  # > 5 seconds, needs reliability
    return {"task_id": task.id}

# Scheduled tasks: Use Celery Beat or APScheduler
```

### 2. Task Design Principles

```python
# ‚úÖ GOOD: Idempotent tasks (can run multiple times safely)
@celery_app.task
def update_user_stats(user_id: int):
    """Calculate stats from scratch each time"""
    stats = calculate_stats(user_id)
    save_stats(user_id, stats)

# ‚ùå BAD: Non-idempotent
@celery_app.task
def increment_counter(user_id: int):
    """If run twice, counter is wrong"""
    counter = get_counter(user_id)
    save_counter(user_id, counter + 1)

# ‚úÖ GOOD: Small, focused tasks
@celery_app.task
def send_single_email(email: str, message: str):
    """One email per task"""
    send_email(email, message)

# ‚ùå BAD: Large, monolithic tasks
@celery_app.task
def send_all_emails(user_ids: list):
    """If fails midway, hard to recover"""
    for user_id in user_ids:
        send_email(user_id)
```

### 3. Error Handling

```python
@celery_app.task(
    autoretry_for=(Exception,),
    retry_kwargs={'max_retries': 3, 'countdown': 5},
    retry_backoff=True
)
def robust_task(data: dict):
    """Task with proper error handling"""
    try:
        process_data(data)
    except SpecificError as e:
        # Log and handle specific errors
        logger.error(f"Specific error: {e}")
        raise  # Retry
    except Exception as e:
        # Log unexpected errors
        logger.exception(f"Unexpected error: {e}")
        raise
```

### 4. Monitoring and Logging

```python
import logging

logger = logging.getLogger(__name__)

@celery_app.task(bind=True)
def monitored_task(self, data: dict):
    """Task with monitoring"""
    logger.info(f"Task started: {self.request.id}")
    
    try:
        # Process data
        result = process(data)
        
        logger.info(f"Task completed: {self.request.id}")
        return result
        
    except Exception as e:
        logger.error(f"Task failed: {self.request.id}, Error: {e}")
        raise
```

---

## Complete Example: Email Service

```python
# email_service.py
from celery_app import celery_app
from fastapi import FastAPI, BackgroundTasks, HTTPException
from pydantic import BaseModel, EmailStr
from typing import List
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart

app = FastAPI()

# ========== Models ==========

class EmailSchema(BaseModel):
    to: EmailStr
    subject: str
    body: str
    
class BulkEmailSchema(BaseModel):
    recipients: List[EmailStr]
    subject: str
    body: str

# ========== Tasks ==========

@celery_app.task(
    name="tasks.send_email",
    autoretry_for=(Exception,),
    retry_kwargs={'max_retries': 3, 'countdown': 60}
)
def send_email_task(to: str, subject: str, body: str):
    """Send individual email"""
    try:
        # Email configuration
        smtp_server = "smtp.gmail.com"
        smtp_port = 587
        sender_email = "your-email@gmail.com"
        sender_password = "your-app-password"
        
        # Create message
        message = MIMEMultipart()
        message["From"] = sender_email
        message["To"] = to
        message["Subject"] = subject
        message.attach(MIMEText(body, "plain"))
        
        # Send email
        with smtplib.SMTP(smtp_server, smtp_port) as server:
            server.starttls()
            server.login(sender_email, sender_password)
            server.send_message(message)
        
        return {"status": "sent", "to": to}
        
    except Exception as e:
        print(f"Failed to send email to {to}: {e}")
        raise

# ========== Endpoints ==========

@app.post("/send-email")
async def send_email(email: EmailSchema):
    """Send single email asynchronously"""
    task = send_email_task.delay(
        email.to,
        email.subject,
        email.body
    )
    
    return {
        "message": "Email queued",
        "task_id": task.id
    }

@app.post("/send-bulk-email")
async def send_bulk_email(bulk_email: BulkEmailSchema):
    """Send emails to multiple recipients"""
    task_ids = []
    
    for recipient in bulk_email.recipients:
        task = send_email_task.delay(
            recipient,
            bulk_email.subject,
            bulk_email.body
        )
        task_ids.append(task.id)
    
    return {
        "message": f"Queued {len(task_ids)} emails",
        "task_ids": task_ids
    }
```

---

## Summary

In Chapter 7, you learned:
- FastAPI Background Tasks for simple async operations
- Startup and shutdown events with lifespan
- Celery for robust task queues
- Scheduled tasks with Celery Beat
- APScheduler as a simpler alternative
- Best practices for background processing
- Complete email service example

**Key Takeaways:**
- Use BackgroundTasks for quick operations (< 5 seconds)
- Use Celery for long-running, critical tasks
- Always design idempotent tasks
- Implement proper retry logic and error handling
- Monitor and log all background tasks

**Next: Chapter 8 - Advanced Routing & API Design**